(target-module-cv-tracking)=
# Computer Vision Tracking
The computer vision tracking module facilitates tracking and analysing the positions of animals within specific regions or areas in the arena that are of behavioural significance (e.g. [nest](target-module-nest), [foraging patch](target-module-foraging-patch)) based on video data from [camera](target-module-camera) devices.
It includes nodes such as `PositionTracking` for detecting and tracking objects, `RegionTracking` for determining if an object is within a specific region, `DistanceFromPoint` for calculating the distance between an object and a point, and `InRange` for checking if an object is within a specified range. 
These enable real-time tracking, event triggering, and data logging, making it easier to monitor and analyse animal behaviour in various experimental setups.

## Nodes
### PositionTracking 
The `PositionTracking (Aeon.Vision)` node accepts `Harp.Timestamped<Aeon.Acquisition.VideoDataFrame>` generated by a [camera](target-module-camera) and detects dark objects against a light background using a standard blob tracking pipeline. 

Briefly, the incoming image is thresholded to a binary image, then masked using a binary mask image generated by the user. 
This mask should include any area in which the animal could be at a given time, and must have the same width and height as the camera event images<!-- does "camera event images" refer to the "VideoDataFrame"s? --> for which it is intended. 
This ensures any movement or activity outside of the region of interest (therefore have pixel values of zero in the user-generated mask) are eliminated and not tracked.
Below is an example of an input image of the arena from a top-view camera and its corresponding mask.

::::things-in-a-row
:::{figure} ../../images/module-cv-tracking-cameratop.png
:height: 200px
:alt: camera top

Arena image from "CameraTop".
:::
:::{figure} ../../images/module-cv-tracking-arena-mask.png
:height: 200px
:alt: arena mask

Arena mask.
:::
::::

Contours are found and located using the `FindContours` and `BinaryRegionAnalysis` functions from openCV.NET (available in `Bonsai.Vision` package) and a custom `TakeLargestRegions (Aeon.Vision)` to take the *n* largest regions as defined by the property [`TrackingCount`](#properties). 

<!-- #### Inputs
Is `Harp.Timestamped<Aeon.Acquisition.VideoDataFrame>` the input? 

#### Outputs
Is `Harp.Timestamped<Bonsai.Vision.ConnectedComponentCollection>` the output? 
-->
#### Properties
##### Tracking parameters
| Property name | Description                                               |
|---------------|-----------------------------------------------------------|
| **Mask**           | The full or relative path to the mask image, in `.png` format    |
| **Threshold** | The threshold pixel value to apply in order to detect a dark blob on the light background |
| **TrackingCount** | The expected number of animals to track simultaneously |

##### Subjects
Events from this node are published to shared `Subjects`. 
Here you set the names used for these `Subjects` to identify events for this node.
Each of these `Subjects` becomes accessible in the bonsai editor's toolbox anywhere in the workflow using the name set here.

###### Device events subjects
| Subject name      | Type        | Description                   |
|-------------------|-------------|-------------------------------|
| **TrackingEvents** | `Harp.Timestamped<Bonsai.Vision.ConnectedComponentCollection>` | The `Subject` to which tracking data will be published. This stream is also output directly by the node. | 

###### Device input subjects
| Subject name      | Type          | Description                                                                                     |
|-------------------|---------------|-------------------------------------------------------------------------------------------------|
| **FrameEvents**   | `Harp.Timestamped<Aeon.Acquisition.VideoDataFrame>` | The `Subject` to subscribe to that carries frame events from a chosen camera | 

#### Usage
Create a `GroupWorkflow` and give it an appropriate name, e.g. "TrackingTop". 
Inside, place a `PositionTracking (Aeon.Vision)` node, externalise all properties, and connect it to the `WorkflowOutput`.
<!-- Is there a use case for the basic workflow without a camera supplying the videodataframes? If none, perhaps we can remove this example and just provide the full example with camera connected. -->
![Aeon.Vision.PositionTracking](../../workflows/positionTrackingBase.svg)

Next, add a `SubscribeSubject` node, connect it to the `PositionTracking` node as an input, and externalise the `Name` property. 
This name will be set to the `Subject` that carries the `CameraEvents` from the [camera](target-module-camera) on which tracking is to be performed (e.g. "CameraTop"). <!-- Clarify `CameraEvents`: is this "FrameEvents" or `Harp.CameraControllerGen2.CameraEvents`? -->
To avoid confusion<!-- with what? -->, change the display name of the externalised `Name` property to "FrameEvents".

![Aeon.Vision.PositionTracking](../../workflows/positionTracking.svg)

### RegionTracking 
The `RegionTracking (Aeon.Vision)` node computes and returns a `boolean` describing whether the [tracking position](#positiontracking) of an animal falls within a defined [region](target-node-regiontracking-properties) of the camera image.
<!-- To be completed
#### Inputs
Tracking events in the format `Harp.Timestamped<Bonsai.Vision.ConnectedComponentCollection>` generated by a [`PositionTracking (Aeon.Vision)`](#positiontracking) node?

#### Outputs
boolean? 
-->
(target-node-regiontracking-properties)=
#### Properties
##### General
| Property name | Description                                               |
|---------------|-----------------------------------------------------------|
| **Region**         | An array of four `OpenCV.NET.Points`. These define the corners (anticlockwise from top left) of the region of interest     |

##### Subjects
Both generated and input events of this node are published to shared `Subjects`. 
Here you set the names used for these `Subjects` to identify events for this node.
Each of these `Subjects` becomes accessible in the bonsai editor's toolbox anywhere in the workflow using the name set here.

###### Device events subjects
| Subject name      | Type        | Description                   |
|-------------------|-------------|-------------------------------|
| **RegionEvents** | `Harp.Timestamped<bool>` | The `Subject` to which region events will be published. This stream is also output directly by the node. | 

###### Device input subjects
| Subject name      | Type          | Description                                                                                     |
|-------------------|---------------|-------------------------------------------------------------------------------------------------|
| **FrameEvents**   | `Harp.Timestamped<Aeon.Acquisition.VideoDataFrame>` | The `Subject` to subscribe to that carries frame events from a chosen camera. | 

#### Usage
Place a `RegionTracking (Aeon.Vision)` node, externalise the `Region` property, and rename it to indicate the region this node is responsible for monitoring (e.g. "NestRegion").
Next, add a `SubscribeSubject` node to subscribe to the common "TrackingEvents" `Subject` (e.g. "TrackingTop") and connect it to the `RegionTracking (Aeon.Vision)` node.

![RegionTracking](../../workflows/regionTracking.svg)

### DistanceFromPoint 
The `DistanceFromPoint (Aeon.Vision)` node computes the distance (in pixels) between the [tracking position](#positiontracking) of an animal and a defined [point](target-node-distancefrompoint-properties) in the camera image. 
<!-- To be completed
#### Inputs
Tracking events in the format `Harp.Timestamped<Bonsai.Vision.ConnectedComponentCollection>` generated by a [`PositionTracking (Aeon.Vision)`](#positiontracking) node?

#### Outputs
A `Harp.Timestamped<double>` distance in pixels?

:::{note}
This node does not output to a published `Subject`.
:::
-->
(target-node-distancefrompoint-properties)=
#### Properties
##### General
<!-- Shouldn't the property be a point "Value"? Check whether the property is indeed "Region". -->
| Property name | Description                                               |
|---------------|-----------------------------------------------------------|
| **Region**         | An array of four `OpenCV.NET.Points`. These define the corners (anticlockwise from top left) of the region of interest     |

#### Usage
Place a `DistanceFromPoint (Aeon.Vision)` node, externalise the `Value` property, and rename it to indicate the point of interest for this node (e.g. "ArenaCenter").
Next, add a `SubscribeSubject` node to subscribe to the common "TrackingEvents" `Subject` (e.g. "TrackingTop") and connect it to the `DistanceFromPoint (Aeon.Vision)` node.

![DistanceFromPoint](../../workflows/distFromPoint.svg)

The output of the `DistanceFromPoint (Aeon.Vision)` node can then be used with other nodes 
like the [`InRange (Aeon.Acquisition)`](#inrange) node to trigger events or commands based on the proximity of an animal to a specific point in the camera image.

### InRange 
The `InRange (Aeon.Acquisition)` node is used to determine whether a [tracked object](#positiontracking) is within a specified [range](target-node-inrange-properties) in a single dimension.
<!-- To be completed
#### Inputs
#### Outputs
A `boolean` describing whether each value of a sequence falls within a specific range
--> 
(target-node-inrange-properties)=
#### Properties
##### General
| Property name | Description                                               |
|---------------|-----------------------------------------------------------|
| **Lower**          | The lower end of the range to check the input value against |
| **Upper**          | The upper end of the range to check the input value against |

##### Subjects
Events from this node are published to shared `Subjects`. 
Here you set the names used for these `Subjects` to identify events for this node.
Each of these `Subjects` becomes accessible in the bonsai editor's toolbox anywhere in the workflow using the name set here.

###### Device events subjects
`HarpMessage` events emitted to a `Subject`

| Subject name      | Type        | Description                   |
|-------------------|-------------|-------------------------------|
| **RangeEvents**   | `Harp.Timestamped<bool>` | The `Subject` to which the result will be published |

#### Usage
The example here is based on a circular arena with a corridor around the outside.
To determine if a given source is within the inner radius of the arena, first create a `GroupWorkflow` and give it an appropriate name, e.g. "InArena".  
Inside, place an `InRange (Aeon.Acquisition)` node, externalise all relevant properties (i.e. the upper bound that is the inner radius of the arena), and connect it to the `WorkflowInput` (e.g. "Source1") and `WorkflowOutput`.

![InArena](../../workflows/inRange.svg)

The input can, for instance, be a [`DistanceFromPoint (Aeon.Vision)`](#distancefrompoint) node, which computes the distance of a tracked animal from the centre of the arena, allowing one to determine if the animal is in the open arena.
Multiple of these `GroupWorkflows` each containing a separate `InRange (Aeon.Acquisition)` node can be used together to determine if a tracked animal position falls within different regions of interest.
For instance, to further determine if the animal is in the corridor, add another `GroupWorkflow`, e.g. "InCorridor", with an `InRange (Aeon.Acquisition)` node with the lower bound set to the inner radius of the arena and the upper bound set to the outer radius of the arena.

![ArenaOrCorridor](../../workflows/corridorOrArena.svg)
<!-- To be completed
## GUI
Description of any user interface components and visualisers.
-->
## Logging
"TrackingEvents" from a `PositionTracking (Aeon.Vision)` node can be logged along with the camera from which the "FrameEvents" originated using a [`LogHarpState (Aeon.Acquisition)`](./logging.md#logharpstate) node. 
First, add a `SubscribeSubject` to subscribe to the "TrackingEvents" `Subject` (e.g. "TrackingTop").
The events can then be formatted as `HarpMessages` and configured to write to register **200** (an unassigned register on all Harp devices) using the custom `FormatBinaryRegions (Aeon.Vision)` node.

![logPositionTracking](../../workflows/logPositionTracking.svg)

Multiple `PositionTracking` nodes can be used to track objects in different camera streams simultaneously. 
To do this, select a different "FrameEvents" `Subject` for each node and save the results to the corresponding camera folders.
<!-- To be coompleted 
## State persistence
Information on state recovery or persistence requirements, if applicable.

## Alerts
Explanation of any alert configurations and links to guides or further configuration steps.
-->