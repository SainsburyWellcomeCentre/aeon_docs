# Computer vision tracking

A number of modules in Project AEON are not driven directly by any hardware but act on data streams or events originating from a hardware module to generate new outputs to monitor animal behviour in real time for quanitification and logging of animal behaviour, but also to trigger task control events in response to specific behaviours.

We think of these as 'soft devices' in that while they are not a hardware device themselves, they behave much like any other device, with inputs and triggers linked to hardware and generating events in synchrony with other hardware events.

One of the most salient and common problems with monitoring behaviour, particularly in freely moving experiments, is the position of the animal subject. This requires online detection and tracking of the animal. We achieve this using one of two or both methods parrallel.

## Computer vision blob detection

Create a `GroupWorkflow`, add an `PositionTracking (Aeon.Acquisition)` node, externalise all properties and connect it to the `WorkflowOutput`:

![Tracking1](./Workflows/Tracking1.svg)

## Inputting camera stream

This node accepts `Harp.Timestamped<Aeon.Acquisition.VideoDataFrame>` generated by the [cameras](../../HardwareDevices/Camera/camera.md) and detects dark objects against a light background using a standard blob tracking pipeline. 

Add a `SubscribeSubject` to your workflow, connect it to the `PositionTracking` node as an input and externalise the `Name` property. This name will be set to the `Subject` that carries the `CameraEvents` from the camera you wish to perform tracking on, for example `CameraTop`. Change the display name of the externalised `Name` property to `FrameEvents` to avoid confusion:

![Tracking1](./Workflows/Tracking2.svg)

Breifly, the incoming image is thresholded, then masked using a binary mask image generated by the user. This mask should include any area in which the animal could be at a given time, and must have the same width and height as the camera event images. This ensures that movement or activity outside of the arena (therefore have pixel values of zero in the manually generated mask) are eliminated and not tracked.

Contours are found and located using the `FindContours` and `BinaryRegionAnalysis` functions from openCV.NET (available in `Bonsai.Vision` package) and a custom `TakeLargestRegions (Aeon.Acquisition)` to take the *n* largest regions (defined by the property **TrackingCount** above). 

## **Properties of the node**

You should now have a `GroupWorkflow` called "Tracking" or similar. The properties of this `GroupWorkflow`, accessible when selected include the parameters for tuning the tracking workflow and the names of the input and output `Subjects`.

### ***Tracking parameters:***

| **Property Name**  | **Description**                                                                           |
|--------------------|-------------------------------------------------------------------------------------------|
| **Mask**           | The full or relative path to the mask image, in `.png` format    |
| **Threshold** | The threshold pixel value to apply in order to detect a dark blob on the light background |
| **TrackingCount** | The expected number of animals to track simultaneously |

### ***Subject names:***
Events from this node are published to a `Subject` to make it available anywhere in the workflow. Here in the properties, you define a name to give this `Subject` 

## **Subjects** 

### **Device Event Subject**:
| **Subject Name**  | **Type** | **Description**                                  |
|-------------------|----------|--------------------------------------------------|
| **TrackingEvents** | `Harp.Timestamped<Bonsai.Vision.ConnectedComponentCollection>` |The `Subject` to which tracking data will be published. This stream is also output directly by the node | 

### **Device Input Subject**

| **Subject Name**  | **Type** | **Description**                                  |
|-------------------|----------|--------------------------------------------------|
| **FrameEvents**   | `Harp.Timestamped<Aeon.Acquisition.VideoDataFrame>` | The `Subject` to subscribe to that carries frame events from a chosen camera | 

## Logging

Logging this soft device is done using a `logHarpState` node detailed in the [guide](../../Logging/LogHarpState.md). This node writes `Harp.HarpMessages` to binary files named according to the `LogName` property. `TrackingEvents` can be formatted using the custom `FormatBinaryRegions (Aeon.Acquisition)` to convert the events to `HarpMessages` and configure writing to register **200**. This is an unassigned register on all harp devices. For Project AEON, we store this tracking data along with the camera from which `FrameEvents` originated. So for `CameraTop` e.g.:

![logTracking](./Workflows/logTracking.svg)

Multiple of these modules can be reused to track different cameras simultaneously, by selecting a different `FrameEvents` subject and saving to the individual camera folders in this way.