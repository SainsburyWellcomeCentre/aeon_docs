# Pose estimation

Recent years have seen the development of a growing number of tools based on deep learning AI networks such as [DeepLabCut](https://deeplabcut.github.io/DeepLabCut/README.html) and [SLEAP](https://sleap.ai/). These tools can be used to identify and track the detailed features of an object, person or animal. In Project AEON we use SLEAP's multi-animal tracking capability to identify and track different individuals in the arena at the same time. SLEAP is fully integrated with Bonsai through the [Bonsai.Sleap](https://bonsai-rx.org/sleap/index.html) package, to run inference in real time using a trained and exported model.

Add a `GroupWorkflow` node, call it something appropriate such as "PoseTracking", place a `PoseTracking (Aeon.Acquisition)` node inside and connect to the `WorkflowOutput`. Externalise all properties for access outside of the `GroupWorkflow`.

![poseTracking](./Workflows/poseTracking.svg)

This node loads a pretrained SLEAP model, and runs inference on FrameEvents coming from one of the [camera](../../HardwareDevices/Camera/camera.md) devices, returning timestamped data containing the position of each animal, their identity and confidence measures. 

For more details on training your own models through the Sleap GUI, please refer to the documentation and instructions for [`SLEAP`](https://sleap.ai/) and [`Bonsai.Sleap`](https://bonsai-rx.org/sleap/index.html)
Once you have a trained model you wish to use for inference, you must export the model into the more generic [Protocol buffer (.pb) format](https://protobuf.dev/) using the command line interface command, [sleap-export](https://sleap.ai/guides/cli.html#sleap-export)

## **Properties of PoseTracking**

You should now have a `GroupWorkflow` called "PoseTracking" or similar. The properties of this `GroupWorkflow`, accessible when selected include the parameters for tuning the tracking workflow and the names of the input and output `Subjects`.

### ***General (Misc.):***

| **Property Name**  | **Description**                                                                           |
|--------------------|-------------------------------------------------------------------------------------------|
| **IdentityMinConfidence** |  Set the minimum confidence score applied to computation of an object (animal) instance's centroid |
| **FrameStep**           | Frame by frame inference and pose estimation is computationally expensive. It may be helpful to downsample the incoming stream to run inference in real time. Here you can set the number of frames to skip between incoming frames  |
| **IdentityMinConfidence** | Set the minimum confidence required to label an instance's identity |
| **ModelPath** | Set the partial path to the saved `.pb` frozen_graph |
| **PartMinConfidence** | Set the minimum confidence required to assign a label to an instance's keypoint |
### ***Subject names:***
Events that supply the inputs and the resulting events generated by this node are published to a `Subject` to make it available anywhere in the workflow. Here in the properties, you define a name to give this `Subject` 

## **Subjects** 

### **Device Event Subject**:
| **Subject Name**  | **Type** | **Description**                                  |
|-------------------|----------|--------------------------------------------------|
| **TrackingEvents** | `Harp.Timestamped<Bonsai.Vision.ConnectedComponentCollection>` |The `Subject` to which tracking data will be published. This stream is also output directly by the node | 

### **Device Input Subject**

| **Subject Name**  | **Type** | **Description**                                  |
|-------------------|----------|--------------------------------------------------|
| **FrameEvents**   | `Harp.Timestamped<Aeon.Acquisition.VideoDataFrame>` | The `Subject` to subscribe to that carries frame events from a chosen camera | 

## Logging

Logging this soft device is done using a `logHarpState` node detailed in the [guide](../../Logging/LogHarpState.md). This node writes `Harp.HarpMessages` to binary files named according to the `LogName` property. `TrackingEvents` can be formatted using the custom `FormatBinaryRegions (Aeon.Acquisition)` to convert the events to `HarpMessages` and configure writing to register **200**. This is an unassigned register on all harp devices. For Project AEON, we store this tracking data along with the camera from which `FrameEvents` originated. So for `CameraTop` e.g.:

![logTracking](./../../Logging/Workflows/logData.svg)

Multiple of these modules can be reused to track different cameras simultaneously, by selecting the `FrameEvents` `Subject` and saving to the individual camera folders in this way.

