{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "657dc9ac",
   "metadata": {},
   "source": [
    "# Platform paper social experiment analysis -- part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a28e1d",
   "metadata": {},
   "source": [
    "In this example we will work with behavioural data collected from experiments `social0.2`, `social0.3`, and `social0.4`, in which two mice foraged for food in the [habitat](target-habitat) with three [foraging patches](target-foraging-patches) whose reward rates changed dynamically over time. \n",
    "\n",
    "The experiments each consist of three periods:\n",
    "\n",
    "1. \"presocial\", in which each mouse was in the habitat alone for 3-4 days.\n",
    "2. \"social\", in which both mice were in the habitat together for 2 weeks.\n",
    "3. \"postsocial\", in which each mouse was in the habitat alone again for 3-4 days.\n",
    "\n",
    "The goal of the experiments was to understand how the mice's behaviour changes as they learn to forage for food in the habitat, and how their behaviour differs between social vs. solo settings.\n",
    "\n",
    "The full datasets are available on the [Datasets](target-full-datasets) page but for the purpose of this example, we have also precomputed [analysis-ready datasets&emdash;**Platform paper social experiment analysed datasets**](https://app.globus.org/file-manager?origin_id=48cc1398-b591-4f52-85d2-f68801306d4a&origin_path=%2F).\n",
    "\n",
    ":::{seealso}\n",
    "\"Extended Data Fig. 7\", in \"Extended Data\" in the \"Supplementary Material\" of the [platform paper](aeon-paper:) for a detailed description of the experiments.\n",
    ":::\n",
    "\n",
    "Below is a brief explanation of how the environment (i.e. patch properties) changed over {term}`blocks <Block>` (60&ndash;180 minute periods of time):\n",
    "\n",
    "1. Every block begins at a random interval $t$:\n",
    "    $$\n",
    "    t \\sim \\mathrm{Uniform}(60,\\,180) \\quad \\text{In minutes}\n",
    "    $$\n",
    "2. At the start of each block, sample a row from the predefined matrix $\\lambda_{\\mathrm{set}}$:\n",
    "    $$\n",
    "    \\lambda_{\\mathrm{set}} = \n",
    "    \\begin{pmatrix}\n",
    "    1 & 1 & 1 \\\\\n",
    "    5 & 5 & 5 \\\\\n",
    "    1 & 3 & 5 \\\\\n",
    "    1 & 5 & 3 \\\\\n",
    "    3 & 1 & 5 \\\\\n",
    "    3 & 5 & 1 \\\\\n",
    "    5 & 1 & 3 \\\\\n",
    "    5 & 3 & 1 \\\\\n",
    "    \\end{pmatrix}\n",
    "    \\quad \\text{In meters}\n",
    "    $$\n",
    "3. Assign the sampled row to specific patch means $\\lambda_{\\mathrm{1}}, \\lambda_{\\mathrm{2}}, \\lambda_{\\mathrm{3}}$ and apply a constant offset $c$ to all thresholds:\n",
    "    $$\n",
    "    \\begin{aligned}\n",
    "    \\lambda_{\\mathrm{1}}, \\lambda_{\\mathrm{2}}, \\lambda_{\\mathrm{3}} &\\sim \\mathrm{Uniform}(\\lambda_{\\mathrm{set}}) \\\\\n",
    "    c &= 0.75\n",
    "    \\end{aligned}\n",
    "    \\quad \\text{Patch means and offset}\n",
    "    $$\n",
    "4. Sample a value from each of $P_{\\mathrm{1}}, P_{\\mathrm{2}}, P_{\\mathrm{3}}$ as the initial threshold for the respective patch. Whenever a patch reaches its threshold, resample a new value from its corresponding distribution:\n",
    "    $$\n",
    "    \\begin{aligned}\n",
    "    P_{\\mathrm{1}} &= c + \\mathrm{Exp}(1/\\lambda_{\\mathrm{1}}) \\\\\n",
    "    P_{\\mathrm{2}} &= c + \\mathrm{Exp}(1/\\lambda_{\\mathrm{2}}) \\\\\n",
    "    P_{\\mathrm{3}} &= c + \\mathrm{Exp}(1/\\lambda_{\\mathrm{3}})\n",
    "    \\end{aligned}\n",
    "    \\quad \\text{Patch distributions}\n",
    "    $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf569eb0",
   "metadata": {},
   "source": [
    "## Set up environment\n",
    "\n",
    "Create and activate a virtual environment named `social-analysis` using [uv](https://docs.astral.sh/uv/getting-started/installation/).\n",
    "```bash\n",
    "uv venv aeon-social-analysis --python \">=3.11\" \n",
    "source aeon-social-analysis/bin/activate   # Unix\n",
    ".\\aeon-social-analysis\\Scripts\\activate   # Windows\n",
    "```\n",
    "\n",
    "Install the required [`ssm` package](https://github.com/lindermanlab/ssm) and its dependencies.\n",
    "```bash\n",
    "uv pip install matplotlib numpy pandas plotly statsmodels pyyaml pyarrow tqdm scipy jupyter\n",
    "```\n",
    "\n",
    "## Import libraries and define variables and helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "339b11c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Tuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objs as go\n",
    "import plotly.io as pio\n",
    "import statsmodels.api as sm\n",
    "import yaml\n",
    "from IPython.display import Image\n",
    "from plotly.subplots import make_subplots\n",
    "from scipy.stats import binomtest, ttest_rel, wilcoxon\n",
    "from tqdm.notebook import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd6ad00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "cm2px = 5.2  # 1 cm = 5.2 px roughly in aeon habitats\n",
    "light_off, light_on = 7, 20  # 7am to 8pm\n",
    "fps = 50  # frames per second for tracking camera\n",
    "\n",
    "# Experiment timelines\n",
    "experiments = [\n",
    "    {\n",
    "        \"name\": \"social0.2-aeon3\",\n",
    "        \"presocial_start\": \"2024-01-31 11:00:00\",\n",
    "        \"presocial_end\": \"2024-02-08 15:00:00\",\n",
    "        \"social_start\": \"2024-02-09 16:00:00\",\n",
    "        \"social_end\": \"2024-02-23 13:00:00\",\n",
    "        \"postsocial_start\": \"2024-02-25 17:00:00\",\n",
    "        \"postsocial_end\": \"2024-03-02 14:00:00\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"social0.2-aeon4\",\n",
    "        \"presocial_start\": \"2024-01-31 11:00:00\",\n",
    "        \"presocial_end\": \"2024-02-08 15:00:00\",\n",
    "        \"social_start\": \"2024-02-09 17:00:00\",\n",
    "        \"social_end\": \"2024-02-23 12:00:00\",\n",
    "        \"postsocial_start\": \"2024-02-25 18:00:00\",\n",
    "        \"postsocial_end\": \"2024-03-02 13:00:00\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"social0.3-aeon3\",\n",
    "        \"presocial_start\": \"2024-06-08 19:00:00\",\n",
    "        \"presocial_end\": \"2024-06-17 13:00:00\",\n",
    "        \"social_start\": \"2024-06-25 11:00:00\",\n",
    "        \"social_end\": \"2024-07-06 13:00:00\",\n",
    "        \"postsocial_start\": \"2024-07-07 16:00:00\",\n",
    "        \"postsocial_end\": \"2024-07-14 14:00:00\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"social0.4-aeon3\",\n",
    "        \"presocial_start\": \"2024-08-16 17:00:00\",\n",
    "        \"presocial_end\": \"2024-08-24 10:00:00\",\n",
    "        \"social_start\": \"2024-08-28 11:00:00\",\n",
    "        \"social_end\": \"2024-09-09 13:00:00\",\n",
    "        \"postsocial_start\": \"2024-09-09 18:00:00\",\n",
    "        \"postsocial_end\": \"2024-09-22 16:00:00\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"social0.4-aeon4\",\n",
    "        \"presocial_start\": \"2024-08-16 15:00:00\",\n",
    "        \"presocial_end\": \"2024-08-24 10:00:00\",\n",
    "        \"social_start\": \"2024-08-28 10:00:00\",\n",
    "        \"social_end\": \"2024-09-09 01:00:00\",\n",
    "        \"postsocial_start\": \"2024-09-09 15:00:00\",\n",
    "        \"postsocial_end\": \"2024-09-22 16:00:00\",\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa093652",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_from_parquet(\n",
    "    experiment_name: str | None,\n",
    "    period: str | None,\n",
    "    data_type: str,\n",
    "    data_dir: Path,\n",
    "    set_time_index: bool = False,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Loads saved data from parquet files.\n",
    "\n",
    "    Args:\n",
    "        experiment_name (str, optional): Filter by experiment name. If None, load all experiments.\n",
    "        period (str, optional): Filter by period (presocial, social, postsocial). If None, load all periods.\n",
    "        data_type (str): Type of data to load (position, patch, foraging, rfid, sleep, explore)\n",
    "        data_dir (Path): Directory containing parquet files.\n",
    "        set_time_index (bool, optional): If True, set 'time' column as DataFrame index.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Combined DataFrame of all matching parquet files.\n",
    "    \"\"\"\n",
    "    if not data_dir.exists():\n",
    "        print(f\"Directory {data_dir} does not exist. No data files found.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Create pattern based on filters\n",
    "    pattern = \"\"\n",
    "    if experiment_name:\n",
    "        pattern += f\"{experiment_name}_\"\n",
    "    else:\n",
    "        pattern += \"*_\"\n",
    "\n",
    "    if period:\n",
    "        pattern += f\"{period}_\"\n",
    "    else:\n",
    "        pattern += \"*_\"\n",
    "\n",
    "    pattern += f\"{data_type}.parquet\"\n",
    "\n",
    "    # Find matching files\n",
    "    matching_files = list(data_dir.glob(pattern))\n",
    "\n",
    "    if not matching_files:\n",
    "        print(f\"No matching data files found with pattern: {pattern}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    print(f\"Found {len(matching_files)} matching files\")\n",
    "\n",
    "    # Load and concatenate matching files\n",
    "    dfs = []\n",
    "    total_rows = 0\n",
    "    for file in matching_files:\n",
    "        print(f\"Loading {file}...\")\n",
    "        df = pd.read_parquet(file)\n",
    "        total_rows += len(df)\n",
    "        dfs.append(df)\n",
    "        print(f\"  Loaded {len(df)} rows\")\n",
    "\n",
    "    # Combine data\n",
    "    if dfs:\n",
    "        combined_df = pd.concat(dfs, ignore_index=True)\n",
    "        if set_time_index and \"time\" in combined_df.columns:\n",
    "            combined_df = combined_df.set_index(\"time\")\n",
    "        print(f\"Combined data: {len(combined_df)} rows\")\n",
    "        return combined_df\n",
    "    else:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "\n",
    "def load_experiment_data(\n",
    "    data_dir: Path,\n",
    "    experiment: dict | None = None,\n",
    "    periods: list | None = None,\n",
    "    data_types: list[str] = [\"rfid\", \"position\"],\n",
    "    trim_days: int | None = None,\n",
    ") -> dict:\n",
    "    \"\"\"Load all data types for specified periods of an experiment.\n",
    "\n",
    "    Parameters:\n",
    "    - experiment: experiment dict with period start/end times\n",
    "    - periods: list of periods to load\n",
    "    - data_types: list of data types to load\n",
    "    - data_dir: directory containing data files\n",
    "    - trim_days: Optional number of days to trim from start (None = no trim)\n",
    "\n",
    "    Returns:\n",
    "    - Dictionary containing dataframes for each period/data type combination\n",
    "    \"\"\"\n",
    "    result = {}\n",
    "\n",
    "    if periods is None:\n",
    "        periods = [None]\n",
    "\n",
    "    for period in periods:\n",
    "        for data_type in data_types:\n",
    "            print(f\"Loading {period} {data_type} data...\")\n",
    "\n",
    "            # Load data\n",
    "            experiment_name = experiment[\"name\"] if experiment is not None else None\n",
    "            df = load_data_from_parquet(\n",
    "                experiment_name=experiment_name,\n",
    "                period=period,\n",
    "                data_type=data_type,\n",
    "                data_dir=data_dir,\n",
    "                set_time_index=(data_type == \"position\"),\n",
    "            )\n",
    "\n",
    "            # Trim if requested\n",
    "            if trim_days is not None and len(df) > 0:\n",
    "                if data_type == \"rfid\":\n",
    "                    start_time = df[\"chunk_start\"].min()\n",
    "                    end_time = start_time + pd.Timedelta(days=trim_days)\n",
    "                    df = df[df[\"chunk_start\"] < end_time]\n",
    "                if data_type == \"foraging\":\n",
    "                    start_time = df[\"start\"].min()\n",
    "                    end_time = start_time + pd.Timedelta(days=trim_days)\n",
    "                    df = df[df[\"start\"] < end_time]\n",
    "                if data_type == \"position\":\n",
    "                    start_time = df.index.min()\n",
    "                    end_time = start_time + pd.Timedelta(days=trim_days)\n",
    "                    df = df.loc[df.index < end_time]\n",
    "\n",
    "                print(f\"  Trimmed to {trim_days} days: {len(df)} records\")\n",
    "\n",
    "            # Store in result\n",
    "            key = f\"{period}_{data_type}\"\n",
    "            result[key] = df\n",
    "\n",
    "            # For position data, handle duplicates\n",
    "            if data_type == \"position\" and len(df) > 0:\n",
    "                original_len = len(df)\n",
    "                df = df.reset_index()\n",
    "                df = df.drop_duplicates(subset=[\"time\", \"identity_name\"])\n",
    "                df = df.set_index(\"time\")\n",
    "                result[key] = df\n",
    "                if len(df) < original_len:\n",
    "                    print(f\"  Removed duplicates: {original_len} -> {len(df)}\")\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37e92a6",
   "metadata": {},
   "source": [
    ":::{note}\n",
    "Change `data_dir` and `save_dir` to the paths where your local dataset (the parquet files) is stored and where you want to save the results.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec7c83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SET THESE VARIABLES ACCORDINGLY\n",
    "data_dir = Path(\"/ceph/aeon/aeon/code/scratchpad/methods_paper_data\")\n",
    "save_dir = Path(\"\")\n",
    "\n",
    "# Load metadata\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "with open(data_dir / \"Metadata_aeon3.yml\", \"r\") as file:\n",
    "    metadata_aeon3 = yaml.safe_load(file)\n",
    "\n",
    "with open(data_dir / \"Metadata_aeon4.yml\", \"r\") as file:\n",
    "    metadata_aeon4 = yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe27ba97",
   "metadata": {},
   "source": [
    "## Dominance plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "973b33e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "DISTANCE_THRESH = 50  # px\n",
    "MIN_DUR, MAX_DUR = 2, 5  # seconds\n",
    "PADDING_SEC = 2\n",
    "SAMPLE_SIZE = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed78b82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all periods for a given experiment\n",
    "experiment = experiments[0]\n",
    "\n",
    "data = load_experiment_data(\n",
    "    experiment=experiment,\n",
    "    data_dir=data_dir,\n",
    "    periods=[\"social\"],\n",
    "    data_types=[\"retreat\", \"position\"],\n",
    ")\n",
    "\n",
    "social_retreat_df = data[\"social_retreat\"]\n",
    "social_position_df = data[\"social_position\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4d6d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp, acq_pc = experiment[\"name\"].split(\"-\", 1)\n",
    "metadata = metadata_aeon3 if acq_pc == \"aeon3\" else metadata_aeon4\n",
    "\n",
    "inner_radius = float(metadata[\"ActiveRegion\"][\"ArenaInnerRadius\"])\n",
    "outer_radius = float(metadata[\"ActiveRegion\"][\"ArenaOuterRadius\"])\n",
    "center_x = float(metadata[\"ActiveRegion\"][\"ArenaCenter\"][\"X\"])\n",
    "center_y = float(metadata[\"ActiveRegion\"][\"ArenaCenter\"][\"Y\"])\n",
    "nest_y1 = float(metadata[\"ActiveRegion\"][\"NestRegion\"][\"ArrayOfPoint\"][1][\"Y\"])\n",
    "nest_y2 = float(metadata[\"ActiveRegion\"][\"NestRegion\"][\"ArrayOfPoint\"][2][\"Y\"])\n",
    "gate_width = 20\n",
    "gate_coordinates = []\n",
    "for device in metadata[\"Devices\"]:\n",
    "    if \"Gate\" in device and \"Rfid\" in device:\n",
    "        gate_coordinates.append(metadata[\"Devices\"][device][\"Location\"])\n",
    "\n",
    "# Compute squared distance from arena center\n",
    "social_position_df[\"dist2\"] = (social_position_df[\"x\"] - center_x) ** 2 + (\n",
    "    social_position_df[\"y\"] - center_y\n",
    ") ** 2\n",
    "\n",
    "# Build “in‐corridor” mask (between inner & outer radii)\n",
    "mask_corridor = social_position_df[\"dist2\"].between(inner_radius**2, outer_radius**2)\n",
    "\n",
    "# Exclude the nest region (to the right of center, between nest_y1 & nest_y2)\n",
    "mask_nest = ~(\n",
    "    (social_position_df[\"x\"] > center_x)\n",
    "    & (social_position_df[\"y\"].between(nest_y1, nest_y2))\n",
    ")\n",
    "\n",
    "# Exclude all gate regions (within gate_width of any gate)\n",
    "mask_gate = pd.Series(True, index=social_position_df.index)\n",
    "for loc in gate_coordinates:\n",
    "    gx, gy = float(loc[\"X\"]), float(loc[\"Y\"])\n",
    "    d2 = (social_position_df[\"x\"] - gx) ** 2 + (social_position_df[\"y\"] - gy) ** 2\n",
    "    mask_gate &= d2 > gate_width**2\n",
    "\n",
    "# Combine spatial masks\n",
    "spatial_mask = mask_corridor & mask_nest & mask_gate\n",
    "\n",
    "# Apply spatial filter\n",
    "df_spatial = social_position_df.loc[spatial_mask].copy()\n",
    "\n",
    "# Exclude retreat‐event timestamps\n",
    "df_spatial[\"time\"] = pd.to_datetime(df_spatial[\"time\"])\n",
    "social_retreat_df[\"start_timestamp\"] = pd.to_datetime(\n",
    "    social_retreat_df[\"start_timestamp\"]\n",
    ")\n",
    "social_retreat_df[\"end_timestamp\"] = pd.to_datetime(social_retreat_df[\"end_timestamp\"])\n",
    "\n",
    "# Build a mask for any retreat interval\n",
    "mask_retreat = pd.Series(False, index=df_spatial.index)\n",
    "for start, end in zip(\n",
    "    social_retreat_df[\"start_timestamp\"], social_retreat_df[\"end_timestamp\"]\n",
    "):\n",
    "    mask_retreat |= df_spatial[\"time\"].between(start, end)\n",
    "\n",
    "# Final filtered DataFrame\n",
    "df_filtered = (\n",
    "    df_spatial.loc[~mask_retreat]  # drop retreat frames\n",
    "    .drop(columns=[\"dist2\"])  # clean up helper column\n",
    "    .reset_index(drop=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717842eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_proximity_periods(\n",
    "    df: pd.DataFrame, distance_threshold: int = 60\n",
    ") -> tuple[list, list]:\n",
    "    \"\"\"Extract time periods where mice are close to each other.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame with columns ['time', 'identity_name', 'x', 'y']\n",
    "    - distance_threshold: Maximum distance in pixels for considering mice \"close\" (default: 60)\n",
    "\n",
    "    Returns:\n",
    "    - Tuple of (start_times, end_times) lists of matching size\n",
    "    \"\"\"\n",
    "    # Convert time to datetime if needed and sort\n",
    "    if not pd.api.types.is_datetime64_any_dtype(df[\"time\"]):\n",
    "        df = df.copy()\n",
    "        df[\"time\"] = pd.to_datetime(df[\"time\"])\n",
    "\n",
    "    # Get unique mice and times\n",
    "    mice = df[\"identity_name\"].unique()\n",
    "    n_mice = len(mice)\n",
    "\n",
    "    if n_mice < 2:\n",
    "        return [], []\n",
    "\n",
    "    # Create a more efficient data structure\n",
    "    # Pivot to get x,y coordinates for each mouse at each time\n",
    "    print(f\"Found {n_mice} mice, reshaping data...\")\n",
    "    df_pivot_x = df.pivot_table(\n",
    "        index=\"time\", columns=\"identity_name\", values=\"x\", aggfunc=\"first\"\n",
    "    )\n",
    "    df_pivot_y = df.pivot_table(\n",
    "        index=\"time\", columns=\"identity_name\", values=\"y\", aggfunc=\"first\"\n",
    "    )\n",
    "\n",
    "    # Get common time points where we have data\n",
    "    times = df_pivot_x.index\n",
    "\n",
    "    # Convert to numpy arrays for speed\n",
    "    x_coords = df_pivot_x.values  # shape: (n_times, n_mice)\n",
    "    y_coords = df_pivot_y.values\n",
    "\n",
    "    # Pre-allocate boolean array for proximity status\n",
    "    is_close = np.zeros(len(times), dtype=bool)\n",
    "\n",
    "    # Process in chunks to manage memory and provide progress\n",
    "    chunk_size = 10000\n",
    "\n",
    "    with tqdm(\n",
    "        total=len(times), desc=\"Calculating distances\", unit=\"timepoints\"\n",
    "    ) as pbar:\n",
    "        for start_idx in range(0, len(times), chunk_size):\n",
    "            end_idx = min(start_idx + chunk_size, len(times))\n",
    "\n",
    "            x_chunk = x_coords[start_idx:end_idx]  # shape: (chunk_size, n_mice)\n",
    "            y_chunk = y_coords[start_idx:end_idx]\n",
    "\n",
    "            # For each time point in chunk, check if any pair is close\n",
    "            for i in range(x_chunk.shape[0]):\n",
    "                x_positions = x_chunk[i]\n",
    "                y_positions = y_chunk[i]\n",
    "\n",
    "                # Remove NaN positions (missing mice)\n",
    "                valid_mask = ~(np.isnan(x_positions) | np.isnan(y_positions))\n",
    "                if np.sum(valid_mask) < 2:\n",
    "                    continue\n",
    "\n",
    "                x_valid = x_positions[valid_mask]\n",
    "                y_valid = y_positions[valid_mask]\n",
    "\n",
    "                # Calculate all pairwise distances using broadcasting\n",
    "                dx = x_valid[:, np.newaxis] - x_valid[np.newaxis, :]\n",
    "                dy = y_valid[:, np.newaxis] - y_valid[np.newaxis, :]\n",
    "                distances = np.sqrt(dx**2 + dy**2)\n",
    "\n",
    "                # Check if any pair (excluding diagonal) is close\n",
    "                # Use upper triangle to avoid checking same pair twice\n",
    "                upper_triangle = np.triu(distances, k=1)\n",
    "\n",
    "                # Only check non-zero distances (exclude the zeros from masking)\n",
    "                non_zero_distances = upper_triangle[upper_triangle > 0]\n",
    "                is_close[start_idx + i] = (\n",
    "                    np.any(non_zero_distances < distance_threshold)\n",
    "                    if len(non_zero_distances) > 0\n",
    "                    else False\n",
    "                )\n",
    "\n",
    "            pbar.update(end_idx - start_idx)\n",
    "\n",
    "    # Find transitions\n",
    "    # Start: current is close and previous is not close (or first point)\n",
    "    starts_mask = is_close & ~np.roll(is_close, 1)\n",
    "    starts_mask[0] = is_close[0]  # Handle first point\n",
    "\n",
    "    # End: current is close and next is not close (or last point)\n",
    "    ends_mask = is_close & ~np.roll(is_close, -1)\n",
    "    ends_mask[-1] = is_close[-1]  # Handle last point\n",
    "\n",
    "    start_times = times[starts_mask].tolist()\n",
    "    end_times = times[ends_mask].tolist()\n",
    "\n",
    "    # Ensure equal length\n",
    "    min_length = min(len(start_times), len(end_times))\n",
    "    start_times = start_times[:min_length]\n",
    "    end_times = end_times[:min_length]\n",
    "\n",
    "    return start_times, end_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3a2cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run proximity and filter durations\n",
    "starts, ends = extract_proximity_periods(df_filtered, DISTANCE_THRESH)\n",
    "\n",
    "# Compute durations & keep short ones\n",
    "durations = [(e - s).total_seconds() for s, e in zip(starts, ends)]\n",
    "periods = [\n",
    "    (s, e, d) for s, e, d in zip(starts, ends, durations) if MIN_DUR < d < MAX_DUR\n",
    "]\n",
    "periods.sort(key=lambda x: x[2])\n",
    "\n",
    "filtered_starts = [p[0] for p in periods]\n",
    "filtered_ends = [p[1] for p in periods]\n",
    "filtered_durations = [p[2] for p in periods]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087a7b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create events DataFrame from proximity periods\n",
    "def create_events_df_from_proximity(\n",
    "    start_times: list, end_times: list, experiment_name: str\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Create events DataFrame from proximity start/end times with root information.\n",
    "\n",
    "    Parameters:\n",
    "    - start_times: List of proximity period start timestamps\n",
    "    - end_times: List of proximity period end timestamps\n",
    "    - experiment_name: Name of experiment in format 'experiment-computer'\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame with proximity events including timestamps, root path, and duration\n",
    "    \"\"\"\n",
    "    events_data = []\n",
    "    experiment, acquisition_computer = experiment_name.split(\"-\", 1)\n",
    "    acquisition_computer = acquisition_computer.upper()\n",
    "    root_path = f\"/ceph/aeon/aeon/data/raw/{acquisition_computer}/{experiment}\"\n",
    "\n",
    "    for start_time, end_time in zip(start_times, end_times):\n",
    "        events_data.append(\n",
    "            {\n",
    "                \"start_timestamp\": start_time,\n",
    "                \"end_timestamp\": end_time,\n",
    "                \"root\": root_path,\n",
    "                \"duration_seconds\": (end_time - start_time).total_seconds(),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return pd.DataFrame(events_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2f687d",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d42c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "social_retreat_df_all_exps = []\n",
    "social_fight_df_all_exps = []\n",
    "social_patchinfo_df_all_exps = []\n",
    "social_patch_df_all_exps = []\n",
    "\n",
    "pbar = tqdm(\n",
    "    [experiments[i] for i in [0, 1, 4, 5]],\n",
    "    desc=\"Loading experiments\",\n",
    "    unit=\"experiment\",\n",
    ")\n",
    "for exp in pbar:\n",
    "    data = load_experiment_data(\n",
    "        experiment=exp,\n",
    "        data_dir=data_dir,\n",
    "        periods=[\"social\"],\n",
    "        data_types=[\"retreat\", \"fight\", \"patchinfo\", \"patch\"],\n",
    "        # trim_days=1  # Optional: trim\n",
    "    )\n",
    "    df_retreat = data[\"social_retreat\"]\n",
    "    df_fight = data[\"social_fight\"]\n",
    "    social_patchinfo_df = data[\"social_patchinfo\"]\n",
    "    social_patch_df = data[\"social_patch\"]\n",
    "    df_retreat[\"experiment_name\"] = exp[\"name\"]\n",
    "    df_fight[\"experiment_name\"] = exp[\"name\"]\n",
    "    social_retreat_df_all_exps.append(df_retreat)\n",
    "    social_fight_df_all_exps.append(df_fight)\n",
    "    social_patchinfo_df_all_exps.append(social_patchinfo_df)\n",
    "    social_patch_df_all_exps.append(social_patch_df)\n",
    "\n",
    "social_retreat_df_all_exps = pd.concat(social_retreat_df_all_exps, ignore_index=True)\n",
    "social_fight_df_all_exps = pd.concat(social_fight_df_all_exps, ignore_index=True)\n",
    "social_patchinfo_df_all_exps = pd.concat(\n",
    "    social_patchinfo_df_all_exps, ignore_index=True\n",
    ")\n",
    "social_patch_df_all_exps = pd.concat(social_patch_df_all_exps, ignore_index=True)\n",
    "\n",
    "tube_test_data = {\n",
    "    \"social0.2-aeon3\": {\n",
    "        \"BAA-1104045\": {\"pre\": 2, \"post\": 1},\n",
    "        \"BAA-1104047\": {\"pre\": 8, \"post\": 9},\n",
    "    },\n",
    "    \"social0.2-aeon4\": {\n",
    "        \"BAA-1104048\": {\"pre\": 7, \"post\": 8},\n",
    "        \"BAA-1104049\": {\"pre\": 3, \"post\": 2},\n",
    "    },\n",
    "    \"social0.4-aeon3\": {\n",
    "        \"BAA-1104794\": {\"pre\": 4, \"post\": 2},\n",
    "        \"BAA-1104792\": {\"pre\": 12, \"post\": 13},\n",
    "    },\n",
    "    \"social0.4-aeon4\": {\n",
    "        \"BAA-1104795\": {\"pre\": 10, \"post\": 12},\n",
    "        \"BAA-1104797\": {\"pre\": 4, \"post\": 3},\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616d10c7",
   "metadata": {},
   "source": [
    "**Tube test results, for reference:**\n",
    "\n",
    "**SOCIAL 0.2**\n",
    "\n",
    "_Pre-social tube test results:_\n",
    "- BAA-1104045: 2, BAA-1104047: 8\n",
    "- BAA-1104048: 7, BAA-1104049: 3\n",
    "\n",
    "_Post-social tube test results:_\n",
    "- BAA-1104045: 1, BAA-1104047: 9\n",
    "- BAA-1104048: 8, BAA-1104049: 2\n",
    "\n",
    "**SOCIAL 0.4**\n",
    "\n",
    "_Pre-social tube test results:_\n",
    "- BAA-1104795: 10, BAA-1104797: 4\n",
    "- BAA-1104792: 4, BAA-1104794: 12\n",
    "\n",
    "_Post-social tube test results:_\n",
    "- BAA-1104795: 12, BAA-1104797: 3\n",
    "- BAA-1104792: 2, BAA-1104794: 13"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810aed11",
   "metadata": {},
   "source": [
    "### 1. Fights and retreats raster plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20160e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build lookup of social_start & compute per-exp offset to align at earliest start-time\n",
    "exp_df = pd.DataFrame(experiments)\n",
    "exp_df[\"social_start\"] = pd.to_datetime(exp_df[\"social_start\"])\n",
    "exp_df[\"tod_sec\"] = (\n",
    "    exp_df[\"social_start\"].dt.hour * 3600\n",
    "    + exp_df[\"social_start\"].dt.minute * 60\n",
    "    + exp_df[\"social_start\"].dt.second\n",
    ")\n",
    "\n",
    "# Earliest start-time (seconds since midnight)\n",
    "min_tod = exp_df[\"tod_sec\"].min()\n",
    "\n",
    "# Offset seconds so each experiment’s Day 0 lines up at min_tod\n",
    "exp_df[\"offset_sec\"] = exp_df[\"tod_sec\"] - min_tod\n",
    "\n",
    "# Compute absolute baseline timestamp per experiment\n",
    "exp_df[\"baseline_ts\"] = exp_df[\"social_start\"] - pd.to_timedelta(\n",
    "    exp_df[\"offset_sec\"], unit=\"s\"\n",
    ")\n",
    "exp_df = exp_df[[\"name\", \"baseline_ts\"]]\n",
    "\n",
    "# Keep original experiment order\n",
    "exp_order = [e[\"name\"] for e in experiments]\n",
    "\n",
    "dark_color = \"#555555\"\n",
    "\n",
    "# Dataframes and titles to plot\n",
    "raster_configs = [\n",
    "    (social_fight_df_all_exps, \"Raster Plot: Social Fights\", \"fights\"),\n",
    "    (social_retreat_df_all_exps, \"Raster Plot: Social Retreats\", \"retreats\"),\n",
    "]\n",
    "\n",
    "# Print aligned Day 0 hour (earliest start-time)\n",
    "aligned_hour = min_tod // 3600\n",
    "print(f\"Aligned Day 0 starts at hour {aligned_hour}\")\n",
    "\n",
    "# Compute global end time across both datasets\n",
    "all_events = pd.concat(\n",
    "    [\n",
    "        social_fight_df_all_exps[[\"experiment_name\", \"start_timestamp\"]],\n",
    "        social_retreat_df_all_exps[[\"experiment_name\", \"start_timestamp\"]],\n",
    "    ]\n",
    ")\n",
    "all_events[\"start_timestamp\"] = pd.to_datetime(all_events[\"start_timestamp\"])\n",
    "\n",
    "# Merge baseline timestamps\n",
    "all_events = all_events.merge(\n",
    "    exp_df, left_on=\"experiment_name\", right_on=\"name\", how=\"left\"\n",
    ")\n",
    "\n",
    "# Compute seconds since baseline\n",
    "all_events[\"rel_sec\"] = (\n",
    "    all_events[\"start_timestamp\"] - all_events[\"baseline_ts\"]\n",
    ").dt.total_seconds()\n",
    "\n",
    "# Find last event\n",
    "last = all_events.loc[all_events[\"rel_sec\"].idxmax()]\n",
    "\n",
    "# Compute end day and hour\n",
    "end_day = int(last[\"rel_sec\"] // 86400)\n",
    "end_hour = int((last[\"rel_sec\"] % 86400) // 3600)\n",
    "print(f\"Global end at Day {end_day}, hour {end_hour}\")\n",
    "\n",
    "# Plot each raster\n",
    "for df, title, behavior in raster_configs:\n",
    "    df2 = df[[\"experiment_name\", \"start_timestamp\"]].copy()\n",
    "    df2[\"start_timestamp\"] = pd.to_datetime(df2[\"start_timestamp\"])\n",
    "    df2 = df2.merge(exp_df, left_on=\"experiment_name\", right_on=\"name\", how=\"left\")\n",
    "\n",
    "    # Compute rel_days\n",
    "    df2[\"rel_days\"] = (\n",
    "        df2[\"start_timestamp\"] - df2[\"baseline_ts\"]\n",
    "    ).dt.total_seconds() / 86400.0\n",
    "\n",
    "    # Enforce experiment ordering\n",
    "    df2[\"experiment_name\"] = pd.Categorical(\n",
    "        df2[\"experiment_name\"], categories=exp_order, ordered=True\n",
    "    )\n",
    "\n",
    "    # Draw\n",
    "    fig = go.Figure(\n",
    "        go.Scatter(\n",
    "            x=df2[\"rel_days\"],\n",
    "            y=df2[\"experiment_name\"],\n",
    "            mode=\"markers\",\n",
    "            marker=dict(symbol=\"line-ns\", color=dark_color, size=8, line_width=1.2),\n",
    "            showlegend=False,\n",
    "            hovertemplate=\"Day: %{x:.2f}<br>Experiment: %{y}<extra></extra>\",\n",
    "        )\n",
    "    )\n",
    "    fig.update_yaxes(\n",
    "        autorange=\"reversed\",\n",
    "        title_text=\"Experiment\",\n",
    "        showgrid=False,\n",
    "        zeroline=False,\n",
    "        showline=False,\n",
    "        ticks=\"\",\n",
    "    )\n",
    "    fig.update_xaxes(\n",
    "        title_text=\"Days since aligned Day 0\",\n",
    "        tick0=0,\n",
    "        dtick=1,\n",
    "        tickformat=\".0f\",\n",
    "        showgrid=False,\n",
    "        zeroline=False,\n",
    "        showline=True,\n",
    "        linecolor=\"black\",\n",
    "    )\n",
    "    fig.update_layout(\n",
    "        template=\"simple_white\",\n",
    "        margin=dict(l=120, r=20, t=60, b=40),\n",
    "        height=100 + 20 * len(exp_order),\n",
    "        title=dict(text=title, x=0.5),\n",
    "    )\n",
    "\n",
    "    # pio.write_image(\n",
    "    #     fig,\n",
    "    #     save_dir / f\"{behavior}_raster.svg\",\n",
    "    #     format=\"svg\"\n",
    "    # )\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2cbd104",
   "metadata": {},
   "source": [
    "### 2. Proportion of retreat event wins over time during social period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbbb41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOT SMOOTHED\n",
    "# Ensure 'end_timestamp' is datetime and extract 'date'\n",
    "social_retreat_df_all_exps[\"end_timestamp\"] = pd.to_datetime(\n",
    "    social_retreat_df_all_exps[\"end_timestamp\"]\n",
    ")\n",
    "social_retreat_df_all_exps[\"date\"] = social_retreat_df_all_exps[\"end_timestamp\"].dt.date\n",
    "\n",
    "# Loop over each experiment_name’s subset and build one plot per experiment\n",
    "for exp_name, df_group in social_retreat_df_all_exps.groupby(\"experiment_name\"):\n",
    "    # Compute daily win‐counts per mouse\n",
    "    wins = df_group.groupby([\"date\", \"winner_identity\"]).size().unstack(fill_value=0)\n",
    "    # Turn counts into proportions\n",
    "    proportions = wins.div(wins.sum(axis=1), axis=0).reset_index()\n",
    "\n",
    "    # Melt to long form for plotting\n",
    "    df_long = proportions.melt(\n",
    "        id_vars=\"date\", var_name=\"winner_identity\", value_name=\"proportion\"\n",
    "    )\n",
    "\n",
    "    # Create and show one figure for this experiment\n",
    "    fig = px.line(\n",
    "        df_long,\n",
    "        x=\"date\",\n",
    "        y=\"proportion\",\n",
    "        color=\"winner_identity\",\n",
    "        markers=True,\n",
    "        title=f\"Experiment: {exp_name}<br>Daily Proportion of Victories per Mouse\",\n",
    "    )\n",
    "    fig.update_layout(\n",
    "        xaxis_title=\"Date\",\n",
    "        yaxis_title=\"Proportion of Victories\",\n",
    "        legend_title=\"Mouse ID\",\n",
    "    )\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daaa025b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SMOOTHED\n",
    "# Parameters\n",
    "BIN_HOURS = 6  # how many hours per bin (e.g. 6, 8, 10, 12, etc.)\n",
    "SMOOTH_DAYS = 3  # how many days to smooth over (e.g. 3, 5, 7, etc.)\n",
    "\n",
    "# Ensure 'end_timestamp' is datetime\n",
    "social_retreat_df_all_exps[\"end_timestamp\"] = pd.to_datetime(\n",
    "    social_retreat_df_all_exps[\"end_timestamp\"]\n",
    ")\n",
    "\n",
    "# Loop over each experiment_name’s subset and build one plot per experiment\n",
    "for exp_name, df_group in social_retreat_df_all_exps.groupby(\"experiment_name\"):\n",
    "    df_group = df_group.copy()\n",
    "\n",
    "    # Compute bin start for each timestamp\n",
    "    earliest_midnight = df_group[\"end_timestamp\"].dt.normalize().min()\n",
    "    bin_size = pd.Timedelta(hours=BIN_HOURS)\n",
    "    offsets = df_group[\"end_timestamp\"] - earliest_midnight\n",
    "    n_bins = (offsets // bin_size).astype(int)\n",
    "    df_group[\"bin_start\"] = earliest_midnight + n_bins * bin_size\n",
    "\n",
    "    # Compute win-counts per bin per mouse\n",
    "    wins = (\n",
    "        df_group.groupby([\"bin_start\", \"winner_identity\"]).size().unstack(fill_value=0)\n",
    "    )\n",
    "\n",
    "    # Drop bins with zero total wins\n",
    "    wins = wins[wins.sum(axis=1) > 0]\n",
    "\n",
    "    # Turn counts into proportions\n",
    "    proportions = wins.div(wins.sum(axis=1), axis=0).reset_index()\n",
    "\n",
    "    # Melt to long form for plotting\n",
    "    df_long = proportions.melt(\n",
    "        id_vars=\"bin_start\", var_name=\"winner_identity\", value_name=\"proportion\"\n",
    "    )\n",
    "\n",
    "    # Compute moving average window size (in bins)\n",
    "    window_size = int((SMOOTH_DAYS * 24) / BIN_HOURS)\n",
    "    window_size = max(window_size, 1)\n",
    "\n",
    "    # Apply centered moving average per mouse\n",
    "    df_long = df_long.sort_values([\"winner_identity\", \"bin_start\"])\n",
    "    df_long[\"smoothed_prop\"] = df_long.groupby(\"winner_identity\")[\n",
    "        \"proportion\"\n",
    "    ].transform(\n",
    "        lambda s: s.rolling(window=window_size, center=True, min_periods=1).mean()\n",
    "    )\n",
    "\n",
    "    # Create and show one figure for this experiment\n",
    "    fig = px.line(\n",
    "        df_long,\n",
    "        x=\"bin_start\",\n",
    "        y=\"smoothed_prop\",\n",
    "        color=\"winner_identity\",\n",
    "        markers=False,\n",
    "        title=(\n",
    "            f\"Experiment: {exp_name}<br>\"\n",
    "            f\"{BIN_HOURS}H Bins, {SMOOTH_DAYS}‐Day Moving Average\"\n",
    "        ),\n",
    "    )\n",
    "    fig.update_layout(\n",
    "        xaxis_title=f\"Bin Start (every {BIN_HOURS} hours from {earliest_midnight.date()})\",\n",
    "        yaxis_title=f\"{SMOOTH_DAYS}‐Day MA of Win Proportion\",\n",
    "        legend_title=\"Mouse ID\",\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "    # if exp_name == experiments[-1]['name']:\n",
    "    # pio.write_image(\n",
    "    #     fig,\n",
    "    #     str(save_dir / f\"social_retreat_{BIN_HOURS}h_bins_{SMOOTH_DAYS}day_smooth.svg\"),\n",
    "    #     format=\"svg\"\n",
    "    # )\n",
    "\n",
    "# Compute average number of retreat events per experiment-day\n",
    "social_retreat_df_all_exps[\"start_timestamp\"] = pd.to_datetime(\n",
    "    social_retreat_df_all_exps[\"start_timestamp\"]\n",
    ")\n",
    "# Extract day from timestamp\n",
    "social_retreat_df_all_exps[\"day\"] = social_retreat_df_all_exps[\n",
    "    \"start_timestamp\"\n",
    "].dt.floor(\"D\")\n",
    "# Count events per (experiment, day)\n",
    "daily_counts = social_retreat_df_all_exps.groupby([\"experiment_name\", \"day\"]).size()\n",
    "# Compute global average\n",
    "avg_events_per_day = daily_counts.mean()\n",
    "print(f\"Average number of retreat events per experiment-day: {avg_events_per_day:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf40095",
   "metadata": {},
   "source": [
    "### 3. Pre/during/post tube test comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b82a3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total win counts per subject during social phase\n",
    "social_counts_df = (\n",
    "    social_retreat_df_all_exps.groupby([\"experiment_name\", \"winner_identity\"])\n",
    "    .size()\n",
    "    .unstack(fill_value=0)\n",
    ")\n",
    "\n",
    "# Compute per-experiment summary metrics for dominant subject\n",
    "records = []\n",
    "for exp, subj_dict in tube_test_data.items():\n",
    "    soc_counts = social_counts_df.loc[exp]\n",
    "    total_pre = sum(d[\"pre\"] for d in subj_dict.values())\n",
    "    total_post = sum(d[\"post\"] for d in subj_dict.values())\n",
    "    total_soc = soc_counts.sum()\n",
    "\n",
    "    # Identify dominant by total wins across all phases\n",
    "    total_wins = {\n",
    "        subj: subj_dict[subj][\"pre\"] + soc_counts.get(subj, 0) + subj_dict[subj][\"post\"]\n",
    "        for subj in subj_dict\n",
    "    }\n",
    "    dominant = max(total_wins, key=total_wins.get)\n",
    "\n",
    "    # Compute win rates for dominant subject\n",
    "    pre_rate = subj_dict[dominant][\"pre\"] / total_pre\n",
    "    social_rate = soc_counts[dominant] / total_soc\n",
    "    post_rate = subj_dict[dominant][\"post\"] / total_post\n",
    "    baseline_rate = (subj_dict[dominant][\"pre\"] + subj_dict[dominant][\"post\"]) / (\n",
    "        total_pre + total_post\n",
    "    )\n",
    "\n",
    "    records.append(\n",
    "        {\n",
    "            \"experiment\": exp,\n",
    "            \"dominant\": dominant,\n",
    "            \"pre_rate\": pre_rate,\n",
    "            \"social_rate\": social_rate,\n",
    "            \"post_rate\": post_rate,\n",
    "            \"baseline_rate\": baseline_rate,\n",
    "        }\n",
    "    )\n",
    "\n",
    "summary_df = pd.DataFrame(records)\n",
    "\n",
    "# Test: is social-phase rate higher than pre+post (Wilcoxon signed-rank)\n",
    "stat, p_value = wilcoxon(summary_df[\"baseline_rate\"], summary_df[\"social_rate\"])\n",
    "print(f\"Wilcoxon signed-rank: W = {stat:.3f}, p-value = {p_value:.3f}\\n\")\n",
    "\n",
    "# Per-experiment plotting and binomial tests\n",
    "x_positions = [0, 1, 2]\n",
    "x_labels = [\"Tube test (pre)\", \"Two weeks foraging\", \"Tube test (post)\"]\n",
    "\n",
    "for experiment, subjects in tube_test_data.items():\n",
    "    ids = list(subjects.keys())\n",
    "    pre_scores = {s: subjects[s][\"pre\"] for s in ids}\n",
    "    post_scores = {s: subjects[s][\"post\"] for s in ids}\n",
    "    soc_counts = social_counts_df.loc[experiment]\n",
    "\n",
    "    # Identify dominant and subordinate\n",
    "    dominant = summary_df.loc[summary_df[\"experiment\"] == experiment, \"dominant\"].iloc[\n",
    "        0\n",
    "    ]\n",
    "    subordinate = [s for s in ids if s != dominant][0]\n",
    "\n",
    "    # Win/loss counts\n",
    "    k_pre = pre_scores[dominant]\n",
    "    n_pre = k_pre + pre_scores[subordinate]\n",
    "    k_post = post_scores[dominant]\n",
    "    n_post = k_post + post_scores[subordinate]\n",
    "    k_social = soc_counts[dominant]\n",
    "    n_social = soc_counts.sum()\n",
    "\n",
    "    # Binomial tests (one-sided: dominant > 50%)\n",
    "    k_pool = k_pre + k_post\n",
    "    n_pool = n_pre + n_post\n",
    "    p_pool = binomtest(k_pool, n_pool, p=0.5, alternative=\"greater\").pvalue\n",
    "    p_social = binomtest(k_social, n_social, p=0.5, alternative=\"greater\").pvalue\n",
    "\n",
    "    # Print results\n",
    "    print(f\"\\n=== {experiment} ({dominant} dominant) ===\")\n",
    "    print(f\"Pooled Pre+Post: wins={k_pool}/{n_pool} → binom p={p_pool:.3f}\")\n",
    "    print(f\"Social:          wins={k_social}/{n_social} → binom p={p_social:.3f}\")\n",
    "\n",
    "    # Plot win rates for both mice\n",
    "    proportions = {\n",
    "        dominant: {\n",
    "            \"pre\": k_pre / n_pre,\n",
    "            \"social\": k_social / n_social,\n",
    "            \"post\": k_post / n_post,\n",
    "            \"color\": \"gold\",\n",
    "        },\n",
    "        subordinate: {\n",
    "            \"pre\": pre_scores[subordinate] / n_pre,\n",
    "            \"social\": soc_counts[subordinate] / n_social,\n",
    "            \"post\": post_scores[subordinate] / n_post,\n",
    "            \"color\": \"brown\",\n",
    "        },\n",
    "    }\n",
    "\n",
    "    fig = go.Figure()\n",
    "    for subj, vals in proportions.items():\n",
    "        for i, x in enumerate(x_positions):\n",
    "            y = [vals[\"pre\"], vals[\"social\"], vals[\"post\"]][i]\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=[x, x],\n",
    "                    y=[0, y],\n",
    "                    mode=\"lines+markers\",\n",
    "                    name=subj,\n",
    "                    line=dict(color=vals[\"color\"]),\n",
    "                    showlegend=(i == 0),\n",
    "                )\n",
    "            )\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=experiment,\n",
    "        plot_bgcolor=\"white\",\n",
    "        width=400,\n",
    "        height=400,\n",
    "        xaxis=dict(tickvals=x_positions, ticktext=x_labels, range=[-0.5, 2.5]),\n",
    "        yaxis=dict(\n",
    "            title=\"Proportion of victories\",\n",
    "            range=[0, 1],\n",
    "            ticks=\"outside\",\n",
    "            ticklen=5,\n",
    "            tickwidth=2,\n",
    "            showline=True,\n",
    "            linecolor=\"black\",\n",
    "        ),\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "    # if experiment == experiments[-1]['name']:\n",
    "    #     pio.write_image(\n",
    "    #         fig,\n",
    "    #         str(save_dir / f\"tube_test_win_rates.svg\"),\n",
    "    #         format=\"svg\"\n",
    "    #     )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae44664",
   "metadata": {},
   "source": [
    "### 4. Dominance summary plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ac5149",
   "metadata": {},
   "outputs": [],
   "source": [
    "social_win_proportions = (\n",
    "    social_retreat_df_all_exps.groupby([\"experiment_name\", \"winner_identity\"])\n",
    "    .size()\n",
    "    .unstack(fill_value=0)\n",
    ")\n",
    "social_win_proportions = social_win_proportions.div(\n",
    "    social_win_proportions.sum(axis=1), axis=0\n",
    ")\n",
    "\n",
    "# Define plot\n",
    "fig = go.Figure()\n",
    "legend_labels_added = {\"gold\": False, \"brown\": False}\n",
    "\n",
    "for experiment, subjects in tube_test_data.items():\n",
    "    ids = list(subjects.keys())\n",
    "    pre_scores = {subj: subjects[subj][\"pre\"] for subj in ids}\n",
    "    post_scores = {subj: subjects[subj][\"post\"] for subj in ids}\n",
    "\n",
    "    pre_dominant = max(pre_scores, key=pre_scores.get)\n",
    "    post_dominant = max(post_scores, key=post_scores.get)\n",
    "\n",
    "    if experiment in social_win_proportions.index:\n",
    "        social_scores = social_win_proportions.loc[experiment]\n",
    "        social_dominant = social_scores.idxmax()\n",
    "    else:\n",
    "        raise ValueError(f\"No social data for {experiment}\")\n",
    "\n",
    "    if len({pre_dominant, post_dominant, social_dominant}) != 1:\n",
    "        raise ValueError(\n",
    "            f\"Inconsistent dominant subject in {experiment}: pre={pre_dominant}, social={social_dominant}, post={post_dominant}\"\n",
    "        )\n",
    "\n",
    "    dominant = pre_dominant\n",
    "    subordinate = [s for s in ids if s != dominant][0]\n",
    "\n",
    "    total_pre = pre_scores[dominant] + pre_scores[subordinate]\n",
    "    total_post = post_scores[dominant] + post_scores[subordinate]\n",
    "\n",
    "    social_dom = (\n",
    "        social_win_proportions.at[experiment, dominant]\n",
    "        if dominant in social_win_proportions.columns\n",
    "        else 0\n",
    "    )\n",
    "    social_sub = (\n",
    "        social_win_proportions.at[experiment, subordinate]\n",
    "        if subordinate in social_win_proportions.columns\n",
    "        else 0\n",
    "    )\n",
    "\n",
    "    proportions = {\n",
    "        dominant: {\n",
    "            \"pre\": -pre_scores[dominant] / total_pre,\n",
    "            \"post\": post_scores[dominant] / total_post,\n",
    "            \"social\": -social_dom,\n",
    "            \"color\": \"gold\",\n",
    "            \"legend\": \"Dominant\",\n",
    "        },\n",
    "        subordinate: {\n",
    "            \"pre\": -pre_scores[subordinate] / total_pre,\n",
    "            \"post\": post_scores[subordinate] / total_post,\n",
    "            \"social\": -social_sub,\n",
    "            \"color\": \"brown\",\n",
    "            \"legend\": \"Subordinate\",\n",
    "        },\n",
    "    }\n",
    "\n",
    "    for subj, vals in proportions.items():\n",
    "        color = vals[\"color\"]\n",
    "        legend_name = vals[\"legend\"] if not legend_labels_added[color] else None\n",
    "        legend_labels_added[color] = True\n",
    "\n",
    "        # Pre point\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=[vals[\"pre\"]],\n",
    "                y=[vals[\"social\"]],\n",
    "                mode=\"markers\",\n",
    "                marker=dict(color=color, size=10),\n",
    "                name=legend_name,\n",
    "                showlegend=legend_name is not None,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Post point\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=[vals[\"post\"]],\n",
    "                y=[-vals[\"social\"]],\n",
    "                mode=\"markers\",\n",
    "                marker=dict(color=color, size=10),\n",
    "                name=legend_name,\n",
    "                showlegend=False,\n",
    "            )\n",
    "        )\n",
    "\n",
    "# Final layout\n",
    "# Hide everything\n",
    "fig.update_xaxes(\n",
    "    showgrid=False, showline=False, zeroline=False, showticklabels=False, ticks=\"\"\n",
    ")\n",
    "fig.update_yaxes(\n",
    "    showgrid=False, showline=False, zeroline=False, showticklabels=False, ticks=\"\"\n",
    ")\n",
    "\n",
    "# Draw the two axes as black lines\n",
    "fig.add_shape(type=\"line\", x0=-1, x1=1, y0=0, y1=0, line=dict(color=\"black\", width=1))\n",
    "fig.add_shape(type=\"line\", x0=0, x1=0, y0=-1, y1=1, line=dict(color=\"black\", width=1))\n",
    "\n",
    "# Draw little ticks at ±1\n",
    "tick_len = 0.02\n",
    "for t in (-1, 1):\n",
    "    # x‐axis tick\n",
    "    fig.add_shape(\n",
    "        type=\"line\",\n",
    "        x0=t,\n",
    "        x1=t,\n",
    "        y0=-tick_len,\n",
    "        y1=+tick_len,\n",
    "        line=dict(color=\"black\", width=1),\n",
    "    )\n",
    "    # y‐axis tick\n",
    "    fig.add_shape(\n",
    "        type=\"line\",\n",
    "        x0=-tick_len,\n",
    "        x1=+tick_len,\n",
    "        y0=t,\n",
    "        y1=t,\n",
    "        line=dict(color=\"black\", width=1),\n",
    "    )\n",
    "\n",
    "# Annotate labels at ±1\n",
    "for t, txt in [(-1, \"−1\"), (1, \"1\")]:\n",
    "    # x‐axis label\n",
    "    fig.add_annotation(\n",
    "        x=t,\n",
    "        y=0,\n",
    "        text=txt,\n",
    "        yshift=-16,  # move it down in px\n",
    "        showarrow=False,\n",
    "        font=dict(size=14),\n",
    "    )\n",
    "    # y‐axis label\n",
    "    fig.add_annotation(\n",
    "        x=0,\n",
    "        y=t,\n",
    "        text=txt,\n",
    "        xshift=16,  # move it left in px\n",
    "        showarrow=False,\n",
    "        font=dict(size=14),\n",
    "    )\n",
    "\n",
    "# Finally, re-add title & legend layout\n",
    "fig.update_layout(\n",
    "    title=\"Pre vs Post Tube Test (Reflected by Social Proportion)\",\n",
    "    plot_bgcolor=\"white\",\n",
    "    width=700,\n",
    "    height=700,\n",
    "    legend=dict(title=\"Subject role\", orientation=\"v\", x=1.02, y=1),\n",
    ")\n",
    "\n",
    "# Draw y=x as a dashed grey line (behind the points)\n",
    "fig.add_shape(\n",
    "    type=\"line\",\n",
    "    x0=-1,\n",
    "    y0=-1,\n",
    "    x1=1,\n",
    "    y1=1,\n",
    "    line=dict(color=\"lightgrey\", width=1, dash=\"dash\"),\n",
    "    layer=\"below\",\n",
    ")\n",
    "\n",
    "# Re-add title, legend, and now axis titles\n",
    "fig.update_layout(\n",
    "    title=\"Pre vs Post Tube Test (Reflected by Social Proportion)\",\n",
    "    xaxis_title=\"Tube test victory proportion (−pre, +post)\",\n",
    "    yaxis_title=\"Social phase proportion (−plotted pre, +plotted post)\",\n",
    "    plot_bgcolor=\"white\",\n",
    "    width=700,\n",
    "    height=700,\n",
    "    legend=dict(title=\"Subject role\", orientation=\"v\", x=1.02, y=1),\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# pio.write_image(\n",
    "#     fig,\n",
    "#     str(save_dir / \"tube_test_scatter.svg\"),\n",
    "#     format=\"svg\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5d3ffd",
   "metadata": {},
   "source": [
    "### 5. Dominant vs subordinate comparison of time spent and distance spun at best patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc501f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = social_patch_df_all_exps.merge(\n",
    "    social_patchinfo_df_all_exps[\n",
    "        [\"experiment_name\", \"block_start\", \"patch_name\", \"patch_rate\"]\n",
    "    ],\n",
    "    on=[\"experiment_name\", \"block_start\", \"patch_name\"],\n",
    "    how=\"left\",\n",
    ").assign(dummy=lambda x: x[\"patch_name\"].str.contains(\"dummy\", case=False))\n",
    "\n",
    "# Keep only blocks where non-dummy patches have exactly 3 different rates\n",
    "df = df[\n",
    "    df.groupby([\"experiment_name\", \"block_start\"])[\"patch_rate\"].transform(\n",
    "        lambda s: s[~df.loc[s.index, \"dummy\"]].nunique() == 3\n",
    "    )\n",
    "]\n",
    "\n",
    "# Create a ranking only for non-dummy patches\n",
    "non_dummy_ranks = (\n",
    "    df[~df[\"dummy\"]]\n",
    "    .groupby([\"experiment_name\", \"block_start\"])[\"patch_rate\"]\n",
    "    .rank(method=\"dense\")\n",
    ")\n",
    "\n",
    "# Add the ranks back to the full dataframe\n",
    "df[\"patch_rank\"] = np.nan\n",
    "df.loc[~df[\"dummy\"], \"patch_rank\"] = non_dummy_ranks\n",
    "\n",
    "# Assign difficulty (rank 1=hard, rank 3=easy, rank 2=medium)\n",
    "df[\"patch_difficulty\"] = np.where(\n",
    "    df[\"dummy\"],\n",
    "    \"dummy\",\n",
    "    np.where(\n",
    "        df[\"patch_rank\"] == 1, \"hard\", np.where(df[\"patch_rank\"] == 3, \"easy\", \"medium\")\n",
    "    ),\n",
    ")\n",
    "\n",
    "df = df.drop(columns=[\"patch_rank\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6757b832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute social‐win proportions\n",
    "swp = (\n",
    "    social_retreat_df_all_exps.groupby([\"experiment_name\", \"winner_identity\"])\n",
    "    .size()\n",
    "    .unstack(fill_value=0)\n",
    ")\n",
    "swp = swp.div(swp.sum(axis=1), axis=0)\n",
    "\n",
    "# Build the dominance DataFrame in one comprehension\n",
    "dominance_df = pd.DataFrame(\n",
    "    [\n",
    "        {\n",
    "            \"experiment_name\": exp,\n",
    "            \"dominant\": dom,\n",
    "            \"subordinate\": next(s for s in subs if s != dom),\n",
    "        }\n",
    "        for exp, subs in tube_test_data.items()\n",
    "        if exp in swp.index\n",
    "        # pick the pre‐tube top scorer…\n",
    "        for dom in [max(subs, key=lambda s: subs[s][\"pre\"])]\n",
    "        # …only keep if post‐tube and social‐win agree\n",
    "        if dom == max(subs, key=lambda s: subs[s][\"post\"]) == swp.loc[exp].idxmax()\n",
    "    ]\n",
    ")\n",
    "\n",
    "dominance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe1f64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute per‐subject, per‐block easy‐time fraction, but only keep blocks ≥1 min\n",
    "time_df = (\n",
    "    df.groupby([\"experiment_name\", \"block_start\", \"subject_name\", \"patch_difficulty\"])[\n",
    "        \"in_patch_time\"\n",
    "    ]\n",
    "    .sum()\n",
    "    .unstack(\"patch_difficulty\", fill_value=0)\n",
    "    .assign(total_time=lambda d: d.sum(axis=1))\n",
    "    # filter out blocks with < 60 s total patch time\n",
    "    .loc[lambda d: d[\"total_time\"] >= 60]\n",
    "    .assign(easy_ratio=lambda d: d[\"easy\"] / d[\"total_time\"])\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Merge in dominant/subordinate labels and tag role\n",
    "time_df = time_df.merge(dominance_df, on=\"experiment_name\", how=\"left\").assign(\n",
    "    role=lambda d: np.where(\n",
    "        d[\"subject_name\"] == d[\"dominant\"], \"dominant\", \"subordinate\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Box‐plot of easy_ratio by role\n",
    "fig = px.box(\n",
    "    time_df,\n",
    "    x=\"role\",\n",
    "    y=\"easy_ratio\",\n",
    "    points=\"all\",\n",
    "    category_orders={\"role\": [\"dominant\", \"subordinate\"]},\n",
    "    title=\"Relative time in easy patch by role (≥ 1 min total time spent in patches per block)\",\n",
    ")\n",
    "fig.update_yaxes(title=\"Easy‐patch time / total patch time\")\n",
    "fig.update_xaxes(title=\"\")\n",
    "fig.show()\n",
    "\n",
    "# Print the medians\n",
    "medians = time_df.groupby(\"role\")[\"easy_ratio\"].median()\n",
    "print(\"Median easy patch time ratio by role:\")\n",
    "for role, median in medians.items():\n",
    "    print(f\"{role}: {median:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0bdaa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get per‐subject, per‐block time by difficulty\n",
    "time_df = (\n",
    "    df.groupby([\"experiment_name\", \"block_start\", \"subject_name\", \"patch_difficulty\"])[\n",
    "        \"in_patch_time\"\n",
    "    ]\n",
    "    .sum()\n",
    "    .unstack(\"patch_difficulty\", fill_value=0)\n",
    "    # drop any block‐subject that didn’t spend ≥10 s in each of hard/medium/easy\n",
    "    .loc[lambda d: (d[[\"hard\", \"medium\", \"easy\"]] >= 10).all(axis=1)]\n",
    "    .assign(\n",
    "        total_time=lambda d: d.sum(axis=1),\n",
    "        easy_ratio=lambda d: d[\"easy\"] / d[\"total_time\"],\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Merge in dominant/subordinate and label role\n",
    "time_df = time_df.merge(dominance_df, on=\"experiment_name\", how=\"left\").assign(\n",
    "    role=lambda d: np.where(\n",
    "        d[\"subject_name\"] == d[\"dominant\"], \"dominant\", \"subordinate\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Box‐plot of easy_ratio by role\n",
    "fig = px.box(\n",
    "    time_df,\n",
    "    x=\"role\",\n",
    "    y=\"easy_ratio\",\n",
    "    points=\"all\",\n",
    "    category_orders={\"role\": [\"dominant\", \"subordinate\"]},\n",
    "    title=\"Easy‐patch fraction by role (≥10 s in each non‐dummy patch)\",\n",
    ")\n",
    "fig.update_yaxes(title=\"Easy time / total patch time\")\n",
    "fig.update_xaxes(title=\"\")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e2356c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute true traveled distance for each patch‐visit, thresholding <1→0\n",
    "df2 = (\n",
    "    df.assign(\n",
    "        # sum of abs steps = true distance\n",
    "        traveled=lambda d: d[\"wheel_cumsum_distance_travelled\"].apply(\n",
    "            lambda arr: np.sum(np.abs(np.diff(arr))) if len(arr) > 1 else 0\n",
    "        )\n",
    "    )\n",
    "    # any tiny (<1) distance becomes 0\n",
    "    .assign(traveled=lambda d: d[\"traveled\"].mask(d[\"traveled\"] < 1, 0))\n",
    ")\n",
    "\n",
    "# Pivot to one row per subject‐block with columns [hard, medium, easy]\n",
    "dist_df = (\n",
    "    df2.groupby([\"experiment_name\", \"block_start\", \"subject_name\", \"patch_difficulty\"])[\n",
    "        \"traveled\"\n",
    "    ]\n",
    "    .sum()\n",
    "    .unstack(\"patch_difficulty\", fill_value=0)\n",
    "    .assign(\n",
    "        total_dist=lambda d: d.sum(axis=1),\n",
    "        easy_ratio=lambda d: d[\"easy\"] / d[\"total_dist\"],\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Merge in dominance & tag role\n",
    "dist_df = dist_df.merge(dominance_df, on=\"experiment_name\", how=\"left\").assign(\n",
    "    role=lambda d: np.where(\n",
    "        d[\"subject_name\"] == d[\"dominant\"], \"dominant\", \"subordinate\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Box‐plot of easy_ratio by role\n",
    "fig = px.box(\n",
    "    dist_df,\n",
    "    x=\"role\",\n",
    "    y=\"easy_ratio\",\n",
    "    points=\"all\",\n",
    "    category_orders={\"role\": [\"dominant\", \"subordinate\"]},\n",
    "    title=\"Easy‐patch distance fraction by role (tiny spins <1 set to 0)\",\n",
    ")\n",
    "fig.update_yaxes(title=\"Easy‐patch distance / total distance\")\n",
    "fig.update_xaxes(title=\"\")\n",
    "fig.show()\n",
    "\n",
    "# Print the medians\n",
    "medians = dist_df.groupby(\"role\")[\"easy_ratio\"].median()\n",
    "print(\"Median easy patch distance ratio by role:\")\n",
    "for role, median in medians.items():\n",
    "    print(f\"{role}: {median:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c741bf05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute true traveled distance per patch‐visit, tiny spins → 0\n",
    "df2 = df.assign(\n",
    "    traveled=lambda d: d[\"wheel_cumsum_distance_travelled\"].apply(\n",
    "        lambda arr: np.sum(np.abs(np.diff(arr))) if len(arr) > 1 else 0\n",
    "    )\n",
    ").assign(traveled=lambda d: d[\"traveled\"].mask(d[\"traveled\"] < 1, 0))\n",
    "\n",
    "# Pivot to one row per subject‐block, but only keep rows where hard,medium,easy >5\n",
    "dist_df = (\n",
    "    df2.groupby([\"experiment_name\", \"block_start\", \"subject_name\", \"patch_difficulty\"])[\n",
    "        \"traveled\"\n",
    "    ]\n",
    "    .sum()\n",
    "    .unstack(\"patch_difficulty\", fill_value=0)\n",
    "    .loc[lambda d: (d[[\"hard\", \"medium\", \"easy\"]] > 500).all(axis=1)]\n",
    "    .assign(\n",
    "        total_dist=lambda d: d.sum(axis=1),\n",
    "        easy_ratio=lambda d: d[\"easy\"] / d[\"total_dist\"],\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Merge in dominance & tag role\n",
    "dist_df = dist_df.merge(dominance_df, on=\"experiment_name\", how=\"left\").assign(\n",
    "    role=lambda d: np.where(\n",
    "        d[\"subject_name\"] == d[\"dominant\"], \"dominant\", \"subordinate\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Box‐plot of easy_ratio by role (dominant first)\n",
    "fig = px.box(\n",
    "    dist_df,\n",
    "    x=\"role\",\n",
    "    y=\"easy_ratio\",\n",
    "    points=\"all\",\n",
    "    category_orders={\"role\": [\"dominant\", \"subordinate\"]},\n",
    "    title=\"Easy‐patch distance fraction by role (all wheels >500cm)\",\n",
    ")\n",
    "fig.update_yaxes(title=\"Easy‐patch distance / total distance\")\n",
    "fig.update_xaxes(title=\"\")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1701ec22",
   "metadata": {},
   "source": [
    "## Patch preference plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c552421",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba180fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_experiment_data(\n",
    "    data_dir=data_dir,\n",
    "    data_types=[\"patch\", \"patchinfo\"],\n",
    ")\n",
    "\n",
    "patch_df = data[\"None_patch\"]\n",
    "patch_info_df = data[\"None_patchinfo\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3461ebf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def patch_info_df_to_dict(df: pd.DataFrame) -> dict:\n",
    "    \"\"\"Convert patch information DataFrame to nested dictionary format.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame containing patch information with columns ['experiment_name', 'block_start', 'patch_name', 'patch_rate', 'patch_offset']\n",
    "\n",
    "    Returns:\n",
    "    - Dictionary with experiments as keys and lists of patch entries as values\n",
    "    \"\"\"\n",
    "    result_dict = {}\n",
    "    for _, row in df.iterrows():\n",
    "        exp = row[\"experiment_name\"]\n",
    "\n",
    "        # Initialize experiment entry if it doesn't exist\n",
    "        if exp not in result_dict:\n",
    "            result_dict[exp] = []\n",
    "\n",
    "        # Create entry dictionary\n",
    "        entry = {\n",
    "            \"block_start\": row[\"block_start\"].to_pydatetime(),\n",
    "            \"patch_name\": row[\"patch_name\"],\n",
    "            \"patch_rate\": row[\"patch_rate\"],\n",
    "            \"patch_offset\": row[\"patch_offset\"],\n",
    "        }\n",
    "\n",
    "        # Add entry to experiment list\n",
    "        result_dict[exp].append(entry)\n",
    "\n",
    "    return result_dict\n",
    "\n",
    "\n",
    "def patch_df_to_dict(df: pd.DataFrame) -> dict:\n",
    "    \"\"\"Convert patch DataFrame to dictionary grouped by experiment.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame containing patch data with 'experiment_name' column\n",
    "\n",
    "    Returns:\n",
    "    - Dictionary with experiment names as keys and filtered DataFrames as values\n",
    "    \"\"\"\n",
    "    results_dict = {}\n",
    "    for experiment in df[\"experiment_name\"].unique():\n",
    "        # Filter the dataframe to only include rows for this experiment\n",
    "        experiment_df = df[df[\"experiment_name\"] == experiment].copy()\n",
    "\n",
    "        # Remove unwanted columns\n",
    "        experiment_df = experiment_df.drop(columns=[\"experiment_name\"])\n",
    "\n",
    "        # Assign the filtered DataFrame directly to the dictionary\n",
    "        results_dict[experiment] = experiment_df\n",
    "\n",
    "    return results_dict\n",
    "\n",
    "\n",
    "def get_first_half_social(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Extract the first half of social period data for each experiment.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame containing experiment data with 'period' and 'experiment_name' columns\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame containing first half of social data for all experiments\n",
    "    \"\"\"\n",
    "    # Get social data\n",
    "    social_data = df[df[\"period\"] == \"social\"]\n",
    "\n",
    "    # Group social data by experiment\n",
    "    social_by_exp = social_data.groupby(\"experiment_name\")\n",
    "\n",
    "    # For each experiment, take the first half of social data\n",
    "    half_social_data_list = []\n",
    "\n",
    "    for _exp_name, exp_data in social_by_exp:\n",
    "        # Sort by block_start to ensure chronological order\n",
    "        exp_data_sorted = exp_data.sort_values(\"block_start\")\n",
    "\n",
    "        # Calculate the half-point\n",
    "        half_point = len(exp_data_sorted) // 2\n",
    "\n",
    "        # Take the first half\n",
    "        first_half = exp_data_sorted.iloc[:half_point]\n",
    "\n",
    "        # Add to list\n",
    "        half_social_data_list.append(first_half)\n",
    "\n",
    "    # Combine all first halves into one dataframe\n",
    "    half_social_data = (\n",
    "        pd.concat(half_social_data_list)\n",
    "        if half_social_data_list\n",
    "        else pd.DataFrame(columns=social_data.columns)\n",
    "    )\n",
    "\n",
    "    return half_social_data\n",
    "\n",
    "\n",
    "block_subject_patch_data_social_combined = patch_df[\n",
    "    patch_df[\"period\"] == \"social\"\n",
    "].copy()\n",
    "block_subject_patch_data_social_combined.drop(columns=[\"period\"], inplace=True)\n",
    "block_subject_patch_data_social_dict = patch_df_to_dict(\n",
    "    block_subject_patch_data_social_combined\n",
    ")\n",
    "block_subject_patch_data_post_social_combined = patch_df[\n",
    "    patch_df[\"period\"] == \"postsocial\"\n",
    "].copy()\n",
    "block_subject_patch_data_post_social_combined.drop(columns=[\"period\"], inplace=True)\n",
    "block_subject_patch_data_post_social_dict = patch_df_to_dict(\n",
    "    block_subject_patch_data_post_social_combined\n",
    ")\n",
    "block_subject_patch_data_social_first_half_combined = get_first_half_social(\n",
    "    patch_df\n",
    ").copy()\n",
    "block_subject_patch_data_social_first_half_combined.drop(\n",
    "    columns=[\"period\"], inplace=True\n",
    ")\n",
    "block_subject_patch_data_social_first_half_dict = patch_df_to_dict(\n",
    "    block_subject_patch_data_social_first_half_combined\n",
    ")\n",
    "patch_info_dict = patch_info_df_to_dict(patch_info_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d138f46",
   "metadata": {},
   "source": [
    "### 1. Wheel distance spun per block, averaged by the number of mice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4e9149",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_subject_patch_data_social_combined[\"final_wheel_cumsum\"] = (\n",
    "    block_subject_patch_data_social_combined[\"wheel_cumsum_distance_travelled\"].apply(\n",
    "        lambda x: x[-1] if isinstance(x, np.ndarray) and len(x) > 0 else 0\n",
    "    )\n",
    ")\n",
    "wheel_total_dist_averaged_social = (\n",
    "    block_subject_patch_data_social_combined.groupby(\"block_start\")[\n",
    "        \"final_wheel_cumsum\"\n",
    "    ].sum()\n",
    "    / 2\n",
    ")\n",
    "wheel_total_dist_averaged_social = wheel_total_dist_averaged_social.reset_index()\n",
    "\n",
    "block_subject_patch_data_post_social_combined[\"final_wheel_cumsum\"] = (\n",
    "    block_subject_patch_data_post_social_combined[\n",
    "        \"wheel_cumsum_distance_travelled\"\n",
    "    ].apply(lambda x: x[-1] if isinstance(x, np.ndarray) and len(x) > 0 else 0)\n",
    ")\n",
    "wheel_total_dist_averaged_post_social = (\n",
    "    block_subject_patch_data_post_social_combined.groupby(\"block_start\")[\n",
    "        \"final_wheel_cumsum\"\n",
    "    ]\n",
    "    .sum()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "wheel_total_dist_averaged_social[\"condition\"] = \"social\"\n",
    "wheel_total_dist_averaged_post_social[\"condition\"] = \"post_social\"\n",
    "wheel_total_dist_averaged = pd.concat(\n",
    "    [wheel_total_dist_averaged_social, wheel_total_dist_averaged_post_social]\n",
    ")\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig = px.box(\n",
    "    wheel_total_dist_averaged,\n",
    "    x=\"condition\",\n",
    "    y=\"final_wheel_cumsum\",\n",
    "    points=\"all\",\n",
    "    title=\"Wheel Distance Spun Per Block Averaged By Number Of Subjects\",\n",
    "    labels={\"final_wheel_cumsum\": \"Wheel Distance Spun Per Block (cm)\"},\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd56f33",
   "metadata": {},
   "source": [
    "### 2. Number of patch switches by each mouse per block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f46b5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_patch_probabilities(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Compute patch probabilities based on block and subject data.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame containing block, subject, pellet, and patch data\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame with probabilities for each patch per pellet interval\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    # Precompute unique block-subject groups\n",
    "    grouped_data = df.groupby([\"block_start\", \"subject_name\"])\n",
    "\n",
    "    for (block_start, subject_name), block_data in grouped_data:\n",
    "        # Process pellet timestamps once\n",
    "        # Extract and ensure all pellet timestamps are float\n",
    "        all_pellet_timestamps = []\n",
    "        for sublist in block_data[\"pellet_timestamps\"]:\n",
    "            for ts in sublist:\n",
    "                try:\n",
    "                    all_pellet_timestamps.append(float(ts))\n",
    "                except (ValueError, TypeError):\n",
    "                    # Skip values that can't be converted to float\n",
    "                    pass\n",
    "\n",
    "        pellet_timestamps = np.sort(np.unique(all_pellet_timestamps))\n",
    "        if len(pellet_timestamps) < 2:\n",
    "            continue\n",
    "\n",
    "        # Create pellet intervals DataFrame\n",
    "        intervals_df = pd.DataFrame(\n",
    "            {\n",
    "                \"interval_start\": pellet_timestamps[:-1],\n",
    "                \"interval_end\": pellet_timestamps[1:],\n",
    "                \"pellet_number\": np.arange(1, len(pellet_timestamps)),\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Prepare a dict to hold in_patch_timestamps for each patch\n",
    "        patches_data = {}\n",
    "        for patch in block_data[\"patch_name\"].unique():\n",
    "            patch_data = block_data[block_data[\"patch_name\"] == patch]\n",
    "            if patch_data.shape[0] != 1:\n",
    "                raise ValueError(\n",
    "                    \"More than one row per block start, subject, patch combination.\"\n",
    "                )\n",
    "\n",
    "            # Get timestamps and ensure they're floats\n",
    "            in_patch_ts_raw = patch_data.iloc[0][\"in_patch_timestamps\"]\n",
    "            in_patch_timestamps = []\n",
    "            for ts in in_patch_ts_raw:\n",
    "                try:\n",
    "                    in_patch_timestamps.append(float(ts))\n",
    "                except (ValueError, TypeError):\n",
    "                    # Skip values that can't be converted to float\n",
    "                    pass\n",
    "\n",
    "            patches_data[patch] = np.sort(np.array(in_patch_timestamps))\n",
    "\n",
    "        # Initialize a DataFrame to store counts per patch\n",
    "        counts_df = intervals_df[[\"pellet_number\"]].copy()\n",
    "\n",
    "        # For each patch, compute counts within each interval using numpy searchsorted\n",
    "        for patch, in_patch_ts in patches_data.items():\n",
    "            counts = np.zeros(len(intervals_df), dtype=int)\n",
    "            if len(in_patch_ts) > 0:\n",
    "                # Convert to float arrays explicitly\n",
    "                in_patch_ts = in_patch_ts.astype(np.float64)\n",
    "                interval_starts = intervals_df[\"interval_start\"].values.astype(\n",
    "                    np.float64\n",
    "                )\n",
    "                interval_ends = intervals_df[\"interval_end\"].values.astype(np.float64)\n",
    "\n",
    "                idx_start = np.searchsorted(in_patch_ts, interval_starts, side=\"left\")\n",
    "                idx_end = np.searchsorted(in_patch_ts, interval_ends, side=\"right\")\n",
    "                counts = idx_end - idx_start\n",
    "            counts_df[f\"count_in_{patch}\"] = counts\n",
    "\n",
    "        # Compute total counts per interval\n",
    "        counts_df[\"total_counts\"] = counts_df.filter(like=\"count_in_\").sum(axis=1)\n",
    "\n",
    "        # Avoid division by zero\n",
    "        counts_df[\"total_counts\"] = counts_df[\"total_counts\"].replace(0, np.nan)\n",
    "\n",
    "        # Compute probabilities per interval\n",
    "        for idx, row in counts_df.iterrows():\n",
    "            pellet_number = row[\"pellet_number\"]\n",
    "            row_data = {\n",
    "                \"block_start\": block_start,\n",
    "                \"subject_name\": subject_name,\n",
    "                \"pellet_number\": pellet_number,\n",
    "            }\n",
    "            ts_in_patches = {\n",
    "                patch: row[f\"count_in_{patch}\"] for patch in patches_data.keys()\n",
    "            }\n",
    "            ts_in_patches_total = row[\"total_counts\"]\n",
    "            if pd.isna(ts_in_patches_total):\n",
    "                prob = {patch: 0 for patch in ts_in_patches.keys()}\n",
    "            else:\n",
    "                prob = {\n",
    "                    patch: ts_in_patches[patch] / ts_in_patches_total\n",
    "                    for patch in ts_in_patches.keys()\n",
    "                }\n",
    "            row_data.update(\n",
    "                {f\"prob_in_{patch}\": prob[patch] for patch in patches_data.keys()}\n",
    "            )\n",
    "            results.append(row_data)\n",
    "\n",
    "    # Create final DataFrame\n",
    "    prob_per_patch = pd.DataFrame(results)\n",
    "    return prob_per_patch\n",
    "\n",
    "\n",
    "def extract_hard_patch_probabilities(\n",
    "    prob_per_patch: pd.DataFrame,\n",
    "    patch_info: List[Dict[str, Any]],\n",
    "    patch_rate: float = 0.002,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Extract probabilities for hard patches matching specified patch rate.\n",
    "\n",
    "    Parameters:\n",
    "    - prob_per_patch: DataFrame containing probabilities per patch\n",
    "    - patch_info: List of dictionaries with patch information\n",
    "    - patch_rate: Patch rate to filter by (default: 0.002)\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame with probabilities for each hard patch\n",
    "    \"\"\"\n",
    "    # Filter the hard patches based on the patch_rate\n",
    "    hard_patches = [\n",
    "        patch_dict\n",
    "        for patch_dict in patch_info\n",
    "        if patch_dict[\"patch_rate\"] == patch_rate\n",
    "    ]\n",
    "\n",
    "    results = []\n",
    "    for hard_patch in hard_patches:\n",
    "        block_start = hard_patch[\"block_start\"]\n",
    "        patch_name = hard_patch[\"patch_name\"]\n",
    "\n",
    "        # Extract the hard patch data\n",
    "        hard_patch_data = prob_per_patch.loc[\n",
    "            prob_per_patch[\"block_start\"] == block_start,\n",
    "            [\"block_start\", \"subject_name\", \"pellet_number\", f\"prob_in_{patch_name}\"],\n",
    "        ]\n",
    "\n",
    "        # Rename the column for hard patch probability\n",
    "        hard_patch_data = hard_patch_data.rename(\n",
    "            columns={f\"prob_in_{patch_name}\": \"prob_in_hard_patch\"}\n",
    "        )\n",
    "\n",
    "        # Append the result to the list\n",
    "        results.append(hard_patch_data)\n",
    "\n",
    "    # Concatenate all results into a single DataFrame\n",
    "    prob_hard_patch = pd.concat(results, ignore_index=True)\n",
    "\n",
    "    return prob_hard_patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b2dd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_per_patch_social_first_half_dict = {}\n",
    "prob_per_patch_social_dict = {}\n",
    "prob_per_patch_post_social_dict = {}\n",
    "prob_hard_patch_social_first_half_dict = {}\n",
    "prob_hard_patch_social_dict = {}\n",
    "prob_hard_patch_post_social_dict = {}\n",
    "prob_hard_patch_mean_social_first_half_dict = {}\n",
    "prob_hard_patch_mean_social_dict = {}\n",
    "prob_hard_patch_mean_post_social_dict = {}\n",
    "\n",
    "for exp in experiments:\n",
    "    exp_name = exp[\"name\"]\n",
    "    block_subject_patch_data_social_first_half = (\n",
    "        block_subject_patch_data_social_first_half_dict[exp_name]\n",
    "    )\n",
    "    block_subject_patch_data_social = block_subject_patch_data_social_dict[exp_name]\n",
    "    block_subject_patch_data_post_social = block_subject_patch_data_post_social_dict[\n",
    "        exp_name\n",
    "    ]\n",
    "    patch_info = patch_info_dict[exp_name]\n",
    "\n",
    "    # Compute patch probabilities\n",
    "    prob_per_patch_social_first_half_dict[exp_name] = compute_patch_probabilities(\n",
    "        block_subject_patch_data_social_first_half\n",
    "    )\n",
    "    prob_per_patch_social_dict[exp_name] = compute_patch_probabilities(\n",
    "        block_subject_patch_data_social\n",
    "    )\n",
    "    prob_per_patch_post_social_dict[exp_name] = compute_patch_probabilities(\n",
    "        block_subject_patch_data_post_social\n",
    "    )\n",
    "\n",
    "    # Extract hard patch probabilities\n",
    "    prob_hard_patch_social_first_half_dict[exp_name] = extract_hard_patch_probabilities(\n",
    "        prob_per_patch_social_first_half_dict[exp_name], patch_info\n",
    "    )\n",
    "    prob_hard_patch_social_dict[exp_name] = extract_hard_patch_probabilities(\n",
    "        prob_per_patch_social_dict[exp_name], patch_info\n",
    "    )\n",
    "    prob_hard_patch_post_social_dict[exp_name] = extract_hard_patch_probabilities(\n",
    "        prob_per_patch_post_social_dict[exp_name], patch_info\n",
    "    )\n",
    "\n",
    "    # Calculate the mean hard patch probability per pellet number\n",
    "    prob_hard_patch_mean_social_first_half_dict[exp_name] = (\n",
    "        prob_hard_patch_social_first_half_dict[exp_name]\n",
    "        .groupby(\"pellet_number\")\n",
    "        .mean(numeric_only=True)\n",
    "        .reset_index()\n",
    "    )\n",
    "    prob_hard_patch_mean_social_dict[exp_name] = (\n",
    "        prob_hard_patch_social_dict[exp_name]\n",
    "        .groupby(\"pellet_number\")\n",
    "        .mean(numeric_only=True)\n",
    "        .reset_index()\n",
    "    )\n",
    "    prob_hard_patch_mean_post_social_dict[exp_name] = (\n",
    "        prob_hard_patch_post_social_dict[exp_name]\n",
    "        .groupby(\"pellet_number\")\n",
    "        .mean(numeric_only=True)\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "# Combine the results\n",
    "prob_hard_patch_social_first_half_combined = pd.concat(\n",
    "    prob_hard_patch_social_first_half_dict.values()\n",
    ")\n",
    "prob_hard_patch_social_combined = pd.concat(prob_hard_patch_social_dict.values())\n",
    "prob_hard_patch_post_social_combined = pd.concat(\n",
    "    prob_hard_patch_post_social_dict.values()\n",
    ")\n",
    "\n",
    "prob_hard_patch_mean_social_first_half_combined = (\n",
    "    prob_hard_patch_social_first_half_combined.groupby(\"pellet_number\")\n",
    "    .mean(numeric_only=True)\n",
    "    .reset_index()\n",
    ")\n",
    "prob_hard_patch_mean_social_combined = (\n",
    "    prob_hard_patch_social_combined.groupby(\"pellet_number\")\n",
    "    .mean(numeric_only=True)\n",
    "    .reset_index()\n",
    ")\n",
    "prob_hard_patch_mean_post_social_combined = (\n",
    "    prob_hard_patch_post_social_combined.groupby(\"pellet_number\")\n",
    "    .mean(numeric_only=True)\n",
    "    .reset_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6b4852",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_patch_probabilities(\n",
    "    data: pd.DataFrame, label: str\n",
    ") -> Tuple[sm.OLS, np.ndarray]:\n",
    "    \"\"\"Analyze patch probabilities using linear regression model.\n",
    "\n",
    "    Parameters:\n",
    "    - data: DataFrame containing pellet_number and prob_in_hard_patch columns\n",
    "    - label: Label for the analysis output\n",
    "\n",
    "    Returns:\n",
    "    - Tuple of (fitted OLS model, predicted values array)\n",
    "    \"\"\"\n",
    "    # Prepare the data for statsmodels (add a constant for the intercept)\n",
    "    X = np.array(data[\"pellet_number\"][0:35])\n",
    "    y = np.array(data[\"prob_in_hard_patch\"][0:35])\n",
    "    # Add a constant to the independent variable X to calculate the intercept\n",
    "    X_with_constant = sm.add_constant(X)\n",
    "    # Fit the model using statsmodels\n",
    "    model = sm.OLS(y, X_with_constant).fit()\n",
    "    y_pred = model.predict(X_with_constant)\n",
    "    # Get the p-value for the slope\n",
    "    p_value = model.pvalues[1]\n",
    "    print(f\"P-value for the {label} slope: {p_value}\")\n",
    "    # Print full statistical summary\n",
    "    print(f\"{label} model summary: \", model.summary())\n",
    "    return model, y_pred\n",
    "\n",
    "\n",
    "model_social_first_half, y_pred_social_first_half = analyze_patch_probabilities(\n",
    "    prob_hard_patch_mean_social_first_half_combined, \"social first half\"\n",
    ")\n",
    "model_social, y_pred_social = analyze_patch_probabilities(\n",
    "    prob_hard_patch_mean_social_combined, \"social\"\n",
    ")\n",
    "model_post_social, y_pred_post_social = analyze_patch_probabilities(\n",
    "    prob_hard_patch_mean_post_social_combined, \"post-social\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7a9e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=prob_hard_patch_mean_social_first_half_combined[\"pellet_number\"],  # [0:35],\n",
    "        y=prob_hard_patch_mean_social_first_half_combined[\n",
    "            \"prob_in_hard_patch\"\n",
    "        ],  # [0:35],\n",
    "        mode=\"lines\",\n",
    "        name=\"First Half of Social Data\",\n",
    "        marker=dict(color=\"blue\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=prob_hard_patch_mean_social_combined[\"pellet_number\"],  # [0:35],\n",
    "        y=prob_hard_patch_mean_social_combined[\"prob_in_hard_patch\"],  # [0:35],\n",
    "        mode=\"lines\",\n",
    "        name=\"Social Data\",\n",
    "        marker=dict(color=\"red\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=prob_hard_patch_mean_post_social_combined[\"pellet_number\"],  # [0:35],\n",
    "        y=prob_hard_patch_mean_post_social_combined[\"prob_in_hard_patch\"],  # [0:35],\n",
    "        mode=\"lines\",\n",
    "        name=\"Post Social Data\",\n",
    "        marker=dict(color=\"#00CC96\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=prob_hard_patch_mean_social_first_half_combined[\"pellet_number\"][0:35],\n",
    "        y=y_pred_social_first_half,\n",
    "        mode=\"lines\",\n",
    "        name=\"Social First Half Linear Regression Line\",\n",
    "        line=dict(dash=\"dash\"),\n",
    "        marker=dict(color=\"blue\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=prob_hard_patch_mean_social_combined[\"pellet_number\"][0:35],\n",
    "        y=y_pred_social,\n",
    "        mode=\"lines\",\n",
    "        name=\"Social Linear Regression Line\",\n",
    "        line=dict(dash=\"dash\"),\n",
    "        marker=dict(color=\"red\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=prob_hard_patch_mean_social_combined[\"pellet_number\"][0:35],\n",
    "        y=y_pred_post_social,\n",
    "        mode=\"lines\",\n",
    "        name=\"Post Social Linear Regression Line\",\n",
    "        line=dict(dash=\"dash\"),\n",
    "        marker=dict(color=\"#00CC96\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Probability of being in hard patch over time\",\n",
    "    xaxis_title=\"Pellet number in block\",\n",
    "    yaxis_title=\"Hard patch probability\",\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# Save the figure as an SVG file\n",
    "# fig.write_image(\"hard_patch_probability.svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f022faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of rows and columns for the subplot grid\n",
    "num_experiments = len(experiments)\n",
    "num_cols = 2\n",
    "num_rows = (num_experiments + num_cols - 1) // num_cols\n",
    "\n",
    "# Create a subplot grid\n",
    "fig = make_subplots(\n",
    "    rows=num_rows, cols=num_cols, subplot_titles=[exp[\"name\"] for exp in experiments]\n",
    ")\n",
    "\n",
    "# Iterate over each experiment and add a plot to the grid\n",
    "for i, exp in enumerate(experiments):\n",
    "    exp_name = exp[\"name\"]\n",
    "    row = (i // num_cols) + 1\n",
    "    col = (i % num_cols) + 1\n",
    "\n",
    "    # Add the plot to the grid\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=prob_hard_patch_mean_social_first_half_dict[exp_name][\"pellet_number\"][\n",
    "                0:35\n",
    "            ],\n",
    "            y=prob_hard_patch_mean_social_first_half_dict[exp_name][\n",
    "                \"prob_in_hard_patch\"\n",
    "            ][0:35],\n",
    "            mode=\"lines\",\n",
    "            name=\"First Half of Social Data\",\n",
    "            marker=dict(color=\"blue\"),\n",
    "            showlegend=(i == 0),\n",
    "        ),\n",
    "        row=row,\n",
    "        col=col,\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=prob_hard_patch_mean_social_dict[exp_name][\"pellet_number\"][0:35],\n",
    "            y=prob_hard_patch_mean_social_dict[exp_name][\"prob_in_hard_patch\"][0:35],\n",
    "            mode=\"lines\",\n",
    "            name=\"Social Data\",\n",
    "            marker=dict(color=\"red\"),\n",
    "            showlegend=(i == 0),\n",
    "        ),\n",
    "        row=row,\n",
    "        col=col,\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=prob_hard_patch_mean_post_social_dict[exp_name][\"pellet_number\"][0:35],\n",
    "            y=prob_hard_patch_mean_post_social_dict[exp_name][\"prob_in_hard_patch\"][\n",
    "                0:35\n",
    "            ],\n",
    "            mode=\"lines\",\n",
    "            name=\"Post Social Data\",\n",
    "            marker=dict(color=\"#00CC96\"),\n",
    "            showlegend=(i == 0),\n",
    "        ),\n",
    "        row=row,\n",
    "        col=col,\n",
    "    )\n",
    "\n",
    "\n",
    "fig.update_layout(height=800, width=1000)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5dba83",
   "metadata": {},
   "source": [
    "## Data overview plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eebc3abf",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431b70fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = experiments[0]\n",
    "\n",
    "data = load_experiment_data(\n",
    "    experiment=experiment,\n",
    "    data_dir=data_dir,\n",
    "    periods=[\"social\"],\n",
    "    data_types=[\"patch\", \"position\", \"foraging\", \"weight\"],\n",
    "    # trim_days=1 # Opional: trim\n",
    ")\n",
    "social_patch_df = data[\"social_patch\"]\n",
    "social_position_df = data[\"social_position\"]\n",
    "social_position_df.set_index(\"time\", inplace=True)\n",
    "social_foraging_df = data[\"social_foraging\"]\n",
    "social_weight_df = data[\"social_weight\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168b8666",
   "metadata": {},
   "source": [
    "### 1. Position heatmaps over time per subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954b7c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy and sort dataframe\n",
    "df = social_position_df.copy().sort_index()\n",
    "\n",
    "# Compute time-of-day and flag dark vs light periods\n",
    "df[\"tod\"] = (\n",
    "    df.index.hour\n",
    "    + df.index.minute / 60\n",
    "    + df.index.second / 3600\n",
    "    + df.index.microsecond / 1e6 / 3600\n",
    ")\n",
    "df[\"is_dark\"] = (df[\"tod\"] >= light_off) & (df[\"tod\"] < light_on)\n",
    "\n",
    "# Detect light/dark transitions and assign period IDs\n",
    "first_dark = df.groupby(\"identity_name\")[\"is_dark\"].transform(\"first\")\n",
    "shifted = df.groupby(\"identity_name\")[\"is_dark\"].shift().fillna(first_dark)\n",
    "df[\"light_change\"] = df[\"is_dark\"] != shifted\n",
    "df[\"light_id\"] = df.groupby(\"identity_name\")[\"light_change\"].cumsum().astype(int) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e541d5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the original index name and reset to unique integer indices\n",
    "original_index_name = df.index.name or \"time\"\n",
    "df = df.reset_index()\n",
    "\n",
    "# Initialize speed column\n",
    "df[\"speed\"] = 0.0\n",
    "\n",
    "# Calculate the speed of each subject based on position data\n",
    "for subject in df[\"identity_name\"].unique():\n",
    "    subject_df = df[df[\"identity_name\"] == subject].copy()\n",
    "    subject_df.sort_values(original_index_name, inplace=True)  # sort by time\n",
    "    if subject_df.empty:\n",
    "        continue\n",
    "\n",
    "    # Calculate the difference in position and time\n",
    "    dxy = subject_df[[\"x\", \"y\"]].diff().values[1:]  # skip the first row (NaN)\n",
    "    dt_ms = (\n",
    "        subject_df[original_index_name].diff().dt.total_seconds().values[1:] * 1000\n",
    "    )  # convert to milliseconds\n",
    "\n",
    "    # Calculate speed in cm/s\n",
    "    speed = np.linalg.norm(dxy, axis=1) / dt_ms * 1000 / cm2px  # convert to cm/s\n",
    "    subject_df[\"speed\"] = np.concatenate(([0], speed))  # add zero for the first row\n",
    "\n",
    "    # Apply a running average filter\n",
    "    k = np.ones(10) / 10  # running avg filter kernel (10 frames)\n",
    "    subject_df[\"speed\"] = np.convolve(subject_df[\"speed\"], k, mode=\"same\")\n",
    "\n",
    "    # Update the original dataframe\n",
    "    mask = df[\"identity_name\"] == subject\n",
    "    df.loc[mask, \"speed\"] = subject_df[\"speed\"].values\n",
    "\n",
    "# Set the index back to time\n",
    "df = df.set_index(original_index_name)\n",
    "\n",
    "# Remove rows where speed is above 250cm/s\n",
    "df_filtered = df[df[\"speed\"] <= 250]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa8e496",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_in_chunks(ax: plt.Axes, x: np.ndarray, y: np.ndarray, **plot_kwargs) -> None:\n",
    "    \"\"\"Break data into segments and plot each as separate path to avoid memory issues.\n",
    "\n",
    "    Parameters:\n",
    "    - ax: Matplotlib axes object to plot on\n",
    "    - x: X-coordinate data array\n",
    "    - y: Y-coordinate data array\n",
    "    - **plot_kwargs: Additional keyword arguments passed to plot function\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    n = len(x)\n",
    "    for i in range(0, n, 20000):\n",
    "        xi = x[i : i + 20000]\n",
    "        yi = y[i : i + 20000]\n",
    "        ax.plot(xi, yi, **plot_kwargs)\n",
    "\n",
    "\n",
    "df_plot = df_filtered.reset_index()\n",
    "\n",
    "dark_color = \"#555555\"\n",
    "light_color = \"#CCCCCC\"\n",
    "subjects = sorted(df_plot[\"identity_name\"].unique())\n",
    "n_subj = len(subjects)\n",
    "n_per = int(df_plot[\"light_id\"].max())\n",
    "\n",
    "# Set up subplots\n",
    "fig, axes = plt.subplots(\n",
    "    nrows=n_subj,\n",
    "    ncols=n_per,\n",
    "    figsize=(2 * n_per, 2 * n_subj),\n",
    "    sharex=True,\n",
    "    sharey=True,\n",
    "    squeeze=False,\n",
    ")\n",
    "\n",
    "# Loop over axes and draw\n",
    "for i, subj in enumerate(subjects):\n",
    "    for j in range(1, n_per + 1):\n",
    "        ax = axes[i, j - 1]\n",
    "        sub = df_plot[(df_plot.identity_name == subj) & (df_plot.light_id == j)]\n",
    "        if sub.empty:\n",
    "            ax.set_axis_off()\n",
    "            continue\n",
    "\n",
    "        col = dark_color if sub.is_dark.iloc[0] else light_color\n",
    "\n",
    "        plot_in_chunks(\n",
    "            ax, sub.x.values, sub.y.values, linestyle=\"-\", linewidth=0.8, color=col\n",
    "        )\n",
    "        ax.set_aspect(\"equal\", \"box\")\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "# Scale bar in bottom-right\n",
    "last_ax = axes[-1, -1]\n",
    "length_px = 0.2 * 100 * cm2px\n",
    "xmin, xmax = last_ax.get_xlim()\n",
    "ymin, ymax = last_ax.get_ylim()\n",
    "x1 = xmax - 0.02 * (xmax - xmin)\n",
    "x0 = x1 - length_px\n",
    "y0 = ymin + 0.02 * (ymax - ymin)\n",
    "last_ax.plot([x0, x1], [y0, y0], \"k-\", lw=2)\n",
    "last_ax.text(\n",
    "    (x0 + x1) / 2,\n",
    "    y0 - 0.06 * (ymax - ymin),\n",
    "    \"0.2 m\",\n",
    "    va=\"top\",\n",
    "    ha=\"center\",\n",
    "    fontsize=8,\n",
    "    color=\"k\",\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# svg_path = save_dir / \"position_maps.pdf\"\n",
    "# plt.savefig(svg_path, format=\"pdf\", dpi=300)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec28e673",
   "metadata": {},
   "source": [
    "### 2. Locomotion speed over time per subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e612a9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_speed = df_filtered.copy()\n",
    "df_speed.index.name = \"time\"\n",
    "\n",
    "dark_color = \"#555555\"\n",
    "light_color = \"#CCCCCC\"\n",
    "subjects = sorted(df_speed[\"identity_name\"].unique())\n",
    "\n",
    "agg = (\n",
    "    df_speed[[\"identity_name\", \"speed\"]]\n",
    "    .groupby(\"identity_name\")\n",
    "    .resample(\"1s\")\n",
    "    .mean()\n",
    "    .dropna()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "fig = go.Figure()\n",
    "for subj in subjects:\n",
    "    sub = agg[agg[\"identity_name\"] == subj]\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=sub[\"time\"],\n",
    "            y=sub[\"speed\"],\n",
    "            mode=\"lines\",\n",
    "            name=subj,\n",
    "            line=dict(width=1),\n",
    "            opacity=1,\n",
    "        )\n",
    "    )\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Speed over Time\",\n",
    "    xaxis_title=\"Time\",\n",
    "    yaxis_title=\"Speed\",\n",
    "    legend_title=\"Subject\",\n",
    "    plot_bgcolor=\"white\",\n",
    ")\n",
    "\n",
    "# pio.write_image(\n",
    "#     fig,\n",
    "#     str(save_dir / \"speed_over_time.svg\"),\n",
    "#     format=\"svg\"\n",
    "# )\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043c67eb",
   "metadata": {},
   "source": [
    "### 3. Foraging bouts raster plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7588f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "dark_color = \"#555555\"\n",
    "subjects = sorted(social_foraging_df[\"subject\"].unique())\n",
    "\n",
    "fig = px.timeline(\n",
    "    social_foraging_df,\n",
    "    x_start=\"start\",\n",
    "    x_end=\"end\",\n",
    "    y=\"subject\",\n",
    "    hover_data=[\"n_pellets\", \"cum_wheel_dist\"],\n",
    "    category_orders={\"subject\": subjects},\n",
    ")\n",
    "\n",
    "fig.update_traces(\n",
    "    opacity=1,\n",
    "    marker_color=dark_color,\n",
    "    marker_line_color=dark_color,\n",
    "    marker_line_width=1.5,\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    template=\"simple_white\",\n",
    "    plot_bgcolor=\"white\",\n",
    "    margin=dict(l=150, r=20, t=20, b=20),\n",
    "    height=max(100, len(subjects) * 25 + 50),\n",
    "    xaxis=dict(\n",
    "        showgrid=False, zeroline=False, showline=False, ticks=\"\", showticklabels=False\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        showgrid=False,\n",
    "        zeroline=False,\n",
    "        showline=False,\n",
    "        ticks=\"\",\n",
    "        showticklabels=True,\n",
    "        title=\"\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "# pio.write_image(\n",
    "#     fig,\n",
    "#     str(save_dir / \"foraging_bouts_raster.svg\"),\n",
    "#     format=\"svg\"\n",
    "# )\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5977087e",
   "metadata": {},
   "source": [
    "### 4. Wheel distance spun "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d92ed1f",
   "metadata": {},
   "source": [
    "#### Over time per subject patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c83fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_seconds = 0.02\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "# Build each trace (continuous + Δ>0.5 downsample)\n",
    "max_y = 0\n",
    "for (subject, patch), grp in social_patch_df.groupby([\"subject_name\", \"patch_name\"]):\n",
    "    grp = grp.sort_values(\"block_start\")\n",
    "    total_n = sum(len(a) for a in grp.wheel_cumsum_distance_travelled)\n",
    "\n",
    "    times = np.empty(total_n, dtype=\"datetime64[ns]\")\n",
    "    dists = np.empty(total_n, dtype=float)\n",
    "    idx, offset = 0, 0.0\n",
    "\n",
    "    for bs_val, arr in zip(grp.block_start, grp.wheel_cumsum_distance_travelled):\n",
    "        arr = np.asarray(arr)\n",
    "        n = arr.size\n",
    "        offs = (np.arange(n) * dt_seconds * 1e9).astype(\"timedelta64[ns]\")\n",
    "        times[idx : idx + n] = np.datetime64(bs_val) + offs\n",
    "        dists[idx : idx + n] = arr + offset\n",
    "        offset += arr[-1]\n",
    "        idx += n\n",
    "\n",
    "    max_y = max(max_y, dists.max())\n",
    "\n",
    "    # Downsample\n",
    "    diffs = np.abs(np.diff(dists, prepend=dists[0]))\n",
    "    mask = diffs > 0.5\n",
    "    mask[0] = True\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=times[mask],\n",
    "            y=dists[mask] / 100,  # convert to meters\n",
    "            mode=\"lines\",\n",
    "            name=f\"{subject} — {patch}\",\n",
    "            line=dict(width=1.5),\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Hide all ticks and tick lines; keep only the y-axis title\n",
    "fig.update_xaxes(\n",
    "    showgrid=False, zeroline=False, showline=False, showticklabels=False, ticks=\"\"\n",
    ")\n",
    "fig.update_yaxes(\n",
    "    title_text=\"Distance spun on wheel (m)\",\n",
    "    showgrid=False,\n",
    "    zeroline=False,\n",
    "    showline=False,\n",
    "    showticklabels=False,\n",
    "    ticks=\"\",\n",
    ")\n",
    "\n",
    "# Vertical scale bar: 250 m high at the right edge\n",
    "fig.add_shape(\n",
    "    type=\"line\",\n",
    "    xref=\"paper\",\n",
    "    x0=1.02,\n",
    "    x1=1.02,\n",
    "    yref=\"y\",\n",
    "    y0=0,\n",
    "    y1=250,\n",
    "    line=dict(color=\"black\", width=2),\n",
    ")\n",
    "fig.add_annotation(\n",
    "    xref=\"paper\",\n",
    "    x=1.04,\n",
    "    y=125,\n",
    "    text=\"250\",\n",
    "    showarrow=False,\n",
    "    xanchor=\"left\",\n",
    "    yanchor=\"middle\",\n",
    "    font=dict(size=10, color=\"black\"),\n",
    ")\n",
    "\n",
    "# Layout tweaks and show\n",
    "fig.update_layout(\n",
    "    template=\"simple_white\", margin=dict(l=20, r=80, t=20, b=20), showlegend=True\n",
    ")\n",
    "\n",
    "# pio.write_image(\n",
    "#     fig,\n",
    "#     str(save_dir / \"wheel_dist_over_time.svg\"),\n",
    "#     format=\"svg\"\n",
    "# )\n",
    "\n",
    "fig.show(\n",
    "    config={\n",
    "        \"staticPlot\": True,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418200f6",
   "metadata": {},
   "source": [
    "#### Dummy patch vs normal patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa67e0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_dfs = []\n",
    "\n",
    "for exp in [experiments[i] for i in [4, 5]]:\n",
    "    data = load_experiment_data(experiment=exp, data_dir=data_dir, data_types=[\"patch\"])\n",
    "    df = data[\"None_patch\"]\n",
    "    patch_dfs.append(df)\n",
    "\n",
    "patch_df_s4 = pd.concat(patch_dfs).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbc8dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_seconds = 0.02\n",
    "summary = []\n",
    "\n",
    "# Compute total distance spun per (subject, patch)\n",
    "for (subject, patch), grp in patch_df_s4.groupby([\"subject_name\", \"patch_name\"]):\n",
    "    total_distance = sum(\n",
    "        np.asarray(w)[-1] for w in grp[\"wheel_cumsum_distance_travelled\"] if len(w) > 0\n",
    "    )\n",
    "    summary.append(\n",
    "        {\"subject\": subject, \"patch\": patch, \"distance\": total_distance / 100}\n",
    "    )  # convert to meters\n",
    "\n",
    "summary_df = pd.DataFrame(summary)\n",
    "\n",
    "# Pivot to subject x patch format\n",
    "pivot_df = summary_df.pivot(index=\"subject\", columns=\"patch\", values=\"distance\").fillna(\n",
    "    0\n",
    ")\n",
    "\n",
    "# Per-subject values for PatchDummy1 and mean of other patches\n",
    "patch_dummy1_vals = pivot_df.get(\"PatchDummy1\", pd.Series(0, index=pivot_df.index))\n",
    "other_patch_vals = pivot_df.drop(columns=\"PatchDummy1\", errors=\"ignore\").mean(axis=1)\n",
    "\n",
    "# Compute means and SEMs\n",
    "means = [patch_dummy1_vals.mean(), other_patch_vals.mean()]\n",
    "sems = [patch_dummy1_vals.sem(), other_patch_vals.sem()]\n",
    "labels = [\"PatchDummy1\", \"Avg Other Patches\"]\n",
    "\n",
    "# Plot\n",
    "fig = go.Figure(\n",
    "    data=[\n",
    "        go.Bar(\n",
    "            x=labels,\n",
    "            y=means,\n",
    "            error_y=dict(type=\"data\", array=sems, visible=True),\n",
    "            marker_color=[\"#636EFA\", \"#EF553B\"],\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Mean Wheel Distance Spun ± SEM\",\n",
    "    yaxis_title=\"Distance (m, log)\",\n",
    "    xaxis_title=\"Patch\",\n",
    "    yaxis_type=\"log\",\n",
    "    template=\"simple_white\",\n",
    "    height=400,\n",
    ")\n",
    "\n",
    "# pio.write_image(\n",
    "#     fig,\n",
    "#     str(save_dir / \"dummy_vs_normal_patches.svg\"),\n",
    "#     format=\"svg\"\n",
    "# )\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17703b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(patch_dummy1_vals, other_patch_vals)\n",
    "\n",
    "t_stat, p_val = ttest_rel(patch_dummy1_vals, other_patch_vals)\n",
    "print(f\"Paired t‐statistic = {t_stat:.3f},  p‐value = {p_val:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d59093",
   "metadata": {},
   "source": [
    "### 5. Pellets raster plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1468c927",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (\n",
    "    social_patch_df[[\"subject_name\", \"patch_name\", \"pellet_timestamps\"]]\n",
    "    .explode(\"pellet_timestamps\")\n",
    "    .dropna(subset=[\"pellet_timestamps\"])\n",
    ")\n",
    "df[\"pellet_timestamps\"] = pd.to_datetime(df[\"pellet_timestamps\"])\n",
    "df[\"patch_idx\"] = df[\"patch_name\"].str.extract(r\"(\\d+)$\").astype(int)\n",
    "\n",
    "dark_color = \"#555555\"\n",
    "subjects = sorted(df[\"subject_name\"].unique())\n",
    "n_subj = len(subjects)\n",
    "\n",
    "fig = make_subplots(rows=n_subj, cols=1, shared_xaxes=True, subplot_titles=subjects)\n",
    "\n",
    "for i, subj in enumerate(subjects, start=1):\n",
    "    sub = df[df[\"subject_name\"] == subj]\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=sub[\"pellet_timestamps\"],\n",
    "            y=sub[\"patch_idx\"],\n",
    "            mode=\"markers\",\n",
    "            marker=dict(symbol=\"line-ns\", color=dark_color, size=8, line_width=1.2),\n",
    "            showlegend=False,\n",
    "        ),\n",
    "        row=i,\n",
    "        col=1,\n",
    "    )\n",
    "    # Y-axis = patch numbers\n",
    "    fig.update_yaxes(\n",
    "        row=i,\n",
    "        col=1,\n",
    "        title=\"Patches\",\n",
    "        tickmode=\"array\",\n",
    "        showgrid=False,\n",
    "        zeroline=False,\n",
    "        showline=False,\n",
    "        ticks=\"\",\n",
    "    )\n",
    "    # Remove x-labels on every row\n",
    "    fig.update_xaxes(\n",
    "        row=i,\n",
    "        col=1,\n",
    "        showticklabels=False,\n",
    "        showgrid=False,\n",
    "        zeroline=False,\n",
    "        showline=False,\n",
    "        ticks=\"\",\n",
    "    )\n",
    "\n",
    "# Tighten the margins and overall height\n",
    "fig.update_layout(\n",
    "    template=\"simple_white\",\n",
    "    plot_bgcolor=\"white\",\n",
    "    margin=dict(l=80, r=20, t=80, b=20),\n",
    "    height=120 * n_subj,\n",
    ")\n",
    "\n",
    "# Center subject titles\n",
    "for ann in fig.layout.annotations:\n",
    "    ann.x = 0.5\n",
    "    ann.xanchor = \"center\"\n",
    "    ann.font = dict(size=16)\n",
    "\n",
    "# pio.write_image(\n",
    "#     fig,\n",
    "#     str(save_dir / \"pellets_raster.svg\"),\n",
    "#     format=\"svg\"\n",
    "# )\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83481174",
   "metadata": {},
   "source": [
    "### 6. Weight over time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693ea04b",
   "metadata": {},
   "source": [
    "#### Per subject for a single experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e2c1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only rows that have timestamps\n",
    "df = social_weight_df[\n",
    "    social_weight_df[\"timestamps\"].apply(lambda lst: len(lst) > 0)\n",
    "].copy()\n",
    "\n",
    "# Explode parallel list-columns into long form\n",
    "df = df.explode([\"timestamps\", \"weight\", \"subject_id\"])\n",
    "\n",
    "# Type conversions\n",
    "df[\"timestamps\"] = pd.to_datetime(df[\"timestamps\"])\n",
    "df[\"weight\"] = df[\"weight\"].astype(float)\n",
    "\n",
    "# Plot\n",
    "fig = px.line(\n",
    "    df,\n",
    "    x=\"timestamps\",\n",
    "    y=\"weight\",\n",
    "    color=\"subject_id\",\n",
    "    markers=True,\n",
    "    title=\"Subject Weights Over Time\",\n",
    "    labels={\"timestamps\": \"Time\", \"weight\": \"Weight\", \"subject_id\": \"Subject ID\"},\n",
    ")\n",
    "fig.update_layout(legend_title_text=\"Subject ID\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21dd0ec",
   "metadata": {},
   "source": [
    "#### Averaged over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4859b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "social_weight_dfs = []\n",
    "\n",
    "for exp in [experiments[i] for i in [0, 1, 4, 5]]:\n",
    "    data = load_experiment_data(\n",
    "        experiment=exp, data_dir=data_dir, periods=[\"social\"], data_types=[\"weight\"]\n",
    "    )\n",
    "    df = data[\"social_weight\"]\n",
    "    social_weight_dfs.append(df)\n",
    "\n",
    "social_weight_df_all_exps = pd.concat(social_weight_dfs).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1409fd4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the “array-of-arrays” df into a long DataFrame\n",
    "records = []\n",
    "for _, row in social_weight_df_all_exps.iterrows():\n",
    "    ts = pd.to_datetime(row[\"timestamps\"])\n",
    "    w = np.asarray(row[\"weight\"], dtype=float)\n",
    "    sids = row[\"subject_id\"]\n",
    "    for t, weight, sid in zip(ts, w, sids):\n",
    "        records.append({\"timestamp\": t, \"subject_id\": sid, \"weight\": weight})\n",
    "\n",
    "df = pd.DataFrame.from_records(records)\n",
    "df = df.sort_values([\"subject_id\", \"timestamp\"])\n",
    "# Drop rows where subject_id is shorter than 11 characters, removes some erroneous entries\n",
    "df = df[df[\"subject_id\"].str.len() >= 11].reset_index(drop=True)\n",
    "\n",
    "# Parameters & 24 h cycle grid\n",
    "sampling_freq = \"10min\"  # resample interval\n",
    "# Choose any date at 08:00 to define “day zero”\n",
    "anchor = pd.Timestamp(f\"2020-01-01 {light_off:02d}:00:00\")\n",
    "\n",
    "# Build the 24 h cycle index\n",
    "n_steps = int(pd.Timedelta(\"1D\") / pd.Timedelta(sampling_freq))\n",
    "cycle_index = pd.timedelta_range(start=0, periods=n_steps, freq=sampling_freq)\n",
    "\n",
    "# Will hold each mouse’s mean‐day\n",
    "cycle_df = pd.DataFrame(index=cycle_index)\n",
    "\n",
    "# Loop over each mouse, resample, fold into 24 h, average days\n",
    "for sid, grp in df.groupby(\"subject_id\"):\n",
    "    # Series of weight vs time\n",
    "    ser = grp.set_index(\"timestamp\")[\"weight\"].sort_index()\n",
    "    # Collapse any exact-duplicate timestamps\n",
    "    ser = ser.groupby(level=0).mean()\n",
    "\n",
    "    # Resample into bins anchored at 08:00, then interpolate\n",
    "    ser_rs = ser.resample(sampling_freq, origin=anchor).mean().interpolate()\n",
    "\n",
    "    # Convert each timestamp into its offset (mod 24 h) from the anchor\n",
    "    offsets = (ser_rs.index - anchor) % pd.Timedelta(\"1D\")\n",
    "    ser_rs.index = offsets\n",
    "\n",
    "    # Average across all days for each offset\n",
    "    daily = ser_rs.groupby(ser_rs.index).mean()\n",
    "\n",
    "    # Align to uniform cycle grid\n",
    "    cycle_df[sid] = daily.reindex(cycle_index)\n",
    "\n",
    "# Baseline‐subtract each mouse’s minimum, then grand‐mean\n",
    "cycle_df_baselined = cycle_df.subtract(cycle_df.min(skipna=True), axis=1)\n",
    "grand_mean = cycle_df_baselined.mean(axis=1)\n",
    "sem = cycle_df_baselined.sem(axis=1)\n",
    "\n",
    "# Smooth both mean and SEM with a centered rolling window\n",
    "window = 20\n",
    "grand_mean_smooth = grand_mean.rolling(window=window, center=True, min_periods=1).mean()\n",
    "sem_smooth = sem.rolling(window=window, center=True, min_periods=1).mean()\n",
    "\n",
    "# Plot each subject's mean-day curve in its own subplot\n",
    "n_subj = cycle_df_baselined.shape[1]\n",
    "n_cols = 4  # adjust as needed\n",
    "n_rows = math.ceil(n_subj / n_cols)\n",
    "\n",
    "fig, axes = plt.subplots(\n",
    "    n_rows, n_cols, figsize=(3.5 * n_cols, 2.5 * n_rows), sharex=True, sharey=True\n",
    ")\n",
    "axes = axes.flatten()\n",
    "x_hours = cycle_df_baselined.index.total_seconds() / 3600\n",
    "\n",
    "for i, sid in enumerate(cycle_df_baselined.columns):\n",
    "    ax = axes[i]\n",
    "    y = cycle_df_baselined[sid]\n",
    "    y_smooth = y.rolling(window=window, center=True, min_periods=1).mean()\n",
    "    ax.plot(x_hours, y_smooth, lw=1.5)\n",
    "    ax.set_title(sid, fontsize=9)\n",
    "    ax.set_xlim(0, 24)\n",
    "    ax.grid(True, linestyle=\":\", linewidth=0.5)\n",
    "\n",
    "# Remove any unused axes\n",
    "for ax in axes[n_subj:]:\n",
    "    ax.set_visible(False)\n",
    "\n",
    "fig.suptitle(\"24 h weight cycle (baseline-subtracted) per subject\", y=1.02)\n",
    "fig.text(0.5, 0.04, f\"Time since {light_off:02d}:00:00 (hours)\", ha=\"center\")\n",
    "fig.text(0.04, 0.5, \"Weight (baseline-subtracted)\", va=\"center\", rotation=\"vertical\")\n",
    "plt.subplots_adjust(hspace=0.4, wspace=0.3, bottom=0.1, left=0.07, right=0.97, top=0.90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32049d88",
   "metadata": {},
   "source": [
    "#### Averaged over time and subjects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6c714d",
   "metadata": {},
   "source": [
    "Baselined by subtracting the minimum of each subject's 24h mean-day curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37e8d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the “array-of-arrays” df into a long DataFrame\n",
    "records = []\n",
    "for _, row in social_weight_df_all_exps.iterrows():\n",
    "    ts = pd.to_datetime(row[\"timestamps\"])\n",
    "    w = np.asarray(row[\"weight\"], dtype=float)\n",
    "    sids = row[\"subject_id\"]\n",
    "    for t, weight, sid in zip(ts, w, sids):\n",
    "        records.append({\"timestamp\": t, \"subject_id\": sid, \"weight\": weight})\n",
    "\n",
    "df = pd.DataFrame.from_records(records)\n",
    "df = df.sort_values([\"subject_id\", \"timestamp\"])\n",
    "# Drop rows where subject_id is shorter than 11 characters, removes some erroneous entries\n",
    "df = df[df[\"subject_id\"].str.len() >= 11].reset_index(drop=True)\n",
    "\n",
    "# Parameters & 24 h cycle grid\n",
    "sampling_freq = \"10min\"  # resample interval\n",
    "# Choose any date at 08:00 to define “day zero”\n",
    "anchor = pd.Timestamp(f\"2020-01-01 {light_off:02d}:00:00\")\n",
    "\n",
    "# Build the 24 h cycle index\n",
    "n_steps = int(pd.Timedelta(\"1D\") / pd.Timedelta(sampling_freq))\n",
    "cycle_index = pd.timedelta_range(start=0, periods=n_steps, freq=sampling_freq)\n",
    "\n",
    "# Will hold each mouse’s mean‐day\n",
    "cycle_df = pd.DataFrame(index=cycle_index)\n",
    "\n",
    "# Loop over each mouse, resample, fold into 24 h, average days\n",
    "for sid, grp in df.groupby(\"subject_id\"):\n",
    "    # Series of weight vs time\n",
    "    ser = grp.set_index(\"timestamp\")[\"weight\"].sort_index()\n",
    "    # Collapse any exact-duplicate timestamps\n",
    "    ser = ser.groupby(level=0).mean()\n",
    "\n",
    "    # Resample into bins anchored at 08:00, then interpolate\n",
    "    ser_rs = ser.resample(sampling_freq, origin=anchor).mean().interpolate()\n",
    "\n",
    "    # Convert each timestamp into its offset (mod 24 h) from the anchor\n",
    "    offsets = (ser_rs.index - anchor) % pd.Timedelta(\"1D\")\n",
    "    ser_rs.index = offsets\n",
    "\n",
    "    # Average across all days for each offset\n",
    "    daily = ser_rs.groupby(ser_rs.index).mean()\n",
    "\n",
    "    # Align to uniform cycle grid\n",
    "    cycle_df[sid] = daily.reindex(cycle_index)\n",
    "\n",
    "# Baseline‐subtract each mouse’s minimum, then grand‐mean\n",
    "cycle_df_baselined = cycle_df.subtract(cycle_df.min())\n",
    "grand_mean = cycle_df_baselined.mean(axis=1)\n",
    "sem = cycle_df_baselined.sem(axis=1)\n",
    "\n",
    "# Smooth both mean and SEM with a centered rolling window\n",
    "window = 20\n",
    "grand_mean_smooth = grand_mean.rolling(window=window, center=True, min_periods=1).mean()\n",
    "sem_smooth = sem.rolling(window=window, center=True, min_periods=1).mean()\n",
    "\n",
    "# Plot mean ± SEM\n",
    "plt.figure(figsize=(10, 4))\n",
    "x_hours = cycle_df_baselined.index.total_seconds() / 3600\n",
    "\n",
    "plt.plot(x_hours, grand_mean_smooth, lw=2, label=\"Mean weight\")\n",
    "plt.fill_between(\n",
    "    x_hours,\n",
    "    grand_mean_smooth - sem_smooth,\n",
    "    grand_mean_smooth + sem_smooth,\n",
    "    alpha=0.3,\n",
    "    label=\"± SEM\",\n",
    ")\n",
    "\n",
    "plt.xlabel(f\"Time since {light_off:02d}:00:00 (hours)\")\n",
    "plt.ylabel(\"Weight (baseline-subtracted)\")\n",
    "plt.title(\"Average 24 h weight cycle across all mice\\n(Mean ± SEM)\")\n",
    "plt.xlim(0, 24)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a57662b",
   "metadata": {},
   "source": [
    "Baselined by subtracting the minimum of each subject's _smoothed_ 24h mean-day curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa51db1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the “array-of-arrays” df into a long DataFrame\n",
    "records = []\n",
    "for _, row in social_weight_df_all_exps.iterrows():\n",
    "    ts = pd.to_datetime(row[\"timestamps\"])\n",
    "    w = np.asarray(row[\"weight\"], dtype=float)\n",
    "    sids = row[\"subject_id\"]\n",
    "    for t, weight, sid in zip(ts, w, sids):\n",
    "        records.append({\"timestamp\": t, \"subject_id\": sid, \"weight\": weight})\n",
    "\n",
    "df = pd.DataFrame.from_records(records)\n",
    "df = df.sort_values([\"subject_id\", \"timestamp\"])\n",
    "# Drop rows where subject_id is shorter than 11 characters, removes some erroneous entries\n",
    "df = df[df[\"subject_id\"].str.len() >= 11].reset_index(drop=True)\n",
    "\n",
    "# Parameters & 24 h cycle grid\n",
    "sampling_freq = \"10min\"  # resample interval\n",
    "# Choose any date at 08:00 to define “day zero”\n",
    "anchor = pd.Timestamp(f\"2020-01-01 {light_off:02d}:00:00\")\n",
    "\n",
    "# Build the 24 h cycle index\n",
    "n_steps = int(pd.Timedelta(\"1D\") / pd.Timedelta(sampling_freq))\n",
    "cycle_index = pd.timedelta_range(start=0, periods=n_steps, freq=sampling_freq)\n",
    "\n",
    "# Will hold each mouse’s mean‐day (unsmoothed)\n",
    "cycle_df = pd.DataFrame(index=cycle_index)\n",
    "\n",
    "# Loop over each mouse, resample, fold into 24 h, average days\n",
    "for sid, grp in df.groupby(\"subject_id\"):\n",
    "    # Series of weight vs time\n",
    "    ser = grp.set_index(\"timestamp\")[\"weight\"].sort_index()\n",
    "    # Collapse any exact-duplicate timestamps\n",
    "    ser = ser.groupby(level=0).mean()\n",
    "\n",
    "    # Resample into bins anchored at 08:00, then interpolate\n",
    "    ser_rs = ser.resample(sampling_freq, origin=anchor).mean().interpolate()\n",
    "\n",
    "    # Convert each timestamp into its offset (mod 24 h) from the anchor\n",
    "    offsets = (ser_rs.index - anchor) % pd.Timedelta(\"1D\")\n",
    "    ser_rs.index = offsets\n",
    "\n",
    "    # Average across all days for each offset\n",
    "    daily = ser_rs.groupby(ser_rs.index).mean()\n",
    "\n",
    "    # Align to uniform cycle grid\n",
    "    cycle_df[sid] = daily.reindex(cycle_index)\n",
    "\n",
    "# Baseline‐subtract using each subject’s smoothed minimum\n",
    "window = 20  # keep same smoothing window for per‐subject curves\n",
    "# Smooth each column (i.e., each subject’s 24 h curve)\n",
    "cycle_df_smooth = cycle_df.rolling(window=window, center=True, min_periods=1).mean()\n",
    "\n",
    "# Find the minimum of each subject’s smoothed curve\n",
    "minima_smooth = cycle_df_smooth.min()  # series indexed by subject_id\n",
    "\n",
    "# Subtract that baseline from the UNSMOOTHED cycle for each subject\n",
    "cycle_df_baselined = cycle_df.subtract(minima_smooth, axis=1)\n",
    "\n",
    "# Now compute grand‐mean and SEM (over baselined, unsmoothed curves)\n",
    "grand_mean = cycle_df_baselined.mean(axis=1)\n",
    "sem = cycle_df_baselined.sem(axis=1)\n",
    "\n",
    "# Smooth both grand‐mean and SEM for plotting\n",
    "grand_mean_smooth = grand_mean.rolling(window=window, center=True, min_periods=1).mean()\n",
    "sem_smooth = sem.rolling(window=window, center=True, min_periods=1).mean()\n",
    "\n",
    "# Plot mean ± SEM\n",
    "plt.figure(figsize=(10, 4))\n",
    "x_hours = cycle_df_baselined.index.total_seconds() / 3600\n",
    "\n",
    "plt.plot(x_hours, grand_mean_smooth, lw=2, label=\"Mean weight (baselined)\")\n",
    "plt.fill_between(\n",
    "    x_hours,\n",
    "    grand_mean_smooth - sem_smooth,\n",
    "    grand_mean_smooth + sem_smooth,\n",
    "    alpha=0.3,\n",
    "    label=\"± SEM\",\n",
    ")\n",
    "\n",
    "plt.xlabel(f\"Time since {light_off:02d}:00:00 (hours)\")\n",
    "plt.ylabel(\"Weight (baseline‐subtracted)\")\n",
    "plt.title(\"Average 24 h weight cycle across all mice\\n(Mean ± SEM)\")\n",
    "plt.xlim(0, 24)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "\n",
    "# svg_path = save_dir / \"24h_weight_cycle.svg\"\n",
    "# plt.savefig(str(svg_path), format='svg')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aeon-social-analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
