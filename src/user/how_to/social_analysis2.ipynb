{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Platform paper social experiment analysis: Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we will work with behavioural data collected from experiments `social0.2`, `social0.3`, and `social0.4`, in which two mice foraged for food in the [habitat](target-habitat) with three [foraging patches](target-foraging-patches) whose reward rates changed dynamically over time. \n",
    "\n",
    "The experiments each consist of three periods:\n",
    "\n",
    "1. \"presocial\", in which each mouse was in the habitat alone for 3-4 days.\n",
    "2. \"social\", in which both mice were in the habitat together for 2 weeks.\n",
    "3. \"postsocial\", in which each mouse was in the habitat alone again for 3-4 days.\n",
    "\n",
    "The goal of the experiments was to understand how the mice's behaviour changes as they learn to forage for food in the habitat, and how their behaviour differs between social vs. solo settings.\n",
    "\n",
    "The full datasets are available on the [Datasets](target-full-datasets) page but for the purpose of this example, we will be using the [precomputed **Platform paper social analysis datasets**](https://app.globus.org/file-manager?origin_id=48cc1398-b591-4f52-85d2-f68801306d4a&origin_path=%2F).\n",
    "\n",
    ":::{seealso}\n",
    "\"Extended Data Fig. 7\", in \"Extended Data\" in the \"Supplementary Material\" of the [platform paper](aeon-paper:) for a detailed description of the experiments.\n",
    ":::\n",
    "\n",
    "Below is a brief explanation of how the environment (i.e. patch properties) changed over {term}`blocks <Block>` (60&ndash;180 minute periods of time):\n",
    "\n",
    "1. Every block begins at a random interval $t$:\n",
    "    $$\n",
    "    t \\sim \\mathrm{Uniform}(60,\\,180) \\quad \\text{In minutes}\n",
    "    $$\n",
    "2. At the start of each block, sample a row from the predefined matrix $\\lambda_{\\mathrm{set}}$:\n",
    "    $$\n",
    "    \\lambda_{\\mathrm{set}} = \n",
    "    \\begin{pmatrix}\n",
    "    1 & 1 & 1 \\\\\n",
    "    5 & 5 & 5 \\\\\n",
    "    1 & 3 & 5 \\\\\n",
    "    1 & 5 & 3 \\\\\n",
    "    3 & 1 & 5 \\\\\n",
    "    3 & 5 & 1 \\\\\n",
    "    5 & 1 & 3 \\\\\n",
    "    5 & 3 & 1 \\\\\n",
    "    \\end{pmatrix}\n",
    "    \\quad \\text{In meters}\n",
    "    $$\n",
    "3. Assign the sampled row to specific patch means $\\lambda_{\\mathrm{1}}, \\lambda_{\\mathrm{2}}, \\lambda_{\\mathrm{3}}$ and apply a constant offset $c$ to all thresholds:\n",
    "    $$\n",
    "    \\begin{aligned}\n",
    "    \\lambda_{\\mathrm{1}}, \\lambda_{\\mathrm{2}}, \\lambda_{\\mathrm{3}} &\\sim \\mathrm{Uniform}(\\lambda_{\\mathrm{set}}) \\\\\n",
    "    c &= 0.75\n",
    "    \\end{aligned}\n",
    "    \\quad \\text{Patch means and offset}\n",
    "    $$\n",
    "4. Sample a value from each of $P_{\\mathrm{1}}, P_{\\mathrm{2}}, P_{\\mathrm{3}}$ as the initial threshold for the respective patch. Whenever a patch reaches its threshold, resample a new value from its corresponding distribution:\n",
    "    $$\n",
    "    \\begin{aligned}\n",
    "    P_{\\mathrm{1}} &= c + \\mathrm{Exp}(1/\\lambda_{\\mathrm{1}}) \\\\\n",
    "    P_{\\mathrm{2}} &= c + \\mathrm{Exp}(1/\\lambda_{\\mathrm{2}}) \\\\\n",
    "    P_{\\mathrm{3}} &= c + \\mathrm{Exp}(1/\\lambda_{\\mathrm{3}})\n",
    "    \\end{aligned}\n",
    "    \\quad \\text{Patch distributions}\n",
    "    $$\n",
    "## Set up environment\n",
    "\n",
    "Create and activate a virtual environment named `social-analysis` using [uv](https://docs.astral.sh/uv/getting-started/installation/).\n",
    "```bash\n",
    "uv venv aeon-social-analysis --python \">=3.11\" \n",
    "source aeon-social-analysis/bin/activate   # Unix\n",
    ".\\aeon-social-analysis\\Scripts\\activate   # Windows\n",
    "```\n",
    "\n",
    "Install the required [`ssm` package](https://github.com/lindermanlab/ssm) and its dependencies.\n",
    "```bash\n",
    "uv pip install matplotlib numpy pandas plotly statsmodels pyyaml pyarrow tqdm scipy jupyter\n",
    "```\n",
    "\n",
    "## Import libraries and define variables and helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_117280/1862467150.py:16: MatplotlibDeprecationWarning: The seaborn styles shipped by Matplotlib are deprecated since 3.6, as they no longer correspond to the styles shipped by seaborn. However, they will remain available as 'seaborn-v0_8-<style>'. Alternatively, directly use the seaborn API instead.\n",
      "  plt.style.use(\"seaborn-whitegrid\")\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Notebook settings and imports\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "from warnings import warn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly\n",
    "import seaborn as sns\n",
    "from IPython.display import Image\n",
    "from scipy import stats\n",
    "from scipy.ndimage import uniform_filter1d\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "plt.style.use(\"seaborn-whitegrid\")\n",
    "plt.rcParams[\"axes.titlesize\"] = 20\n",
    "plt.rcParams[\"axes.labelsize\"] = 18\n",
    "plt.rcParams[\"xtick.labelsize\"] = 15\n",
    "plt.rcParams[\"ytick.labelsize\"] = 15\n",
    "plt.rcParams[\"legend.title_fontsize\"] = 15\n",
    "plt.rcParams[\"legend.fontsize\"] = 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "cm2px = 5.2  # 1 cm = 5.2 px roughly in aeon arenas\n",
    "light_off, light_on = 7, 20  # 7am to 8pm\n",
    "\n",
    "subject_colors = plotly.colors.qualitative.Dark24\n",
    "patch_colors = plotly.colors.qualitative.Light24\n",
    "patch_markers = [\n",
    "    \"circle\",\n",
    "    \"bowtie\",\n",
    "    \"square\",\n",
    "    \"hourglass\",\n",
    "    \"diamond\",\n",
    "    \"cross\",\n",
    "    \"x\",\n",
    "    \"triangle\",\n",
    "    \"star\",\n",
    "]\n",
    "patch_markers_symbols = [\"●\", \"⧓\", \"■\", \"⧗\", \"♦\", \"✖\", \"×\", \"▲\", \"★\"]\n",
    "patch_markers_dict = dict(zip(patch_markers, patch_markers_symbols, strict=False))\n",
    "patch_markers_linestyles = [\"solid\", \"dash\", \"dot\", \"dashdot\", \"longdashdot\"]\n",
    "subject_markers_linestyles = patch_markers_linestyles.copy()\n",
    "patch_type_mean_map = {100: \"l\", 300: \"m\", 500: \"h\", 200: \"l\", 600: \"m\", 1000: \"h\"}\n",
    "patch_type_rate_map = {\n",
    "    0.01: \"l\",\n",
    "    0.0033: \"m\",\n",
    "    0.002: \"h\",\n",
    "    0.005: \"l\",\n",
    "    0.00167: \"m\",\n",
    "    0.001: \"h\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments = [\n",
    "    {\n",
    "        \"name\": \"social0.2-aeon3\",\n",
    "        \"presocial_start\": \"2024-01-31 11:00:00\",\n",
    "        \"presocial_end\": \"2024-02-08 15:00:00\",\n",
    "        \"social_start\": \"2024-02-09 16:00:00\",\n",
    "        \"social_end\": \"2024-02-23 13:00:00\",\n",
    "        \"postsocial_start\": \"2024-02-25 17:00:00\",\n",
    "        \"postsocial_end\": \"2024-03-02 14:00:00\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"social0.2-aeon4\",\n",
    "        \"presocial_start\": \"2024-01-31 11:00:00\",\n",
    "        \"presocial_end\": \"2024-02-08 15:00:00\",\n",
    "        \"social_start\": \"2024-02-09 17:00:00\",\n",
    "        \"social_end\": \"2024-02-23 12:00:00\",\n",
    "        \"postsocial_start\": \"2024-02-25 18:00:00\",\n",
    "        \"postsocial_end\": \"2024-03-02 13:00:00\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"social0.3-aeon3\",\n",
    "        \"presocial_start\": \"2024-06-08 19:00:00\",\n",
    "        \"presocial_end\": \"2024-06-17 13:00:00\",\n",
    "        \"social_start\": \"2024-06-25 11:00:00\",\n",
    "        \"social_end\": \"2024-07-06 13:00:00\",\n",
    "        \"postsocial_start\": \"2024-07-07 16:00:00\",\n",
    "        \"postsocial_end\": \"2024-07-14 14:00:00\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"social0.4-aeon3\",\n",
    "        \"presocial_start\": \"2024-08-16 17:00:00\",\n",
    "        \"presocial_end\": \"2024-08-24 10:00:00\",\n",
    "        \"social_start\": \"2024-08-28 11:00:00\",\n",
    "        \"social_end\": \"2024-09-09 13:00:00\",\n",
    "        \"postsocial_start\": \"2024-09-09 18:00:00\",\n",
    "        \"postsocial_end\": \"2024-09-22 16:00:00\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"social0.4-aeon4\",\n",
    "        \"presocial_start\": \"2024-08-16 15:00:00\",\n",
    "        \"presocial_end\": \"2024-08-24 10:00:00\",\n",
    "        \"social_start\": \"2024-08-28 10:00:00\",\n",
    "        \"social_end\": \"2024-09-09 01:00:00\",\n",
    "        \"postsocial_start\": \"2024-09-09 15:00:00\",\n",
    "        \"postsocial_end\": \"2024-09-22 16:00:00\",\n",
    "    },\n",
    "]\n",
    "\n",
    "periods = [\"social\", \"postsocial\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the possible combos of social and light\n",
    "combos = [\n",
    "    (True, True),  # Social + Light\n",
    "    (True, False),  # Social + Dark\n",
    "    (False, True),  # Solo + Light\n",
    "    (False, False),  # Solo + Dark\n",
    "]\n",
    "\n",
    "# Define colors based on light condition (light=blue, dark=orange)\n",
    "colors = {\n",
    "    True: \"#1f77b4\",  # Blue for light conditions\n",
    "    False: \"#ff7f0e\",  # Orange for dark conditions\n",
    "}\n",
    "\n",
    "# Define hatch patterns based on social condition\n",
    "hatches = {\n",
    "    True: \"///\",  # Hatched pattern for social\n",
    "    False: None,  # No pattern (solid) for solo\n",
    "}\n",
    "\n",
    "labels = [\"Social-Light\", \"Social-Dark\", \"Solo-Light\", \"Solo-Dark\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_from_parquet(\n",
    "    experiment_name: str | None,\n",
    "    period: str | None,\n",
    "    data_type: str,\n",
    "    data_dir: Path,\n",
    "    set_time_index: bool = False,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Loads saved data from parquet files.\n",
    "\n",
    "    Args:\n",
    "        experiment_name (str, optional): Filter by experiment name. If None, load all experiments.\n",
    "        period (str, optional): Filter by period (presocial, social, postsocial). If None, load all periods.\n",
    "        data_type (str): Type of data to load (position, patch, foraging, rfid, sleep, explore)\n",
    "        data_dir (Path): Directory containing parquet files.\n",
    "        set_time_index (bool, optional): If True, set 'time' column as DataFrame index.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Combined DataFrame of all matching parquet files.\n",
    "    \"\"\"\n",
    "    if not data_dir.exists():\n",
    "        print(f\"Directory {data_dir} does not exist. No data files found.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Create pattern based on filters\n",
    "    pattern = \"\"\n",
    "    if experiment_name:\n",
    "        pattern += f\"{experiment_name}_\"\n",
    "    else:\n",
    "        pattern += \"*_\"\n",
    "\n",
    "    if period:\n",
    "        pattern += f\"{period}_\"\n",
    "    else:\n",
    "        pattern += \"*_\"\n",
    "\n",
    "    pattern += f\"{data_type}.parquet\"\n",
    "\n",
    "    # Find matching files\n",
    "    matching_files = list(data_dir.glob(pattern))\n",
    "\n",
    "    if not matching_files:\n",
    "        print(f\"No matching data files found with pattern: {pattern}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    print(f\"Found {len(matching_files)} matching files\")\n",
    "\n",
    "    # Load and concatenate matching files\n",
    "    dfs = []\n",
    "    total_rows = 0\n",
    "    for file in matching_files:\n",
    "        print(f\"Loading {file}...\")\n",
    "        df = pd.read_parquet(file)\n",
    "        total_rows += len(df)\n",
    "        dfs.append(df)\n",
    "        print(f\"  Loaded {len(df)} rows\")\n",
    "\n",
    "    # Combine data\n",
    "    if dfs:\n",
    "        combined_df = pd.concat(dfs, ignore_index=True)\n",
    "        if set_time_index and \"time\" in combined_df.columns:\n",
    "            combined_df = combined_df.set_index(\"time\")\n",
    "        print(f\"Combined data: {len(combined_df)} rows\")\n",
    "        return combined_df\n",
    "    else:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "\n",
    "def load_experiment_data(\n",
    "    data_dir: Path,\n",
    "    experiment: dict | None = None,\n",
    "    periods: list | None = None,\n",
    "    data_types: list[str] = [\"rfid\", \"position\"],\n",
    "    trim_days: int | None = None,\n",
    ") -> dict:\n",
    "    \"\"\"Load all data types for specified periods of an experiment.\n",
    "\n",
    "    Parameters:\n",
    "    - experiment: experiment dict with period start/end times\n",
    "    - periods: list of periods to load\n",
    "    - data_types: list of data types to load\n",
    "    - data_dir: directory containing data files\n",
    "    - trim_days: Optional number of days to trim from start (None = no trim)\n",
    "\n",
    "    Returns:\n",
    "    - Dictionary containing dataframes for each period/data type combination\n",
    "    \"\"\"\n",
    "    result = {}\n",
    "\n",
    "    if periods is None:\n",
    "        periods = [None]\n",
    "\n",
    "    for period in periods:\n",
    "        for data_type in data_types:\n",
    "            print(f\"Loading {period} {data_type} data...\")\n",
    "\n",
    "            # Load data\n",
    "            experiment_name = experiment[\"name\"] if experiment is not None else None\n",
    "            df = load_data_from_parquet(\n",
    "                experiment_name=experiment_name,\n",
    "                period=period,\n",
    "                data_type=data_type,\n",
    "                data_dir=data_dir,\n",
    "                set_time_index=(data_type == \"position\"),\n",
    "            )\n",
    "\n",
    "            # Trim if requested\n",
    "            if trim_days is not None and len(df) > 0:\n",
    "                if data_type == \"rfid\":\n",
    "                    start_time = df[\"chunk_start\"].min()\n",
    "                    end_time = start_time + pd.Timedelta(days=trim_days)\n",
    "                    df = df[df[\"chunk_start\"] < end_time]\n",
    "                if data_type == \"foraging\":\n",
    "                    start_time = df[\"start\"].min()\n",
    "                    end_time = start_time + pd.Timedelta(days=trim_days)\n",
    "                    df = df[df[\"start\"] < end_time]\n",
    "                if data_type == \"position\":\n",
    "                    start_time = df.index.min()\n",
    "                    end_time = start_time + pd.Timedelta(days=trim_days)\n",
    "                    df = df.loc[df.index < end_time]\n",
    "\n",
    "                print(f\"  Trimmed to {trim_days} days: {len(df)} records\")\n",
    "\n",
    "            # Store in result\n",
    "            key = f\"{period}_{data_type}\"\n",
    "            result[key] = df\n",
    "\n",
    "            # For position data, handle duplicates\n",
    "            if data_type == \"position\" and len(df) > 0:\n",
    "                original_len = len(df)\n",
    "                df = df.reset_index()\n",
    "                df = df.drop_duplicates(subset=[\"time\", \"identity_name\"])\n",
    "                df = df.set_index(\"time\")\n",
    "                result[key] = df\n",
    "                if len(df) < original_len:\n",
    "                    print(f\"  Removed duplicates: {original_len} -> {len(df)}\")\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{note}\n",
    "Change `data_dir` and `save_dir` to the paths where your local dataset (the parquet files) is stored and where you want to save the results.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SET THESE VARIABLES ACCORDINGLY\n",
    "data_dir = Path(\"\")\n",
    "save_dir = Path(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solo vs. Social Behaviours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distance travelled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final df:\n",
    "# rows = hour-datetime,\n",
    "# columns = distance, exp, social-bool, subject, light-bool\n",
    "\n",
    "dist_trav_hour_df = pd.DataFrame(\n",
    "    columns=[\"hour\", \"distance\", \"exp\", \"social\", \"subject\", \"light\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each period\n",
    "# Load pos data\n",
    "# Split into individual dfs\n",
    "# If social, excise swaps\n",
    "# Smooth down to 1s\n",
    "# Calculate hour-by-hour distance traveled, and put into final df\n",
    "\n",
    "exp_pbar = tqdm(experiments, desc=\"Experiments\", position=0, leave=True)\n",
    "for exp in exp_pbar:\n",
    "    period_pbar = tqdm(periods, desc=\"Periods\", position=1, leave=True)\n",
    "    for period in period_pbar:\n",
    "        pos_df = load_data_from_parquet(\n",
    "            experiment_name=exp[\"name\"],\n",
    "            period=period,\n",
    "            data_type=\"position\",\n",
    "            data_dir=data_dir,\n",
    "            set_time_index=True,\n",
    "        )\n",
    "        for subject in pos_df[\"identity_name\"].unique():\n",
    "            pos_df_subj = pos_df[pos_df[\"identity_name\"] == subject]\n",
    "            pos_df_subj = pos_df_subj.resample(\"200ms\").first().dropna(subset=[\"x\"])\n",
    "            pos_df_subj[[\"x\", \"y\"]] = pos_df_subj[[\"x\", \"y\"]].rolling(\"1s\").mean()\n",
    "            pos_df_subj = pos_df_subj.resample(\"1s\").first().dropna(subset=[\"x\"])\n",
    "            pos_df_subj[\"distance\"] = np.sqrt(\n",
    "                (pos_df_subj[\"x\"].diff() ** 2) + (pos_df_subj[\"y\"].diff() ** 2)\n",
    "            )\n",
    "            pos_df_subj.at[pos_df_subj.index[0], \"distance\"] = 0\n",
    "            pos_df_subj[\"distance\"] /= cm2px * 100  # convert to m\n",
    "            pos_df_subj[\"hour\"] = pos_df_subj.index.floor(\"h\")\n",
    "            pos_df_subj_hour = (\n",
    "                pos_df_subj.groupby(\"hour\")[\"distance\"].sum().reset_index()\n",
    "            )\n",
    "            pos_df_subj_hour[\"exp\"] = exp[\"name\"]\n",
    "            pos_df_subj_hour[\"social\"] = period == \"social\"\n",
    "            pos_df_subj_hour[\"subject\"] = subject\n",
    "            hour = pos_df_subj_hour[\"hour\"].dt.hour\n",
    "            pos_df_subj_hour[\"light\"] = ~((hour > light_off) & (hour < light_on))\n",
    "            dist_trav_hour_df = pd.concat(\n",
    "                [dist_trav_hour_df, pos_df_subj_hour], ignore_index=True\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save as parquet\n",
    "# dist_trav_hour_df.to_parquet(\n",
    "#     data_dir / \"for_plots\" / \"dist_trav_hour_df.parquet\",\n",
    "#     engine=\"pyarrow\",\n",
    "#     compression=\"snappy\",\n",
    "#     index=False,\n",
    "# )\n",
    "\n",
    "# Load the parquet file\n",
    "dist_trav_hour_df = pd.read_parquet(\n",
    "    data_dir / \"for_plots\" / \"dist_trav_hour_df.parquet\",\n",
    "    engine=\"pyarrow\",\n",
    ")\n",
    "\n",
    "display(dist_trav_hour_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Hists.\"\"\"\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "# Plot histograms for each combination\n",
    "for i, (social_val, light_val) in enumerate(combos):\n",
    "    # Filter data for this combination\n",
    "    subset = dist_trav_hour_df[\n",
    "        (dist_trav_hour_df[\"social\"] == social_val)\n",
    "        & (dist_trav_hour_df[\"light\"] == light_val)\n",
    "    ]\n",
    "    # Plot normalized histogram\n",
    "    hist = sns.histplot(\n",
    "        data=subset,\n",
    "        x=\"distance\",\n",
    "        stat=\"probability\",  # This normalizes the histogram\n",
    "        alpha=0.5,\n",
    "        color=colors[light_val],\n",
    "        label=labels[i],\n",
    "        # kde=True,  # Add kernel density estimate\n",
    "        common_norm=False,  # Ensure each histogram is normalized separately\n",
    "        axes=ax,\n",
    "        binwidth=20,\n",
    "    )\n",
    "\n",
    "    # Set hatch pattern for bars\n",
    "    if hatches[social_val]:\n",
    "        for bar in hist.patches:\n",
    "            bar.set_hatch(hatches[social_val])\n",
    "\n",
    "ax.set_title(\n",
    "    \"Normalized Distance Traveled Distributions by Social and Light Conditions\"\n",
    ")\n",
    "ax.set_xlabel(\"Distance Traveled (m / h)\")\n",
    "ax.set_ylabel(\"Probability\")\n",
    "ax.legend(title=\"Conditions\")\n",
    "\n",
    "ax.set_ylim(0, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Bars.\"\"\"\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "summary_data = []\n",
    "for social_val in [True, False]:\n",
    "    for light_val in [True, False]:\n",
    "        subset = dist_trav_hour_df[\n",
    "            (dist_trav_hour_df[\"social\"] == social_val)\n",
    "            & (dist_trav_hour_df[\"light\"] == light_val)\n",
    "        ]\n",
    "        mean_dist = subset[\"distance\"].mean()\n",
    "        sem_dist = subset[\"distance\"].sem()\n",
    "        n_samples = len(subset)\n",
    "        summary_data.append(\n",
    "            {\n",
    "                \"social\": social_val,\n",
    "                \"light\": light_val,\n",
    "                \"mean_distance\": mean_dist,\n",
    "                \"sem\": sem_dist,\n",
    "                \"condition\": (\n",
    "                    f\"{'Social' if social_val else 'Solo'}-\"\n",
    "                    f\"{'Light' if light_val else 'Dark'}\",\n",
    "                ),\n",
    "                \"n\": n_samples,\n",
    "            }\n",
    "        )\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "\n",
    "# Set up positions for the bars\n",
    "bar_width = 0.5\n",
    "x_pos = np.array([0.25, 2.25, 0.75, 2.75])  # create two groups with a gap in the middle\n",
    "\n",
    "# Plot bars\n",
    "for i, row in enumerate(summary_data):\n",
    "    pos = x_pos[i]\n",
    "    social_val = row[\"social\"]\n",
    "    light_val = row[\"light\"]\n",
    "\n",
    "    bar = ax.bar(\n",
    "        pos,\n",
    "        row[\"mean_distance\"],\n",
    "        bar_width,\n",
    "        yerr=row[\"sem\"],\n",
    "        color=colors[light_val],\n",
    "        edgecolor=\"black\",\n",
    "        capsize=7,\n",
    "        label=row[\"condition\"],\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f\"Plotting {row['condition']}: mean={row['mean_distance']:.2f}, sem={row['sem']:.2f}, n={row['n']}\"\n",
    "    )\n",
    "\n",
    "    # Apply hatching for social conditions\n",
    "    if hatches[social_val]:\n",
    "        bar[0].set_hatch(hatches[social_val])\n",
    "\n",
    "    # Add sample size as text above each bar\n",
    "    sample_size_txt = ax.text(\n",
    "        pos,\n",
    "        row[\"mean_distance\"] + row[\"sem\"] + 5,\n",
    "        f\"n={row['n']}\",\n",
    "        ha=\"center\",\n",
    "        va=\"bottom\",\n",
    "    )\n",
    "    sample_size_txt.set_fontsize(11)\n",
    "\n",
    "ax.set_ylabel(\"Mean Distance Traveled (m / h)\")\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels([\"Social\\nLight\", \"Social\\nDark\", \"Solo\\nLight\", \"Solo\\nDark\"])\n",
    "ax.set_title(\"Mean Distance Traveled by Social and Light Conditions\")\n",
    "ax.legend(title=\"Conditions\", loc=\"upper left\")\n",
    "ax.xaxis.grid(False)\n",
    "\n",
    "# Add stats tests\n",
    "\n",
    "light_social = dist_trav_hour_df[\n",
    "    (dist_trav_hour_df[\"social\"] == True) & (dist_trav_hour_df[\"light\"] == True)\n",
    "][\"distance\"]\n",
    "light_solo = dist_trav_hour_df[\n",
    "    (dist_trav_hour_df[\"social\"] == False) & (dist_trav_hour_df[\"light\"] == True)\n",
    "][\"distance\"]\n",
    "\n",
    "dark_social = dist_trav_hour_df[\n",
    "    (dist_trav_hour_df[\"social\"] == True) & (dist_trav_hour_df[\"light\"] == False)\n",
    "][\"distance\"]\n",
    "dark_solo = dist_trav_hour_df[\n",
    "    (dist_trav_hour_df[\"social\"] == False) & (dist_trav_hour_df[\"light\"] == False)\n",
    "][\"distance\"]\n",
    "\n",
    "light_stat, light_p = stats.ttest_ind(\n",
    "    light_social, light_solo, alternative=\"two-sided\", equal_var=False\n",
    ")\n",
    "dark_stat, dark_p = stats.ttest_ind(\n",
    "    dark_social, dark_solo, alternative=\"two-sided\", equal_var=False\n",
    ")\n",
    "\n",
    "test_text = (\n",
    "    f\"Two-sample t-tests:\\n\"\n",
    "    f\"Light conditions: p = {light_p:.2e}\\nDark conditions: p = {dark_p:.2e}\"\n",
    ")\n",
    "props = dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightgray\", alpha=0.8)\n",
    "ax.text(\n",
    "    0.02,\n",
    "    0.68,  # Position below the legend\n",
    "    test_text,\n",
    "    transform=ax.transAxes,\n",
    "    fontsize=10,\n",
    "    verticalalignment=\"top\",\n",
    "    bbox=props,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final df:\n",
    "# rows = hour-datetime,\n",
    "# columns = n_bouts, exp, social-bool, subject, light-bool\n",
    "\n",
    "explore_hour_df = pd.DataFrame(\n",
    "    columns=[\"hour\", \"n_bouts\", \"exp\", \"social\", \"subject\", \"light\"]\n",
    ")\n",
    "explore_dur_df = pd.DataFrame(\n",
    "    columns=[\"start\", \"duration\", \"exp\", \"social\", \"subject\", \"light\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_pbar = tqdm(experiments, desc=\"Experiments\", position=0, leave=True)\n",
    "for exp in exp_pbar:\n",
    "    period_pbar = tqdm(periods, desc=\"Periods\", position=1, leave=False)\n",
    "    for period in period_pbar:\n",
    "        explore_bouts_df = load_data_from_parquet(\n",
    "            experiment_name=exp[\"name\"],\n",
    "            period=period,\n",
    "            data_type=\"explore\",\n",
    "            data_dir=data_dir,\n",
    "            set_time_index=True,\n",
    "        )\n",
    "        for subject in explore_bouts_df[\"subject\"].unique():\n",
    "            explore_df_subj = explore_bouts_df[explore_bouts_df[\"subject\"] == subject]\n",
    "            explore_df_subj[\"hour\"] = explore_df_subj[\"start\"].dt.floor(\"h\")\n",
    "            min_hour, max_hour = (\n",
    "                explore_df_subj[\"hour\"].min(),\n",
    "                explore_df_subj[\"hour\"].max(),\n",
    "            )\n",
    "            complete_hours = pd.DataFrame(\n",
    "                {\"hour\": pd.date_range(start=min_hour, end=max_hour, freq=\"h\")}\n",
    "            )\n",
    "            hour_counts = (\n",
    "                explore_df_subj.groupby(\"hour\").size().reset_index(name=\"n_bouts\")\n",
    "            )\n",
    "            explore_df_subj_hour = pd.merge(\n",
    "                complete_hours, hour_counts, on=\"hour\", how=\"left\"\n",
    "            ).fillna(0)\n",
    "            explore_df_subj_hour[\"n_bouts\"] = explore_df_subj_hour[\"n_bouts\"].astype(\n",
    "                int\n",
    "            )\n",
    "            explore_df_subj_hour[\"exp\"] = exp[\"name\"]\n",
    "            explore_df_subj_hour[\"social\"] = period == \"social\"\n",
    "            explore_df_subj_hour[\"subject\"] = subject\n",
    "            hour = explore_df_subj_hour[\"hour\"].dt.hour\n",
    "            explore_df_subj_hour[\"light\"] = ~((hour > light_off) & (hour < light_on))\n",
    "            explore_hour_df = pd.concat(\n",
    "                [explore_hour_df, explore_df_subj_hour], ignore_index=True\n",
    "            )\n",
    "\n",
    "            explore_dur_subj = explore_df_subj[[\"start\", \"duration\"]].copy()\n",
    "            explore_dur_subj[\"exp\"] = exp[\"name\"]\n",
    "            explore_dur_subj[\"social\"] = period == \"social\"\n",
    "            explore_dur_subj[\"subject\"] = subject\n",
    "            hour = explore_dur_subj[\"start\"].dt.hour\n",
    "            explore_dur_subj[\"light\"] = ~((hour > light_off) & (hour < light_on))\n",
    "            explore_dur_df = pd.concat(\n",
    "                [explore_dur_df, explore_dur_subj], ignore_index=True\n",
    "            )\n",
    "\n",
    "explore_dur_df[\"duration\"] = explore_dur_df[\"duration\"].dt.total_seconds() / 60\n",
    "explore_dur_df = explore_dur_df[explore_dur_df[\"duration\"] < 120]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Plot hist of bouts per hour\"\"\"\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "# Plot histograms for each combination\n",
    "for i, (social_val, light_val) in enumerate(combos):\n",
    "    subset = explore_hour_df[\n",
    "        (explore_hour_df[\"social\"] == social_val)\n",
    "        & (explore_hour_df[\"light\"] == light_val)\n",
    "    ]\n",
    "    # Plot normalized histogram\n",
    "    hist = sns.histplot(\n",
    "        data=subset,\n",
    "        x=\"n_bouts\",\n",
    "        stat=\"probability\",\n",
    "        alpha=0.5,\n",
    "        color=colors[light_val],\n",
    "        label=labels[i],\n",
    "        common_norm=False,  # Ensure each histogram is normalized separately\n",
    "        axes=ax,\n",
    "        binwidth=1,\n",
    "    )\n",
    "\n",
    "    # Set hatch pattern for bars\n",
    "    if hatches[social_val]:\n",
    "        for bar in hist.patches:\n",
    "            bar.set_hatch(hatches[social_val])\n",
    "\n",
    "ax.set_title(\"Normalized Exploration Bout Distributions by Social and Light Conditions\")\n",
    "ax.set_xlabel(\"Number of bouts / hour\")\n",
    "ax.set_ylabel(\"Probability\")\n",
    "ax.legend(title=\"Conditions\")\n",
    "\n",
    "ax.set_xticks(np.arange(0, 15, 2))\n",
    "ax.set_xlim(0, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Plot bars of bouts per hour\"\"\"\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "summary_data = []\n",
    "for social_val in [True, False]:\n",
    "    for light_val in [True, False]:\n",
    "        subset = explore_hour_df[\n",
    "            (explore_hour_df[\"social\"] == social_val)\n",
    "            & (explore_hour_df[\"light\"] == light_val)\n",
    "        ]\n",
    "        mean_n_bouts = subset[\"n_bouts\"].mean()\n",
    "        sem_n_bouts = subset[\"n_bouts\"].sem()\n",
    "        n_samples = len(subset)\n",
    "        summary_data.append(\n",
    "            {\n",
    "                \"social\": social_val,\n",
    "                \"light\": light_val,\n",
    "                \"mean_n_bouts\": mean_n_bouts,\n",
    "                \"sem\": sem_n_bouts,\n",
    "                \"condition\": f\"{'Social' if social_val else 'Solo'}-{'Light' if light_val else 'Dark'}\",\n",
    "                \"n\": n_samples,\n",
    "            }\n",
    "        )\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "\n",
    "# Set up positions for the bars\n",
    "bar_width = 0.5\n",
    "x_pos = np.array([0.25, 2.25, 0.75, 2.75])  # create two groups with a gap in the middle\n",
    "\n",
    "# Plot bars\n",
    "for i, row in enumerate(summary_data):\n",
    "    pos = x_pos[i]\n",
    "    social_val = row[\"social\"]\n",
    "    light_val = row[\"light\"]\n",
    "\n",
    "    bar = ax.bar(\n",
    "        pos,\n",
    "        row[\"mean_n_bouts\"],\n",
    "        bar_width,\n",
    "        yerr=row[\"sem\"],\n",
    "        color=colors[light_val],\n",
    "        edgecolor=\"black\",\n",
    "        capsize=7,\n",
    "        label=row[\"condition\"],\n",
    "    )\n",
    "\n",
    "    # Apply hatching for social conditions\n",
    "    if hatches[social_val]:\n",
    "        bar[0].set_hatch(hatches[social_val])\n",
    "\n",
    "    # Add sample size as text above each bar\n",
    "    sample_size_txt = ax.text(\n",
    "        pos,\n",
    "        row[\"mean_n_bouts\"] + row[\"sem\"] + 0.1,\n",
    "        f\"n={row['n']}\",\n",
    "        ha=\"center\",\n",
    "        va=\"bottom\",\n",
    "    )\n",
    "    sample_size_txt.set_fontsize(11)\n",
    "\n",
    "ax.set_title(\"Mean Number of Exploration Bouts by Social and Light Conditions\")\n",
    "ax.set_ylabel(\"Number of bouts / hour\")\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels([\"Social\\nLight\", \"Social\\nDark\", \"Solo\\nLight\", \"Solo\\nDark\"])\n",
    "ax.legend(title=\"Conditions\", loc=\"upper left\")\n",
    "ax.xaxis.grid(False)\n",
    "\n",
    "# Perform Wilcoxon rank sum tests (Mann-Whitney U)\n",
    "light_social = explore_hour_df[\n",
    "    (explore_hour_df[\"social\"] == True) & (explore_hour_df[\"light\"] == True)\n",
    "][\"n_bouts\"]\n",
    "light_solo = explore_hour_df[\n",
    "    (explore_hour_df[\"social\"] == False) & (explore_hour_df[\"light\"] == True)\n",
    "][\"n_bouts\"]\n",
    "\n",
    "dark_social = explore_hour_df[\n",
    "    (explore_hour_df[\"social\"] == True) & (explore_hour_df[\"light\"] == False)\n",
    "][\"n_bouts\"]\n",
    "dark_solo = explore_hour_df[\n",
    "    (explore_hour_df[\"social\"] == False) & (explore_hour_df[\"light\"] == False)\n",
    "][\"n_bouts\"]\n",
    "\n",
    "light_social = pd.to_numeric(light_social, errors=\"coerce\").dropna()\n",
    "light_solo = pd.to_numeric(light_solo, errors=\"coerce\").dropna()\n",
    "dark_social = pd.to_numeric(dark_social, errors=\"coerce\").dropna()\n",
    "dark_solo = pd.to_numeric(dark_solo, errors=\"coerce\").dropna()\n",
    "\n",
    "light_stat, light_p = stats.ttest_ind(\n",
    "    light_social, light_solo, alternative=\"two-sided\", equal_var=False\n",
    ")\n",
    "dark_stat, dark_p = stats.ttest_ind(\n",
    "    dark_social, dark_solo, alternative=\"two-sided\", equal_var=False\n",
    ")\n",
    "\n",
    "test_text = (\n",
    "    f\"Two-sample t-tests:\\n\"\n",
    "    f\"Light conditions: p = {light_p:.2e}\\nDark conditions: p = {dark_p:.2e}\"\n",
    ")\n",
    "props = dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightgray\", alpha=0.8)\n",
    "ax.text(\n",
    "    0.02,\n",
    "    0.68,  # Position below the legend\n",
    "    test_text,\n",
    "    transform=ax.transAxes,\n",
    "    fontsize=10,\n",
    "    verticalalignment=\"top\",\n",
    "    bbox=props,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Plot hist of durations of bouts.\"\"\"\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "# Plot histograms for each combination\n",
    "for i, (social_val, light_val) in enumerate(combos):\n",
    "    subset = explore_dur_df[\n",
    "        (explore_dur_df[\"social\"] == social_val)\n",
    "        & (explore_dur_df[\"light\"] == light_val)\n",
    "    ]\n",
    "    # Plot normalized histogram\n",
    "    hist = sns.histplot(\n",
    "        data=subset,\n",
    "        x=\"duration\",\n",
    "        stat=\"probability\",\n",
    "        alpha=0.5,\n",
    "        color=colors[light_val],\n",
    "        label=labels[i],\n",
    "        # kde=True,  # Add kernel density estimate\n",
    "        common_norm=False,  # Ensure each histogram is normalized separately\n",
    "        axes=ax,\n",
    "        binwidth=2,\n",
    "    )\n",
    "\n",
    "    # Set hatch pattern for bars\n",
    "    if hatches[social_val]:\n",
    "        for bar in hist.patches:\n",
    "            bar.set_hatch(hatches[social_val])\n",
    "\n",
    "ax.set_title(\n",
    "    \"Normalized Exploration Bout Duration Distributions by Social and Light Conditions\"\n",
    ")\n",
    "ax.set_xlabel(\"Duration (mins)\")\n",
    "ax.set_ylabel(\"Probability\")\n",
    "ax.legend(title=\"Conditions\")\n",
    "\n",
    "ax.set_ylim(0, 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Plot bars of durations of bouts.\"\"\"\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "summary_data = []\n",
    "for social_val in [True, False]:\n",
    "    for light_val in [True, False]:\n",
    "        subset = explore_dur_df[\n",
    "            (explore_dur_df[\"social\"] == social_val)\n",
    "            & (explore_dur_df[\"light\"] == light_val)\n",
    "        ]\n",
    "        mean_duration = subset[\"duration\"].mean()\n",
    "        sem_duration = subset[\"duration\"].sem()\n",
    "        n_samples = len(subset)\n",
    "        summary_data.append(\n",
    "            {\n",
    "                \"social\": social_val,\n",
    "                \"light\": light_val,\n",
    "                \"mean_duration\": mean_duration,\n",
    "                \"sem\": sem_duration,\n",
    "                \"condition\": f\"{'Social' if social_val else 'Solo'}-{'Light' if light_val else 'Dark'}\",\n",
    "                \"n\": n_samples,\n",
    "            }\n",
    "        )\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "\n",
    "# Set up positions for the bars\n",
    "bar_width = 0.5\n",
    "x_pos = np.array([0.25, 2.25, 0.75, 2.75])  # create two groups with a gap in the middle\n",
    "\n",
    "# Plot bars\n",
    "for i, row in enumerate(summary_data):\n",
    "    pos = x_pos[i]\n",
    "    social_val = row[\"social\"]\n",
    "    light_val = row[\"light\"]\n",
    "\n",
    "    bar = ax.bar(\n",
    "        pos,\n",
    "        row[\"mean_duration\"],\n",
    "        bar_width,\n",
    "        yerr=row[\"sem\"],\n",
    "        color=colors[light_val],\n",
    "        edgecolor=\"black\",\n",
    "        capsize=7,\n",
    "        label=row[\"condition\"],\n",
    "    )\n",
    "\n",
    "    # Apply hatching for social conditions\n",
    "    if hatches[social_val]:\n",
    "        bar[0].set_hatch(hatches[social_val])\n",
    "\n",
    "    # Add sample size as text above each bar\n",
    "    sample_size_txt = ax.text(\n",
    "        pos,\n",
    "        row[\"mean_duration\"] + row[\"sem\"] + 0.2,\n",
    "        f\"n={row['n']}\",\n",
    "        ha=\"center\",\n",
    "        va=\"bottom\",\n",
    "    )\n",
    "    sample_size_txt.set_fontsize(11)\n",
    "\n",
    "ax.set_title(\"Mean Exploration Bout Duration by Social and Light Conditions\")\n",
    "ax.set_ylabel(\"Duration (minutes)\")\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels([\"Social\\nLight\", \"Social\\nDark\", \"Solo\\nLight\", \"Solo\\nDark\"])\n",
    "ax.legend(title=\"Conditions\", loc=\"upper left\")\n",
    "ax.xaxis.grid(False)\n",
    "\n",
    "# Perform Wilcoxon rank sum tests (Mann-Whitney U)\n",
    "light_social = explore_dur_df[\n",
    "    (explore_dur_df[\"social\"] == True) & (explore_dur_df[\"light\"] == True)\n",
    "][\"duration\"]\n",
    "light_solo = explore_dur_df[\n",
    "    (explore_dur_df[\"social\"] == False) & (explore_dur_df[\"light\"] == True)\n",
    "][\"duration\"]\n",
    "\n",
    "dark_social = explore_dur_df[\n",
    "    (explore_dur_df[\"social\"] == True) & (explore_dur_df[\"light\"] == False)\n",
    "][\"duration\"]\n",
    "dark_solo = explore_dur_df[\n",
    "    (explore_dur_df[\"social\"] == False) & (explore_dur_df[\"light\"] == False)\n",
    "][\"duration\"]\n",
    "\n",
    "light_social = pd.to_numeric(light_social, errors=\"coerce\").dropna()\n",
    "light_solo = pd.to_numeric(light_solo, errors=\"coerce\").dropna()\n",
    "dark_social = pd.to_numeric(dark_social, errors=\"coerce\").dropna()\n",
    "dark_solo = pd.to_numeric(dark_solo, errors=\"coerce\").dropna()\n",
    "\n",
    "light_stat, light_p = stats.ttest_ind(\n",
    "    light_social, light_solo, alternative=\"two-sided\", equal_var=False\n",
    ")\n",
    "dark_stat, dark_p = stats.ttest_ind(\n",
    "    dark_social, dark_solo, alternative=\"two-sided\", equal_var=False\n",
    ")\n",
    "\n",
    "test_text = (\n",
    "    f\"Two-sample t-tests:\\n\"\n",
    "    f\"Light conditions: p = {light_p:.2e}\\nDark conditions: p = {dark_p:.2e}\"\n",
    ")\n",
    "props = dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightgray\", alpha=0.8)\n",
    "ax.text(\n",
    "    0.02,\n",
    "    0.68,  # Position below the legend\n",
    "    test_text,\n",
    "    transform=ax.transAxes,\n",
    "    fontsize=10,\n",
    "    verticalalignment=\"top\",\n",
    "    bbox=props,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Plot hist of times of bouts over all hours.\"\"\"\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "for i, social_val in enumerate([True, False]):\n",
    "    subset = explore_dur_df[(explore_dur_df[\"social\"] == social_val)]\n",
    "\n",
    "    # Create the histogram\n",
    "    hist = sns.histplot(\n",
    "        data=subset,\n",
    "        x=subset[\"start\"].dt.hour,\n",
    "        stat=\"probability\",  # Normalize to show probability\n",
    "        alpha=0.5,\n",
    "        color=\"teal\",\n",
    "        label=\"Social\" if social_val else \"Solo\",\n",
    "        common_norm=False,  # Each condition normalized separately\n",
    "        ax=ax,\n",
    "        bins=24,  # 24 hours\n",
    "        discrete=True,  # Since hours are discrete values\n",
    "    )\n",
    "\n",
    "    # Apply hatching pattern for social conditions\n",
    "    if hatches[social_val]:\n",
    "        # Apply the hatch pattern to each bar\n",
    "        for patch in hist.patches:\n",
    "            patch.set_hatch(hatches[social_val])\n",
    "\n",
    "# Set x-tick labels for every hour\n",
    "ax.set_xticks(range(0, 24))\n",
    "ax.set_xticklabels([f\"{h:02d}:00\" for h in range(0, 24)], rotation=45)\n",
    "\n",
    "# Customize axis labels and title\n",
    "ax.set_title(\"Distribution of Exploration Bouts Throughout the Day\")\n",
    "ax.set_xlabel(\"Hour of Day\")\n",
    "ax.set_ylabel(\"Probability\")\n",
    "ax.legend(title=\"Conditions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Foraging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final dfs:\n",
    "# 1. forage_hour_df: hour, n_pellets, dist_forage, n_bouts, exp, social-bool, subject, light-bool\n",
    "# 2. forage_dur_df: start, duration(mins), exp, social-bool, subject, light-bool\n",
    "\n",
    "forage_hour_df = pd.DataFrame(\n",
    "    columns=[\n",
    "        \"hour\",\n",
    "        \"n_bouts\",\n",
    "        \"n_pellets\",\n",
    "        \"dist_forage\",\n",
    "        \"exp\",\n",
    "        \"social\",\n",
    "        \"subject\",\n",
    "        \"light\",\n",
    "    ]\n",
    ")\n",
    "forage_dur_df = pd.DataFrame(\n",
    "    columns=[\"start\", \"duration\", \"exp\", \"social\", \"subject\", \"light\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each period\n",
    "# Load foraging data\n",
    "# Split into individual dfs\n",
    "# Calculate hour-by-hour metrics and put into final df\n",
    "\n",
    "exp_pbar = tqdm(experiments, desc=\"Experiments\", position=0, leave=True)\n",
    "for exp in exp_pbar:\n",
    "    period_pbar = tqdm(periods, desc=\"Periods\", position=1, leave=False)\n",
    "    for period in period_pbar:\n",
    "        forage_df = load_data_from_parquet(\n",
    "            experiment_name=exp[\"name\"],\n",
    "            period=period,\n",
    "            data_type=\"foraging\",\n",
    "            data_dir=data_dir,\n",
    "            set_time_index=True,\n",
    "        )\n",
    "        for subject in forage_df[\"subject\"].unique():\n",
    "            forage_df_subj = forage_df[forage_df[\"subject\"] == subject]\n",
    "            forage_df_subj[\"hour\"] = forage_df_subj[\"start\"].dt.floor(\"h\")\n",
    "            hour_counts = pd.merge(\n",
    "                forage_df_subj.groupby(\"hour\").size().reset_index(name=\"n_bouts\"),\n",
    "                forage_df_subj.groupby(\"hour\").agg(\n",
    "                    n_pellets=(\"n_pellets\", \"sum\"),\n",
    "                    cum_wheel_dist=(\"cum_wheel_dist\", \"sum\"),\n",
    "                ),\n",
    "                on=\"hour\",\n",
    "                how=\"left\",\n",
    "            )\n",
    "            min_hour, max_hour = (\n",
    "                forage_df_subj[\"hour\"].min(),\n",
    "                forage_df_subj[\"hour\"].max(),\n",
    "            )\n",
    "            complete_hours = pd.DataFrame(\n",
    "                {\"hour\": pd.date_range(start=min_hour, end=max_hour, freq=\"h\")}\n",
    "            )\n",
    "            forage_df_subj_hour = pd.merge(\n",
    "                complete_hours, hour_counts, on=\"hour\", how=\"left\"\n",
    "            ).fillna(0)\n",
    "            forage_df_subj_hour[\"n_bouts\"] = forage_df_subj_hour[\"n_bouts\"].astype(int)\n",
    "            # Rename 'cum_wheel_dist' col\n",
    "            forage_df_subj_hour.rename(\n",
    "                columns={\"cum_wheel_dist\": \"dist_forage\"}, inplace=True\n",
    "            )\n",
    "            forage_df_subj_hour[\"exp\"] = exp[\"name\"]\n",
    "            forage_df_subj_hour[\"social\"] = period == \"social\"\n",
    "            forage_df_subj_hour[\"subject\"] = subject\n",
    "            hour = forage_df_subj_hour[\"hour\"].dt.hour\n",
    "            forage_df_subj_hour[\"light\"] = ~((hour > light_off) & (hour < light_on))\n",
    "            forage_hour_df = pd.concat(\n",
    "                [forage_hour_df, forage_df_subj_hour], ignore_index=True\n",
    "            )\n",
    "\n",
    "            forage_dur_subj = forage_df_subj[[\"start\"]].copy()\n",
    "            forage_dur_subj[\"duration\"] = (\n",
    "                forage_df_subj[\"end\"] - forage_df_subj[\"start\"]\n",
    "            ).dt.total_seconds() / 60\n",
    "            forage_dur_subj[\"exp\"] = exp[\"name\"]\n",
    "            forage_dur_subj[\"social\"] = period == \"social\"\n",
    "            forage_dur_subj[\"subject\"] = subject\n",
    "            hour = forage_df_subj[\"start\"].dt.hour\n",
    "            forage_dur_subj[\"light\"] = ~((hour > light_off) & (hour < light_on))\n",
    "            forage_dur_df = pd.concat(\n",
    "                [forage_dur_df, forage_dur_subj], ignore_index=True\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Foraging bouts per hour histogram.\"\"\"\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "# Plot histograms for each combination\n",
    "for i, (social_val, light_val) in enumerate(combos):\n",
    "    subset = forage_hour_df[\n",
    "        (forage_hour_df[\"social\"] == social_val)\n",
    "        & (forage_hour_df[\"light\"] == light_val)\n",
    "        & (forage_hour_df[\"n_pellets\"] > 0)\n",
    "    ]\n",
    "    # Plot normalized histogram\n",
    "    hist = sns.histplot(\n",
    "        data=subset,\n",
    "        x=\"n_bouts\",\n",
    "        stat=\"probability\",\n",
    "        alpha=0.5,\n",
    "        color=colors[light_val],\n",
    "        label=labels[i],\n",
    "        # kde=True,  # Add kernel density estimate\n",
    "        common_norm=False,  # Ensure each histogram is normalized separately\n",
    "        axes=ax,\n",
    "        binwidth=1,\n",
    "    )\n",
    "\n",
    "    # Set hatch pattern for bars\n",
    "    if hatches[social_val]:\n",
    "        for bar in hist.patches:\n",
    "            bar.set_hatch(hatches[social_val])\n",
    "\n",
    "ax.set_title(\"Normalized Foraging Bout Distributions by Social and Light Conditions\")\n",
    "ax.set_xlabel(\"Foraging bouts / hour\")\n",
    "ax.set_ylabel(\"Probability\")\n",
    "ax.legend(title=\"Conditions\")\n",
    "\n",
    "ax.set_xlim(1, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Foraging bouts per hour bars.\"\"\"\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "summary_data = []\n",
    "for social_val in [True, False]:\n",
    "    for light_val in [True, False]:\n",
    "        subset = forage_hour_df[\n",
    "            (forage_hour_df[\"social\"] == social_val)\n",
    "            & (forage_hour_df[\"light\"] == light_val)\n",
    "        ]\n",
    "        mean_n_bouts = subset[\"n_bouts\"].mean()\n",
    "        sem_n_bouts = subset[\"n_bouts\"].sem()\n",
    "        n_samples = len(subset)\n",
    "        summary_data.append(\n",
    "            {\n",
    "                \"social\": social_val,\n",
    "                \"light\": light_val,\n",
    "                \"mean_n_bouts\": mean_n_bouts,\n",
    "                \"sem\": sem_n_bouts,\n",
    "                \"condition\": f\"{'Social' if social_val else 'Solo'}-{'Light' if light_val else 'Dark'}\",\n",
    "                \"n\": n_samples,\n",
    "            }\n",
    "        )\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "\n",
    "# Set up positions for the bars\n",
    "bar_width = 0.5\n",
    "x_pos = np.array([0.25, 2.25, 0.75, 2.75])  # create two groups with a gap in the middle\n",
    "\n",
    "# Plot bars\n",
    "for i, row in enumerate(summary_data):\n",
    "    pos = x_pos[i]\n",
    "    social_val = row[\"social\"]\n",
    "    light_val = row[\"light\"]\n",
    "\n",
    "    bar = ax.bar(\n",
    "        pos,\n",
    "        row[\"mean_n_bouts\"],\n",
    "        bar_width,\n",
    "        yerr=row[\"sem\"],\n",
    "        color=colors[light_val],\n",
    "        edgecolor=\"black\",\n",
    "        capsize=7,\n",
    "        label=row[\"condition\"],\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f\"Plotting {row['condition']}: mean={row['mean_n_bouts']:.2f}, sem={row['sem']:.2f}, n={row['n']}\"\n",
    "    )\n",
    "\n",
    "    # Apply hatching for social conditions\n",
    "    if hatches[social_val]:\n",
    "        bar[0].set_hatch(hatches[social_val])\n",
    "\n",
    "    # Add sample size as text above each bar\n",
    "    sample_size_txt = ax.text(\n",
    "        pos,\n",
    "        row[\"mean_n_bouts\"] + row[\"sem\"] + 0.1,\n",
    "        f\"n={row['n']}\",\n",
    "        ha=\"center\",\n",
    "        va=\"bottom\",\n",
    "    )\n",
    "    sample_size_txt.set_fontsize(11)\n",
    "\n",
    "ax.set_title(\"Mean Number of Foraging Bouts per Hour by Social and Light Conditions\")\n",
    "ax.set_ylabel(\"Number of bouts / hour\")\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels([\"Social\\nLight\", \"Social\\nDark\", \"Solo\\nLight\", \"Solo\\nDark\"])\n",
    "ax.legend(title=\"Conditions\", loc=\"upper left\")\n",
    "ax.xaxis.grid(False)\n",
    "\n",
    "# Wilcoxon rank sum tests\n",
    "light_social = forage_hour_df[\n",
    "    (forage_hour_df[\"social\"] == True) & (forage_hour_df[\"light\"] == True)\n",
    "][\"n_bouts\"]\n",
    "light_solo = forage_hour_df[\n",
    "    (forage_hour_df[\"social\"] == False) & (forage_hour_df[\"light\"] == True)\n",
    "][\"n_bouts\"]\n",
    "\n",
    "dark_social = forage_hour_df[\n",
    "    (forage_hour_df[\"social\"] == True) & (forage_hour_df[\"light\"] == False)\n",
    "][\"n_bouts\"]\n",
    "dark_solo = forage_hour_df[\n",
    "    (forage_hour_df[\"social\"] == False) & (forage_hour_df[\"light\"] == False)\n",
    "][\"n_bouts\"]\n",
    "\n",
    "light_social = pd.to_numeric(light_social, errors=\"coerce\").dropna()\n",
    "light_solo = pd.to_numeric(light_solo, errors=\"coerce\").dropna()\n",
    "dark_social = pd.to_numeric(dark_social, errors=\"coerce\").dropna()\n",
    "dark_solo = pd.to_numeric(dark_solo, errors=\"coerce\").dropna()\n",
    "\n",
    "light_stat, light_p = stats.ttest_ind(\n",
    "    light_social, light_solo, alternative=\"two-sided\", equal_var=False\n",
    ")\n",
    "dark_stat, dark_p = stats.ttest_ind(\n",
    "    dark_social, dark_solo, alternative=\"two-sided\", equal_var=False\n",
    ")\n",
    "\n",
    "test_text = (\n",
    "    f\"Two-sample t-tests:\\n\"\n",
    "    f\"Light conditions: p = {light_p:.2e}\\nDark conditions: p = {dark_p:.2e}\"\n",
    ")\n",
    "props = dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightgray\", alpha=0.8)\n",
    "ax.text(\n",
    "    0.02,\n",
    "    0.68,\n",
    "    test_text,\n",
    "    transform=ax.transAxes,\n",
    "    fontsize=10,\n",
    "    verticalalignment=\"top\",\n",
    "    bbox=props,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Foraging bouts duration histogram.\"\"\"\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "# Plot histograms for each combination\n",
    "for i, (social_val, light_val) in enumerate(combos):\n",
    "    subset = forage_dur_df[\n",
    "        (forage_dur_df[\"social\"] == social_val) & (forage_dur_df[\"light\"] == light_val)\n",
    "    ]\n",
    "    # Plot normalized histogram\n",
    "    hist = sns.histplot(\n",
    "        data=subset,\n",
    "        x=\"duration\",\n",
    "        stat=\"probability\",\n",
    "        alpha=0.5,\n",
    "        color=colors[light_val],\n",
    "        label=labels[i],\n",
    "        # kde=True,  # Add kernel density estimate\n",
    "        common_norm=False,  # Ensure each histogram is normalized separately\n",
    "        axes=ax,\n",
    "        binwidth=1,\n",
    "    )\n",
    "\n",
    "    # Set hatch pattern for bars\n",
    "    if hatches[social_val]:\n",
    "        for bar in hist.patches:\n",
    "            bar.set_hatch(hatches[social_val])\n",
    "\n",
    "ax.set_title(\n",
    "    \"Normalized Foraging Bout Duration Distributions by Social and Light Conditions\"\n",
    ")\n",
    "ax.set_xlabel(\"Duration (mins)\")\n",
    "ax.set_ylabel(\"Probability\")\n",
    "ax.legend(title=\"Conditions\")\n",
    "\n",
    "ax.set_xlim(0, 20)\n",
    "# ax.set_ylim(0, 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Foraging bouts duration bars.\"\"\"\n",
    "\n",
    "max_forage_thresh = 30  # in minutes\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "summary_data = []\n",
    "for social_val in [True, False]:\n",
    "    for light_val in [True, False]:\n",
    "        subset = forage_dur_df[\n",
    "            (forage_dur_df[\"social\"] == social_val)\n",
    "            & (forage_dur_df[\"light\"] == light_val)\n",
    "            & (forage_dur_df[\"duration\"] < max_forage_thresh)\n",
    "        ]\n",
    "        mean_duration = subset[\"duration\"].mean()\n",
    "        sem_duration = subset[\"duration\"].sem()\n",
    "        n_samples = len(subset)\n",
    "        summary_data.append(\n",
    "            {\n",
    "                \"social\": social_val,\n",
    "                \"light\": light_val,\n",
    "                \"mean_duration\": mean_duration,\n",
    "                \"sem\": sem_duration,\n",
    "                \"condition\": f\"{'Social' if social_val else 'Solo'}-{'Light' if light_val else 'Dark'}\",\n",
    "                \"n\": n_samples,\n",
    "            }\n",
    "        )\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "\n",
    "# Set up positions for the bars\n",
    "bar_width = 0.5\n",
    "x_pos = np.array([0.25, 2.25, 0.75, 2.75])  # create two groups with a gap in the middle\n",
    "\n",
    "# Plot bars\n",
    "for i, row in enumerate(summary_data):\n",
    "    pos = x_pos[i]\n",
    "    social_val = row[\"social\"]\n",
    "    light_val = row[\"light\"]\n",
    "\n",
    "    bar = ax.bar(\n",
    "        pos,\n",
    "        row[\"mean_duration\"],\n",
    "        bar_width,\n",
    "        yerr=row[\"sem\"],\n",
    "        color=colors[light_val],\n",
    "        edgecolor=\"black\",\n",
    "        capsize=7,\n",
    "        label=row[\"condition\"],\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f\"Plotting {row['condition']}: mean={row['mean_duration']:.2f}, sem={row['sem']:.2f}, n={row['n']}\"\n",
    "    )\n",
    "\n",
    "    # Apply hatching for social conditions\n",
    "    if hatches[social_val]:\n",
    "        bar[0].set_hatch(hatches[social_val])\n",
    "\n",
    "    # Add sample size as text above each bar\n",
    "    sample_size_txt = ax.text(\n",
    "        pos,\n",
    "        row[\"mean_duration\"] + row[\"sem\"] + 0.1,\n",
    "        f\"n={row['n']}\",\n",
    "        ha=\"center\",\n",
    "        va=\"bottom\",\n",
    "    )\n",
    "    sample_size_txt.set_fontsize(11)\n",
    "\n",
    "ax.set_title(\"Mean Duration of Foraging Bouts by Social and Light Conditions\")\n",
    "ax.set_ylabel(\"Mean Duration (minutes)\")\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels([\"Social\\nLight\", \"Social\\nDark\", \"Solo\\nLight\", \"Solo\\nDark\"])\n",
    "ax.legend(title=\"Conditions\", loc=\"upper right\")\n",
    "ax.xaxis.grid(False)\n",
    "\n",
    "# Wilcoxon rank sum tests\n",
    "\n",
    "light_social = forage_dur_df[\n",
    "    (forage_dur_df[\"social\"] == True)\n",
    "    & (forage_dur_df[\"light\"] == True)\n",
    "    & (forage_dur_df[\"duration\"] < max_forage_thresh)\n",
    "][\"duration\"]\n",
    "light_solo = forage_dur_df[\n",
    "    (forage_dur_df[\"social\"] == False)\n",
    "    & (forage_dur_df[\"light\"] == True)\n",
    "    & (forage_dur_df[\"duration\"] < max_forage_thresh)\n",
    "][\"duration\"]\n",
    "\n",
    "dark_social = forage_dur_df[\n",
    "    (forage_dur_df[\"social\"] == True)\n",
    "    & (forage_dur_df[\"light\"] == False)\n",
    "    & (forage_dur_df[\"duration\"] < max_forage_thresh)\n",
    "][\"duration\"]\n",
    "dark_solo = forage_dur_df[\n",
    "    (forage_dur_df[\"social\"] == False)\n",
    "    & (forage_dur_df[\"light\"] == False)\n",
    "    & (forage_dur_df[\"duration\"] < max_forage_thresh)\n",
    "][\"duration\"]\n",
    "\n",
    "light_social = pd.to_numeric(light_social, errors=\"coerce\").dropna()\n",
    "light_solo = pd.to_numeric(light_solo, errors=\"coerce\").dropna()\n",
    "dark_social = pd.to_numeric(dark_social, errors=\"coerce\").dropna()\n",
    "dark_solo = pd.to_numeric(dark_solo, errors=\"coerce\").dropna()\n",
    "\n",
    "light_stat, light_p = stats.ttest_ind(\n",
    "    light_social, light_solo, alternative=\"two-sided\", equal_var=False\n",
    ")\n",
    "dark_stat, dark_p = stats.ttest_ind(\n",
    "    dark_social, dark_solo, alternative=\"two-sided\", equal_var=False\n",
    ")\n",
    "\n",
    "test_text = (\n",
    "    f\"Two-sample t-tests:\\n\"\n",
    "    f\"Light conditions: p = {light_p:.2e}\\nDark conditions: p = {dark_p:.2e}\"\n",
    ")\n",
    "props = dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightgray\", alpha=0.8)\n",
    "ax.text(\n",
    "    0.80,\n",
    "    0.68,\n",
    "    test_text,\n",
    "    transform=ax.transAxes,\n",
    "    fontsize=10,\n",
    "    verticalalignment=\"top\",\n",
    "    bbox=props,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Foraging bouts over all hours histogram.\"\"\"\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "for i, social_val in enumerate([True, False]):\n",
    "    subset = forage_dur_df[(forage_dur_df[\"social\"] == social_val)]\n",
    "\n",
    "    # Create the histogram\n",
    "    hist = sns.histplot(\n",
    "        data=subset,\n",
    "        x=subset[\"start\"].dt.hour,\n",
    "        stat=\"probability\",  # Normalize to show probability\n",
    "        alpha=0.5,\n",
    "        color=\"teal\",\n",
    "        label=\"Social\" if social_val else \"Solo\",\n",
    "        common_norm=False,  # Each condition normalized separately\n",
    "        ax=ax,\n",
    "        bins=24,  # 24 hours\n",
    "        discrete=True,  # Since hours are discrete values\n",
    "    )\n",
    "\n",
    "    # Apply hatching pattern for social conditions\n",
    "    if hatches[social_val]:\n",
    "        # Apply the hatch pattern to each bar\n",
    "        for patch in hist.patches:\n",
    "            patch.set_hatch(hatches[social_val])\n",
    "\n",
    "# Set x-tick labels for every hour\n",
    "ax.set_xticks(range(0, 24))\n",
    "ax.set_xticklabels([f\"{h:02d}:00\" for h in range(0, 24)], rotation=45)\n",
    "\n",
    "# Customize axis labels and title\n",
    "ax.set_title(\"Distribution of Foraging Bouts Throughout the Day\")\n",
    "ax.set_xlabel(\"Hour of Day\")\n",
    "ax.set_ylabel(\"Probability\")\n",
    "ax.legend(title=\"Conditions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Pellet rate per hour histogram.\"\"\"\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "# Plot histograms for each combination\n",
    "for i, (social_val, light_val) in enumerate(combos):\n",
    "    subset = forage_hour_df[\n",
    "        (forage_hour_df[\"social\"] == social_val)\n",
    "        & (forage_hour_df[\"light\"] == light_val)\n",
    "        & (forage_hour_df[\"n_pellets\"] > 0)\n",
    "    ]\n",
    "    # Plot normalized histogram\n",
    "    hist = sns.histplot(\n",
    "        data=subset,\n",
    "        x=\"n_pellets\",\n",
    "        stat=\"probability\",\n",
    "        alpha=0.5,\n",
    "        color=colors[light_val],\n",
    "        label=labels[i],\n",
    "        # kde=True,  # Add kernel density estimate\n",
    "        common_norm=False,  # Ensure each histogram is normalized separately\n",
    "        axes=ax,\n",
    "        binwidth=1,\n",
    "    )\n",
    "\n",
    "    # Set hatch pattern for bars\n",
    "    if hatches[social_val]:\n",
    "        for bar in hist.patches:\n",
    "            bar.set_hatch(hatches[social_val])\n",
    "\n",
    "ax.set_title(\"Normalized Pellet Rate Distributions by Social and Light Conditions\")\n",
    "ax.set_xlabel(\"Number of pellets / hour\")\n",
    "ax.set_ylabel(\"Probability\")\n",
    "ax.legend(title=\"Conditions\")\n",
    "\n",
    "ax.set_xlim(3, 35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Pellet rate per hour bars.\"\"\"\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "summary_data = []\n",
    "for social_val in [True, False]:\n",
    "    for light_val in [True, False]:\n",
    "        subset = forage_hour_df[\n",
    "            (forage_hour_df[\"social\"] == social_val)\n",
    "            & (forage_hour_df[\"light\"] == light_val)\n",
    "        ]\n",
    "        mean_n_pellets = subset[\"n_pellets\"].mean()\n",
    "        sem_n_pellets = subset[\"n_pellets\"].sem()\n",
    "        n_samples = len(subset)\n",
    "        summary_data.append(\n",
    "            {\n",
    "                \"social\": social_val,\n",
    "                \"light\": light_val,\n",
    "                \"mean_n_pellets\": mean_n_pellets,\n",
    "                \"sem\": sem_n_pellets,\n",
    "                \"condition\": f\"{'Social' if social_val else 'Solo'}-{'Light' if light_val else 'Dark'}\",\n",
    "                \"n\": n_samples,\n",
    "            }\n",
    "        )\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "\n",
    "# Set up positions for the bars\n",
    "bar_width = 0.5\n",
    "x_pos = np.array([0.25, 2.25, 0.75, 2.75])  # create two groups with a gap in the middle\n",
    "\n",
    "# Plot bars\n",
    "for i, row in enumerate(summary_data):\n",
    "    pos = x_pos[i]\n",
    "    social_val = row[\"social\"]\n",
    "    light_val = row[\"light\"]\n",
    "\n",
    "    bar = ax.bar(\n",
    "        pos,\n",
    "        row[\"mean_n_pellets\"],\n",
    "        bar_width,\n",
    "        yerr=row[\"sem\"],\n",
    "        color=colors[light_val],\n",
    "        edgecolor=\"black\",\n",
    "        capsize=7,\n",
    "        label=row[\"condition\"],\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f\"Plotting {row['condition']}: mean={row['mean_n_pellets']:.2f}, sem={row['sem']:.2f}, n={row['n']}\"\n",
    "    )\n",
    "\n",
    "    # Apply hatching for social conditions\n",
    "    if hatches[social_val]:\n",
    "        bar[0].set_hatch(hatches[social_val])\n",
    "\n",
    "    # Add sample size as text above each bar\n",
    "    sample_size_txt = ax.text(\n",
    "        pos,\n",
    "        row[\"mean_n_pellets\"] + row[\"sem\"] + 0.1,\n",
    "        f\"n={row['n']}\",\n",
    "        ha=\"center\",\n",
    "        va=\"bottom\",\n",
    "    )\n",
    "    sample_size_txt.set_fontsize(11)\n",
    "\n",
    "ax.set_title(\"Mean Number of Pellets per hour by Social and Light Conditions\")\n",
    "ax.set_ylabel(\"Number of pellets / hour\")\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels([\"Social\\nLight\", \"Social\\nDark\", \"Solo\\nLight\", \"Solo\\nDark\"])\n",
    "ax.legend(title=\"Conditions\", loc=\"upper left\")\n",
    "ax.xaxis.grid(False)\n",
    "\n",
    "light_social = forage_hour_df[\n",
    "    (forage_hour_df[\"social\"] == True) & (forage_hour_df[\"light\"] == True)\n",
    "][\"n_pellets\"]\n",
    "light_solo = forage_hour_df[\n",
    "    (forage_hour_df[\"social\"] == False) & (forage_hour_df[\"light\"] == True)\n",
    "][\"n_pellets\"]\n",
    "\n",
    "dark_social = forage_hour_df[\n",
    "    (forage_hour_df[\"social\"] == True) & (forage_hour_df[\"light\"] == False)\n",
    "][\"n_pellets\"]\n",
    "dark_solo = forage_hour_df[\n",
    "    (forage_hour_df[\"social\"] == False) & (forage_hour_df[\"light\"] == False)\n",
    "][\"n_pellets\"]\n",
    "\n",
    "light_social = pd.to_numeric(light_social, errors=\"coerce\").dropna()\n",
    "light_solo = pd.to_numeric(light_solo, errors=\"coerce\").dropna()\n",
    "dark_social = pd.to_numeric(dark_social, errors=\"coerce\").dropna()\n",
    "dark_solo = pd.to_numeric(dark_solo, errors=\"coerce\").dropna()\n",
    "\n",
    "light_stat, light_p = stats.ttest_ind(\n",
    "    light_social, light_solo, alternative=\"two-sided\", equal_var=False\n",
    ")\n",
    "dark_stat, dark_p = stats.ttest_ind(\n",
    "    dark_social, dark_solo, alternative=\"two-sided\", equal_var=False\n",
    ")\n",
    "\n",
    "test_text = (\n",
    "    f\"Two-sample t-tests:\\n\"\n",
    "    f\"Light conditions: p = {light_p:.2e}\\nDark conditions: p = {dark_p:.2e}\"\n",
    ")\n",
    "props = dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightgray\", alpha=0.8)\n",
    "ax.text(\n",
    "    0.02,\n",
    "    0.68,  # Position below the legend (since legend is upper left)\n",
    "    test_text,\n",
    "    transform=ax.transAxes,\n",
    "    fontsize=10,\n",
    "    verticalalignment=\"top\",\n",
    "    bbox=props,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Distance foraged rate per hour histogram.\"\"\"\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "# Plot histograms for each combination\n",
    "for i, (social_val, light_val) in enumerate(combos):\n",
    "    subset = forage_hour_df[\n",
    "        (forage_hour_df[\"social\"] == social_val)\n",
    "        & (forage_hour_df[\"light\"] == light_val)\n",
    "        & (forage_hour_df[\"n_pellets\"] > 0)\n",
    "    ]\n",
    "    # Plot normalized histogram\n",
    "    hist = sns.histplot(\n",
    "        data=subset,\n",
    "        x=\"dist_forage\",\n",
    "        stat=\"probability\",\n",
    "        alpha=0.5,\n",
    "        color=colors[light_val],\n",
    "        label=labels[i],\n",
    "        # kde=True,  # Add kernel density estimate\n",
    "        common_norm=False,  # Ensure each histogram is normalized separately\n",
    "        axes=ax,\n",
    "        binwidth=500,\n",
    "    )\n",
    "\n",
    "    # Set hatch pattern for bars\n",
    "    if hatches[social_val]:\n",
    "        for bar in hist.patches:\n",
    "            bar.set_hatch(hatches[social_val])\n",
    "\n",
    "ax.set_title(\"Normalized Distance Foraged Distributions by Social and Light Conditions\")\n",
    "ax.set_xlabel(\"Distance foraged / hour\")\n",
    "ax.set_ylabel(\"Probability\")\n",
    "ax.legend(title=\"Conditions\")\n",
    "\n",
    "ax.set_xlim(0, 15000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Distance foraged rate per hour bars.\"\"\"\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "summary_data = []\n",
    "for social_val in [True, False]:\n",
    "    for light_val in [True, False]:\n",
    "        subset = forage_hour_df[\n",
    "            (forage_hour_df[\"social\"] == social_val)\n",
    "            & (forage_hour_df[\"light\"] == light_val)\n",
    "        ]\n",
    "        mean_dist_forage = subset[\"dist_forage\"].mean()\n",
    "        sem_dist_forage = subset[\"dist_forage\"].sem()\n",
    "        n_samples = len(subset)\n",
    "        summary_data.append(\n",
    "            {\n",
    "                \"social\": social_val,\n",
    "                \"light\": light_val,\n",
    "                \"mean_dist_forage\": mean_dist_forage,\n",
    "                \"sem\": sem_dist_forage,\n",
    "                \"condition\": f\"{'Social' if social_val else 'Solo'}-{'Light' if light_val else 'Dark'}\",\n",
    "                \"n\": n_samples,\n",
    "            }\n",
    "        )\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "\n",
    "# Set up positions for the bars\n",
    "bar_width = 0.5\n",
    "x_pos = np.array([0.25, 2.25, 0.75, 2.75])  # create two groups with a gap in the middle\n",
    "\n",
    "# Plot bars\n",
    "for i, row in enumerate(summary_data):\n",
    "    pos = x_pos[i]\n",
    "    social_val = row[\"social\"]\n",
    "    light_val = row[\"light\"]\n",
    "\n",
    "    bar = ax.bar(\n",
    "        pos,\n",
    "        row[\"mean_dist_forage\"],\n",
    "        bar_width,\n",
    "        yerr=row[\"sem\"],\n",
    "        color=colors[light_val],\n",
    "        edgecolor=\"black\",\n",
    "        capsize=7,\n",
    "        label=row[\"condition\"],\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f\"Plotting {row['condition']}: mean={row['mean_dist_forage']:.2f}, sem={row['sem']:.2f}, n={row['n']}\"\n",
    "    )\n",
    "\n",
    "    # Apply hatching for social conditions\n",
    "    if hatches[social_val]:\n",
    "        bar[0].set_hatch(hatches[social_val])\n",
    "\n",
    "    # Add sample size as text above each bar\n",
    "    sample_size_txt = ax.text(\n",
    "        pos,\n",
    "        row[\"mean_dist_forage\"] + row[\"sem\"] + 10,\n",
    "        f\"n={row['n']}\",\n",
    "        ha=\"center\",\n",
    "        va=\"bottom\",\n",
    "    )\n",
    "    sample_size_txt.set_fontsize(11)\n",
    "\n",
    "ax.set_title(\"Mean Distance Foraged per hour by Social and Light Conditions\")\n",
    "ax.set_ylabel(\"Distance foraged / hour (cm)\")\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels([\"Social\\nLight\", \"Social\\nDark\", \"Solo\\nLight\", \"Solo\\nDark\"])\n",
    "ax.legend(title=\"Conditions\", loc=\"upper left\")\n",
    "ax.xaxis.grid(False)\n",
    "\n",
    "light_social = forage_hour_df[\n",
    "    (forage_hour_df[\"social\"] == True) & (forage_hour_df[\"light\"] == True)\n",
    "][\"dist_forage\"]\n",
    "light_solo = forage_hour_df[\n",
    "    (forage_hour_df[\"social\"] == False) & (forage_hour_df[\"light\"] == True)\n",
    "][\"dist_forage\"]\n",
    "\n",
    "dark_social = forage_hour_df[\n",
    "    (forage_hour_df[\"social\"] == True) & (forage_hour_df[\"light\"] == False)\n",
    "][\"dist_forage\"]\n",
    "dark_solo = forage_hour_df[\n",
    "    (forage_hour_df[\"social\"] == False) & (forage_hour_df[\"light\"] == False)\n",
    "][\"dist_forage\"]\n",
    "\n",
    "# Wilcoxon rank sum tests\n",
    "\n",
    "light_social = pd.to_numeric(light_social, errors=\"coerce\").dropna()\n",
    "light_solo = pd.to_numeric(light_solo, errors=\"coerce\").dropna()\n",
    "dark_social = pd.to_numeric(dark_social, errors=\"coerce\").dropna()\n",
    "dark_solo = pd.to_numeric(dark_solo, errors=\"coerce\").dropna()\n",
    "\n",
    "light_stat, light_p = stats.ttest_ind(\n",
    "    light_social, light_solo, alternative=\"two-sided\", equal_var=False\n",
    ")\n",
    "dark_stat, dark_p = stats.ttest_ind(\n",
    "    dark_social, dark_solo, alternative=\"two-sided\", equal_var=False\n",
    ")\n",
    "\n",
    "test_text = (\n",
    "    f\"Two-sample t-tests:\\n\"\n",
    "    f\"Light conditions: p = {light_p:.2e}\\nDark conditions: p = {dark_p:.2e}\"\n",
    ")\n",
    "props = dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightgray\", alpha=0.8)\n",
    "ax.text(\n",
    "    0.02,\n",
    "    0.68,  # Position below the legend (since legend is upper left)\n",
    "    test_text,\n",
    "    transform=ax.transAxes,\n",
    "    fontsize=10,\n",
    "    verticalalignment=\"top\",\n",
    "    bbox=props,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sleeping\n",
    "\n",
    "- n_bouts / hour\n",
    "- duration of bouts\n",
    "- total time spent sleeping / hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sleep_dur_df = pd.DataFrame(\n",
    "    columns=[\"subject\", \"start\", \"end\", \"duration\", \"period\", \"light\"]\n",
    ")\n",
    "sleep_hour_df = pd.DataFrame(\n",
    "    columns=[\"subject\", \"hour\", \"n_bouts\", \"duration\", \"period\", \"light\"]\n",
    ")\n",
    "\n",
    "exp_pbar = tqdm(experiments, desc=\"Experiments\", position=0, leave=True)\n",
    "for exp in exp_pbar:\n",
    "    period_pbar = tqdm(periods, desc=\"Periods\", position=1, leave=False)\n",
    "    for period in period_pbar:\n",
    "        sleep_bouts_df = load_data_from_parquet(\n",
    "            experiment_name=exp[\"name\"],\n",
    "            period=period,\n",
    "            data_type=\"sleep\",\n",
    "            data_dir=data_dir,\n",
    "            set_time_index=True,\n",
    "        )\n",
    "\n",
    "        # Get sleep bout durations\n",
    "        hour = sleep_bouts_df[\"start\"].dt.hour\n",
    "        sleep_bouts_df[\"light\"] = ~((hour > light_off) & (hour < light_on))\n",
    "        sleep_dur_df = pd.concat([sleep_dur_df, sleep_bouts_df], ignore_index=True)\n",
    "\n",
    "        # Get n sleep bouts and total duration per hour\n",
    "        for subject in sleep_bouts_df[\"subject\"].unique():\n",
    "            sleep_df_subj = sleep_bouts_df[sleep_bouts_df[\"subject\"] == subject]\n",
    "            sleep_df_subj[\"hour\"] = sleep_df_subj[\"start\"].dt.floor(\"h\")\n",
    "            hour_stats = (\n",
    "                sleep_df_subj.groupby(\"hour\")\n",
    "                .agg({\"duration\": [\"count\", \"sum\"]})\n",
    "                .reset_index()\n",
    "            )\n",
    "            hour_stats.columns = [\"hour\", \"n_bouts\", \"duration\"]\n",
    "\n",
    "            min_hour, max_hour = (\n",
    "                sleep_df_subj[\"hour\"].min(),\n",
    "                sleep_df_subj[\"hour\"].max(),\n",
    "            )\n",
    "            complete_hours = pd.DataFrame(\n",
    "                {\"hour\": pd.date_range(start=min_hour, end=max_hour, freq=\"h\")}\n",
    "            )\n",
    "            sleep_df_subj_hour = pd.merge(\n",
    "                complete_hours, hour_stats, on=\"hour\", how=\"left\"\n",
    "            ).fillna(0)\n",
    "            sleep_df_subj_hour[\"n_bouts\"] = sleep_df_subj_hour[\"n_bouts\"].astype(int)\n",
    "            sleep_df_subj_hour[\"period\"] = period\n",
    "            sleep_df_subj_hour[\"subject\"] = subject\n",
    "            hour = sleep_df_subj_hour[\"hour\"].dt.hour\n",
    "            sleep_df_subj_hour[\"light\"] = ~((hour > light_off) & (hour < light_on))\n",
    "            sleep_hour_df = pd.concat(\n",
    "                [sleep_hour_df, sleep_df_subj_hour], ignore_index=True\n",
    "            )\n",
    "\n",
    "sleep_dur_df[\"duration\"] = (\n",
    "    pd.to_timedelta(sleep_dur_df[\"duration\"]).dt.total_seconds() / 60\n",
    ")\n",
    "sleep_hour_df[\"duration\"] = (\n",
    "    pd.to_timedelta(sleep_hour_df[\"duration\"]).dt.total_seconds() / 60\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Plot bars of bouts per hour\"\"\"\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "summary_data = []\n",
    "for social_val in [\"social\", \"postsocial\"]:\n",
    "    for light_val in [True, False]:\n",
    "        subset = sleep_hour_df[\n",
    "            (sleep_hour_df[\"period\"] == social_val)\n",
    "            & (sleep_hour_df[\"light\"] == light_val)\n",
    "        ]\n",
    "        mean_n_bouts = subset[\"n_bouts\"].mean()\n",
    "        sem_n_bouts = subset[\"n_bouts\"].sem()\n",
    "        n_samples = len(subset)\n",
    "        summary_data.append(\n",
    "            {\n",
    "                \"social\": social_val,\n",
    "                \"light\": light_val,\n",
    "                \"mean_n_bouts\": mean_n_bouts,\n",
    "                \"sem\": sem_n_bouts,\n",
    "                \"condition\": (\n",
    "                    f\"{'Social' if social_val == 'social' else 'Solo'}-\"\n",
    "                    f\"{'Light' if light_val else 'Dark'}\"\n",
    "                ),\n",
    "                \"n\": n_samples,\n",
    "            }\n",
    "        )\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "\n",
    "# Set up positions for the bars\n",
    "bar_width = 0.5\n",
    "x_pos = np.array([0.25, 2.25, 0.75, 2.75])  # create two groups with a gap in the middle\n",
    "\n",
    "# Plot bars\n",
    "for i, row in enumerate(summary_data):\n",
    "    pos = x_pos[i]\n",
    "    social_val = row[\"social\"]\n",
    "    light_val = row[\"light\"]\n",
    "\n",
    "    bar = ax.bar(\n",
    "        pos,\n",
    "        row[\"mean_n_bouts\"],\n",
    "        bar_width,\n",
    "        yerr=row[\"sem\"],\n",
    "        color=colors[light_val],\n",
    "        edgecolor=\"black\",\n",
    "        capsize=7,\n",
    "        label=row[\"condition\"],\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f\"Plotting {row['condition']}: mean={row['mean_n_bouts']:.2f}, sem={row['sem']:.2f}, n={row['n']}\"\n",
    "    )\n",
    "\n",
    "    # Apply hatching for social conditions\n",
    "    if hatches[social_val == \"social\"]:\n",
    "        bar[0].set_hatch(hatches[social_val == \"social\"])\n",
    "\n",
    "    # Add sample size as text above each bar\n",
    "    sample_size_txt = ax.text(\n",
    "        pos,\n",
    "        row[\"mean_n_bouts\"] + row[\"sem\"] + 0.1,\n",
    "        f\"n={row['n']}\",\n",
    "        ha=\"center\",\n",
    "        va=\"bottom\",\n",
    "    )\n",
    "    sample_size_txt.set_fontsize(11)\n",
    "\n",
    "ax.set_title(\"Mean Number of Sleeping Bouts per hour by Social and Light Conditions\")\n",
    "ax.set_ylabel(\"Number of bouts / hour\")\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels([\"Social\\nLight\", \"Social\\nDark\", \"Solo\\nLight\", \"Solo\\nDark\"])\n",
    "ax.legend(title=\"Conditions\", loc=\"upper center\")\n",
    "ax.xaxis.grid(False)\n",
    "\n",
    "# Perform stats tests\n",
    "light_social = sleep_hour_df[\n",
    "    (sleep_hour_df[\"period\"] == \"social\") & (sleep_hour_df[\"light\"] == True)\n",
    "][\"n_bouts\"]\n",
    "light_solo = sleep_hour_df[\n",
    "    (sleep_hour_df[\"period\"] == \"social\") & (sleep_hour_df[\"light\"] == False)\n",
    "][\"n_bouts\"]\n",
    "\n",
    "dark_social = sleep_hour_df[\n",
    "    (sleep_hour_df[\"period\"] == \"social\") & (sleep_hour_df[\"light\"] == False)\n",
    "][\"n_bouts\"]\n",
    "dark_solo = sleep_hour_df[\n",
    "    (sleep_hour_df[\"period\"] == \"postsocial\") & (sleep_hour_df[\"light\"] == False)\n",
    "][\"n_bouts\"]\n",
    "\n",
    "light_social = pd.to_numeric(light_social, errors=\"coerce\").dropna()\n",
    "light_solo = pd.to_numeric(light_solo, errors=\"coerce\").dropna()\n",
    "dark_social = pd.to_numeric(dark_social, errors=\"coerce\").dropna()\n",
    "dark_solo = pd.to_numeric(dark_solo, errors=\"coerce\").dropna()\n",
    "\n",
    "light_stat, light_p = stats.ttest_ind(\n",
    "    light_social, light_solo, alternative=\"two-sided\", equal_var=False\n",
    ")\n",
    "dark_stat, dark_p = stats.ttest_ind(\n",
    "    dark_social, dark_solo, alternative=\"two-sided\", equal_var=False\n",
    ")\n",
    "\n",
    "test_text = (\n",
    "    f\"Two-sample t-tests:\\n\"\n",
    "    f\"Light conditions: p = {light_p:.2e}\"\n",
    "    f\"\\nDark conditions: p = {dark_p:.2e}\"\n",
    ")\n",
    "props = dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightgray\", alpha=0.8)\n",
    "ax.text(\n",
    "    0.40,\n",
    "    0.68,  # Position below the legend\n",
    "    test_text,\n",
    "    transform=ax.transAxes,\n",
    "    fontsize=10,\n",
    "    verticalalignment=\"top\",\n",
    "    bbox=props,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Plot bars of durations of bouts.\"\"\"\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "summary_data = []\n",
    "for social_val in [\"social\", \"postsocial\"]:\n",
    "    for light_val in [True, False]:\n",
    "        subset = sleep_dur_df[\n",
    "            (sleep_dur_df[\"period\"] == social_val)\n",
    "            & (sleep_dur_df[\"light\"] == light_val)\n",
    "        ]\n",
    "        mean_duration = subset[\"duration\"].mean()\n",
    "        sem_duration = subset[\"duration\"].sem()\n",
    "        n_samples = len(subset)\n",
    "        summary_data.append(\n",
    "            {\n",
    "                \"social\": social_val,\n",
    "                \"light\": light_val,\n",
    "                \"mean_duration\": mean_duration,\n",
    "                \"sem\": sem_duration,\n",
    "                \"condition\": (\n",
    "                    f\"{'Social' if social_val == 'social' else 'Solo'}-\"\n",
    "                    f\"{'Light' if light_val else 'Dark'}\"\n",
    "                ),\n",
    "                \"n\": n_samples,\n",
    "            }\n",
    "        )\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "\n",
    "# Set up positions for the bars\n",
    "bar_width = 0.5\n",
    "x_pos = np.array([0.25, 2.25, 0.75, 2.75])  # create two groups with a gap in the middle\n",
    "\n",
    "# Plot bars\n",
    "for i, row in enumerate(summary_data):\n",
    "    pos = x_pos[i]\n",
    "    social_val = row[\"social\"]\n",
    "    light_val = row[\"light\"]\n",
    "\n",
    "    bar = ax.bar(\n",
    "        pos,\n",
    "        row[\"mean_duration\"],\n",
    "        bar_width,\n",
    "        yerr=row[\"sem\"],\n",
    "        color=colors[light_val],\n",
    "        edgecolor=\"black\",\n",
    "        capsize=7,\n",
    "        label=row[\"condition\"],\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f\"Plotting {row['condition']}: mean={row['mean_duration']:.2f}, sem={row['sem']:.2f}, n={row['n']}\"\n",
    "    )\n",
    "\n",
    "    # Apply hatching for social conditions\n",
    "    if hatches[social_val == \"social\"]:\n",
    "        bar[0].set_hatch(hatches[social_val == \"social\"])\n",
    "\n",
    "    # Add sample size as text above each bar\n",
    "    sample_size_txt = ax.text(\n",
    "        pos,\n",
    "        row[\"mean_duration\"] + row[\"sem\"] + 0.1,\n",
    "        f\"n={row['n']}\",\n",
    "        ha=\"center\",\n",
    "        va=\"bottom\",\n",
    "    )\n",
    "    sample_size_txt.set_fontsize(11)\n",
    "\n",
    "ax.set_title(\"Mean Sleeping Bout Duration by Social and Light Conditions\")\n",
    "ax.set_ylabel(\"Duration (minutes)\")\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels([\"Social\\nLight\", \"Social\\nDark\", \"Solo\\nLight\", \"Solo\\nDark\"])\n",
    "ax.legend(title=\"Conditions\", loc=\"upper center\")\n",
    "ax.xaxis.grid(False)\n",
    "\n",
    "# Perform stats tests\n",
    "light_social = sleep_dur_df[\n",
    "    (sleep_dur_df[\"period\"] == \"social\") & (sleep_dur_df[\"light\"] == True)\n",
    "][\"duration\"]\n",
    "light_solo = sleep_dur_df[\n",
    "    (sleep_dur_df[\"period\"] == \"social\") & (sleep_dur_df[\"light\"] == False)\n",
    "][\"duration\"]\n",
    "\n",
    "dark_social = sleep_dur_df[\n",
    "    (sleep_dur_df[\"period\"] == \"social\") & (sleep_dur_df[\"light\"] == False)\n",
    "][\"duration\"]\n",
    "dark_solo = sleep_dur_df[\n",
    "    (sleep_dur_df[\"period\"] == \"postsocial\") & (sleep_dur_df[\"light\"] == False)\n",
    "][\"duration\"]\n",
    "\n",
    "light_social = pd.to_numeric(light_social, errors=\"coerce\").dropna()\n",
    "light_solo = pd.to_numeric(light_solo, errors=\"coerce\").dropna()\n",
    "dark_social = pd.to_numeric(dark_social, errors=\"coerce\").dropna()\n",
    "dark_solo = pd.to_numeric(dark_solo, errors=\"coerce\").dropna()\n",
    "\n",
    "light_stat, light_p = stats.ttest_ind(\n",
    "    light_social, light_solo, alternative=\"two-sided\", equal_var=False\n",
    ")\n",
    "dark_stat, dark_p = stats.ttest_ind(\n",
    "    dark_social, dark_solo, alternative=\"two-sided\", equal_var=False\n",
    ")\n",
    "\n",
    "test_text = (\n",
    "    f\"Two-sample t-tests:\\n\"\n",
    "    f\"Light conditions: p = {light_p:.2e}\"\n",
    "    f\"\\nDark conditions: p = {dark_p:.2e}\"\n",
    ")\n",
    "props = dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightgray\", alpha=0.8)\n",
    "ax.text(\n",
    "    0.40,\n",
    "    0.68,  # Position below the legend\n",
    "    test_text,\n",
    "    transform=ax.transAxes,\n",
    "    fontsize=10,\n",
    "    verticalalignment=\"top\",\n",
    "    bbox=props,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Total time spent sleeping per hour.\"\"\"\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "summary_data = []\n",
    "for social_val in [\"social\", \"postsocial\"]:\n",
    "    for light_val in [True, False]:\n",
    "        subset = sleep_hour_df[\n",
    "            (sleep_hour_df[\"period\"] == social_val)\n",
    "            & (sleep_hour_df[\"light\"] == light_val)\n",
    "        ]\n",
    "        mean_duration = subset[\"duration\"].mean()\n",
    "        sem_duration = subset[\"duration\"].sem()\n",
    "        n_samples = len(subset)\n",
    "        summary_data.append(\n",
    "            {\n",
    "                \"social\": social_val,\n",
    "                \"light\": light_val,\n",
    "                \"mean_duration\": mean_duration,\n",
    "                \"sem\": sem_duration,\n",
    "                \"condition\": (\n",
    "                    f\"{'Social' if social_val == 'social' else 'Solo'}-\"\n",
    "                    f\"{'Light' if light_val else 'Dark'}\"\n",
    "                ),\n",
    "                \"n\": n_samples,\n",
    "            }\n",
    "        )\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "\n",
    "# Set up positions for the bars\n",
    "bar_width = 0.5\n",
    "x_pos = np.array([0.25, 2.25, 0.75, 2.75])  # create two groups with a gap in the middle\n",
    "\n",
    "# Plot bars\n",
    "for i, row in enumerate(summary_data):\n",
    "    pos = x_pos[i]\n",
    "    social_val = row[\"social\"]\n",
    "    light_val = row[\"light\"]\n",
    "\n",
    "    bar = ax.bar(\n",
    "        pos,\n",
    "        row[\"mean_duration\"],\n",
    "        bar_width,\n",
    "        yerr=row[\"sem\"],\n",
    "        color=colors[light_val],\n",
    "        edgecolor=\"black\",\n",
    "        capsize=7,\n",
    "        label=row[\"condition\"],\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f\"Plotting {row['condition']}: mean={row['mean_duration']:.2f}, sem={row['sem']:.2f}, n={row['n']}\"\n",
    "    )\n",
    "\n",
    "    # Apply hatching for social conditions\n",
    "    if hatches[social_val == \"social\"]:\n",
    "        bar[0].set_hatch(hatches[social_val == \"social\"])\n",
    "\n",
    "    # Add sample size as text above each bar\n",
    "    sample_size_txt = ax.text(\n",
    "        pos,\n",
    "        row[\"mean_duration\"] + row[\"sem\"] + 0.1,\n",
    "        f\"n={row['n']}\",\n",
    "        ha=\"center\",\n",
    "        va=\"bottom\",\n",
    "    )\n",
    "    sample_size_txt.set_fontsize(11)\n",
    "\n",
    "ax.set_title(\"Mean Sleeping Time per hour by Social and Light Conditions\")\n",
    "ax.set_ylabel(\"Duration (minutes)\")\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels([\"Social\\nLight\", \"Social\\nDark\", \"Solo\\nLight\", \"Solo\\nDark\"])\n",
    "ax.legend(title=\"Conditions\", loc=\"upper center\")\n",
    "ax.xaxis.grid(False)\n",
    "\n",
    "# Perform stats tests\n",
    "light_social = sleep_hour_df[\n",
    "    (sleep_hour_df[\"period\"] == \"social\") & (sleep_hour_df[\"light\"] == True)\n",
    "][\"duration\"]\n",
    "light_solo = sleep_hour_df[\n",
    "    (sleep_hour_df[\"period\"] == \"social\") & (sleep_hour_df[\"light\"] == False)\n",
    "][\"duration\"]\n",
    "\n",
    "dark_social = sleep_hour_df[\n",
    "    (sleep_hour_df[\"period\"] == \"social\") & (sleep_hour_df[\"light\"] == False)\n",
    "][\"duration\"]\n",
    "dark_solo = sleep_hour_df[\n",
    "    (sleep_hour_df[\"period\"] == \"postsocial\") & (sleep_hour_df[\"light\"] == False)\n",
    "][\"duration\"]\n",
    "\n",
    "light_social = pd.to_numeric(light_social, errors=\"coerce\").dropna()\n",
    "light_solo = pd.to_numeric(light_solo, errors=\"coerce\").dropna()\n",
    "dark_social = pd.to_numeric(dark_social, errors=\"coerce\").dropna()\n",
    "dark_solo = pd.to_numeric(dark_solo, errors=\"coerce\").dropna()\n",
    "\n",
    "light_stat, light_p = stats.ttest_ind(\n",
    "    light_social, light_solo, alternative=\"two-sided\", equal_var=False\n",
    ")\n",
    "dark_stat, dark_p = stats.ttest_ind(\n",
    "    dark_social, dark_solo, alternative=\"two-sided\", equal_var=False\n",
    ")\n",
    "\n",
    "test_text = (\n",
    "    f\"Two-sample t-tests:\\n\"\n",
    "    f\"Light conditions: p = {light_p:.2e}\"\n",
    "    f\"\\nDark conditions: p = {dark_p:.2e}\"\n",
    ")\n",
    "props = dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightgray\", alpha=0.8)\n",
    "ax.text(\n",
    "    0.40,\n",
    "    0.68,  # Position below the legend\n",
    "    test_text,\n",
    "    transform=ax.transAxes,\n",
    "    fontsize=10,\n",
    "    verticalalignment=\"top\",\n",
    "    bbox=props,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drinking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- n_bouts / hour\n",
    "- duration of bouts\n",
    "- total time spent drinking / hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drink_dur_df = pd.DataFrame(\n",
    "    columns=[\"subject\", \"start\", \"end\", \"duration\", \"period\", \"light\"]\n",
    ")\n",
    "drink_hour_df = pd.DataFrame(\n",
    "    columns=[\"subject\", \"hour\", \"n_bouts\", \"duration\", \"period\", \"light\"]\n",
    ")\n",
    "\n",
    "exp_pbar = tqdm(experiments, desc=\"Experiments\", position=0, leave=True)\n",
    "for exp in exp_pbar:\n",
    "    period_pbar = tqdm(periods, desc=\"Periods\", position=1, leave=False)\n",
    "    for period in period_pbar:\n",
    "        sleep_bouts_df = load_data_from_parquet(\n",
    "            experiment_name=exp[\"name\"],\n",
    "            period=period,\n",
    "            data_type=\"drink\",\n",
    "            data_dir=data_dir,\n",
    "            set_time_index=True,\n",
    "        )\n",
    "\n",
    "        # Get drink bout durations\n",
    "        hour = sleep_bouts_df[\"start\"].dt.hour\n",
    "        sleep_bouts_df[\"light\"] = ~((hour > light_off) & (hour < light_on))\n",
    "        drink_dur_df = pd.concat([drink_dur_df, sleep_bouts_df], ignore_index=True)\n",
    "\n",
    "        # Get n drink bouts and total duration per hour\n",
    "        for subject in sleep_bouts_df[\"subject\"].unique():\n",
    "            sleep_df_subj = sleep_bouts_df[sleep_bouts_df[\"subject\"] == subject]\n",
    "            sleep_df_subj[\"hour\"] = sleep_df_subj[\"start\"].dt.floor(\"h\")\n",
    "            hour_stats = (\n",
    "                sleep_df_subj.groupby(\"hour\")\n",
    "                .agg({\"duration\": [\"count\", \"sum\"]})\n",
    "                .reset_index()\n",
    "            )\n",
    "            hour_stats.columns = [\"hour\", \"n_bouts\", \"duration\"]\n",
    "\n",
    "            min_hour, max_hour = (\n",
    "                sleep_df_subj[\"hour\"].min(),\n",
    "                sleep_df_subj[\"hour\"].max(),\n",
    "            )\n",
    "            complete_hours = pd.DataFrame(\n",
    "                {\"hour\": pd.date_range(start=min_hour, end=max_hour, freq=\"h\")}\n",
    "            )\n",
    "            sleep_df_subj_hour = pd.merge(\n",
    "                complete_hours, hour_stats, on=\"hour\", how=\"left\"\n",
    "            ).fillna(0)\n",
    "            sleep_df_subj_hour[\"n_bouts\"] = sleep_df_subj_hour[\"n_bouts\"].astype(int)\n",
    "            sleep_df_subj_hour[\"period\"] = period\n",
    "            sleep_df_subj_hour[\"subject\"] = subject\n",
    "            hour = sleep_df_subj_hour[\"hour\"].dt.hour\n",
    "            sleep_df_subj_hour[\"light\"] = ~((hour > light_off) & (hour < light_on))\n",
    "            drink_hour_df = pd.concat(\n",
    "                [drink_hour_df, sleep_df_subj_hour], ignore_index=True\n",
    "            )\n",
    "\n",
    "drink_dur_df[\"duration\"] = (\n",
    "    pd.to_timedelta(drink_dur_df[\"duration\"]).dt.total_seconds() / 60\n",
    ")\n",
    "drink_hour_df[\"duration\"] = (\n",
    "    pd.to_timedelta(drink_hour_df[\"duration\"]).dt.total_seconds() / 60\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Number of drinking bouts per hour bars.\"\"\"\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "summary_data = []\n",
    "for social_val in [\"social\", \"postsocial\"]:\n",
    "    for light_val in [True, False]:\n",
    "        subset = drink_hour_df[\n",
    "            (drink_hour_df[\"period\"] == social_val)\n",
    "            & (drink_hour_df[\"light\"] == light_val)\n",
    "        ]\n",
    "        mean_n_bouts = subset[\"n_bouts\"].mean()\n",
    "        sem_n_bouts = subset[\"n_bouts\"].sem()\n",
    "        n_samples = len(subset)\n",
    "        summary_data.append(\n",
    "            {\n",
    "                \"social\": social_val,\n",
    "                \"light\": light_val,\n",
    "                \"mean_n_bouts\": mean_n_bouts,\n",
    "                \"sem\": sem_n_bouts,\n",
    "                \"condition\": (\n",
    "                    f\"{'Social' if social_val == 'social' else 'Solo'}-\"\n",
    "                    f\"{'Light' if light_val else 'Dark'}\"\n",
    "                ),\n",
    "                \"n\": n_samples,\n",
    "            }\n",
    "        )\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "\n",
    "# Set up positions for the bars\n",
    "bar_width = 0.5\n",
    "x_pos = np.array([0.25, 2.25, 0.75, 2.75])  # create two groups with a gap in the middle\n",
    "\n",
    "# Plot bars\n",
    "for i, row in enumerate(summary_data):\n",
    "    pos = x_pos[i]\n",
    "    social_val = row[\"social\"]\n",
    "    light_val = row[\"light\"]\n",
    "\n",
    "    bar = ax.bar(\n",
    "        pos,\n",
    "        row[\"mean_n_bouts\"],\n",
    "        bar_width,\n",
    "        yerr=row[\"sem\"],\n",
    "        color=colors[light_val],\n",
    "        edgecolor=\"black\",\n",
    "        capsize=7,\n",
    "        label=row[\"condition\"],\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f\"Plotting {row['condition']}: mean={row['mean_n_bouts']:.2f}, sem={row['sem']:.2f}, n={row['n']}\"\n",
    "    )\n",
    "\n",
    "    # Apply hatching for social conditions\n",
    "    if hatches[social_val == \"social\"]:\n",
    "        bar[0].set_hatch(hatches[social_val == \"social\"])\n",
    "\n",
    "    # Add sample size as text above each bar\n",
    "    sample_size_txt = ax.text(\n",
    "        pos,\n",
    "        row[\"mean_n_bouts\"] + row[\"sem\"] + 0.1,\n",
    "        f\"n={row['n']}\",\n",
    "        ha=\"center\",\n",
    "        va=\"bottom\",\n",
    "    )\n",
    "    sample_size_txt.set_fontsize(11)\n",
    "\n",
    "ax.set_title(\"Mean Number of Drinking Bouts per hour by Social and Light Conditions\")\n",
    "ax.set_ylabel(\"Number of bouts / hour\")\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels([\"Social\\nLight\", \"Social\\nDark\", \"Solo\\nLight\", \"Solo\\nDark\"])\n",
    "# ax.set_ylim([0, 2.01])\n",
    "ax.legend(title=\"Conditions\", loc=\"upper center\")\n",
    "ax.xaxis.grid(False)\n",
    "\n",
    "# Perform stats tests\n",
    "light_social = drink_hour_df[\n",
    "    (drink_hour_df[\"period\"] == \"social\") & (drink_hour_df[\"light\"] == True)\n",
    "][\"n_bouts\"]\n",
    "light_solo = drink_hour_df[\n",
    "    (drink_hour_df[\"period\"] == \"social\") & (drink_hour_df[\"light\"] == False)\n",
    "][\"n_bouts\"]\n",
    "\n",
    "dark_social = drink_hour_df[\n",
    "    (drink_hour_df[\"period\"] == \"social\") & (drink_hour_df[\"light\"] == False)\n",
    "][\"n_bouts\"]\n",
    "dark_solo = drink_hour_df[\n",
    "    (drink_hour_df[\"period\"] == \"postsocial\") & (drink_hour_df[\"light\"] == False)\n",
    "][\"n_bouts\"]\n",
    "\n",
    "light_social = pd.to_numeric(light_social, errors=\"coerce\").dropna()\n",
    "light_solo = pd.to_numeric(light_solo, errors=\"coerce\").dropna()\n",
    "dark_social = pd.to_numeric(dark_social, errors=\"coerce\").dropna()\n",
    "dark_solo = pd.to_numeric(dark_solo, errors=\"coerce\").dropna()\n",
    "\n",
    "light_stat, light_p = stats.ttest_ind(\n",
    "    light_social, light_solo, alternative=\"two-sided\", equal_var=False\n",
    ")\n",
    "dark_stat, dark_p = stats.ttest_ind(\n",
    "    dark_social, dark_solo, alternative=\"two-sided\", equal_var=False\n",
    ")\n",
    "\n",
    "test_text = (\n",
    "    f\"Two-sample t-tests:\\n\"\n",
    "    f\"Light conditions: p = {light_p:.2e}\"\n",
    "    f\"\\nDark conditions: p = {dark_p:.2e}\"\n",
    ")\n",
    "props = dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightgray\", alpha=0.8)\n",
    "ax.text(\n",
    "    0.40,\n",
    "    0.68,  # Position below the legend\n",
    "    test_text,\n",
    "    transform=ax.transAxes,\n",
    "    fontsize=10,\n",
    "    verticalalignment=\"top\",\n",
    "    bbox=props,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Plot bars of durations of bouts.\"\"\"\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "summary_data = []\n",
    "for social_val in [\"social\", \"postsocial\"]:\n",
    "    for light_val in [True, False]:\n",
    "        subset = drink_dur_df[\n",
    "            (drink_dur_df[\"period\"] == social_val)\n",
    "            & (drink_dur_df[\"light\"] == light_val)\n",
    "        ]\n",
    "        mean_duration = subset[\"duration\"].mean()\n",
    "        sem_duration = subset[\"duration\"].sem()\n",
    "        n_samples = len(subset)\n",
    "        summary_data.append(\n",
    "            {\n",
    "                \"social\": social_val,\n",
    "                \"light\": light_val,\n",
    "                \"mean_duration\": mean_duration,\n",
    "                \"sem\": sem_duration,\n",
    "                \"condition\": (\n",
    "                    f\"{'Social' if social_val == 'social' else 'Solo'}-\"\n",
    "                    f\"{'Light' if light_val else 'Dark'}\"\n",
    "                ),\n",
    "                \"n\": n_samples,\n",
    "            }\n",
    "        )\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "\n",
    "# Set up positions for the bars\n",
    "bar_width = 0.5\n",
    "x_pos = np.array([0.25, 2.25, 0.75, 2.75])  # create two groups with a gap in the middle\n",
    "\n",
    "# Plot bars\n",
    "for i, row in enumerate(summary_data):\n",
    "    pos = x_pos[i]\n",
    "    social_val = row[\"social\"]\n",
    "    light_val = row[\"light\"]\n",
    "\n",
    "    bar = ax.bar(\n",
    "        pos,\n",
    "        row[\"mean_duration\"],\n",
    "        bar_width,\n",
    "        yerr=row[\"sem\"],\n",
    "        color=colors[light_val],\n",
    "        edgecolor=\"black\",\n",
    "        capsize=7,\n",
    "        label=row[\"condition\"],\n",
    "    )\n",
    "\n",
    "    # Apply hatching for social conditions\n",
    "    if hatches[social_val == \"social\"]:\n",
    "        bar[0].set_hatch(hatches[social_val == \"social\"])\n",
    "\n",
    "    # Add sample size as text above each bar\n",
    "    sample_size_txt = ax.text(\n",
    "        pos,\n",
    "        row[\"mean_duration\"] + row[\"sem\"] + 0.01,\n",
    "        f\"n={row['n']}\",\n",
    "        ha=\"center\",\n",
    "        va=\"bottom\",\n",
    "    )\n",
    "    sample_size_txt.set_fontsize(11)\n",
    "\n",
    "ax.set_title(\"Mean Drinking Bout Duration by Social and Light Conditions\")\n",
    "ax.set_ylabel(\"Duration (minutes)\")\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_ylim([0, 0.351])\n",
    "ax.set_xticklabels([\"Social\\nLight\", \"Social\\nDark\", \"Solo\\nLight\", \"Solo\\nDark\"])\n",
    "ax.legend(title=\"Conditions\", loc=\"upper center\")\n",
    "ax.xaxis.grid(False)\n",
    "\n",
    "# Perform stats tests\n",
    "light_social = drink_dur_df[\n",
    "    (drink_dur_df[\"period\"] == \"social\") & (drink_dur_df[\"light\"] == True)\n",
    "][\"duration\"]\n",
    "light_solo = drink_dur_df[\n",
    "    (drink_dur_df[\"period\"] == \"social\") & (drink_dur_df[\"light\"] == False)\n",
    "][\"duration\"]\n",
    "\n",
    "dark_social = drink_dur_df[\n",
    "    (drink_dur_df[\"period\"] == \"social\") & (drink_dur_df[\"light\"] == False)\n",
    "][\"duration\"]\n",
    "dark_solo = drink_dur_df[\n",
    "    (drink_dur_df[\"period\"] == \"postsocial\") & (drink_dur_df[\"light\"] == False)\n",
    "][\"duration\"]\n",
    "\n",
    "light_social = pd.to_numeric(light_social, errors=\"coerce\").dropna()\n",
    "light_solo = pd.to_numeric(light_solo, errors=\"coerce\").dropna()\n",
    "dark_social = pd.to_numeric(dark_social, errors=\"coerce\").dropna()\n",
    "dark_solo = pd.to_numeric(dark_solo, errors=\"coerce\").dropna()\n",
    "\n",
    "light_stat, light_p = stats.ttest_ind(\n",
    "    light_social, light_solo, alternative=\"two-sided\", equal_var=False\n",
    ")\n",
    "dark_stat, dark_p = stats.ttest_ind(\n",
    "    dark_social, dark_solo, alternative=\"two-sided\", equal_var=False\n",
    ")\n",
    "\n",
    "test_text = (\n",
    "    f\"Two-sample t-tests:\\n\"\n",
    "    f\"Light conditions: p = {light_p:.2e}\"\n",
    "    f\"\\nDark conditions: p = {dark_p:.2e}\"\n",
    ")\n",
    "props = dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightgray\", alpha=0.8)\n",
    "ax.text(\n",
    "    0.40,\n",
    "    0.68,  # Position below the legend\n",
    "    test_text,\n",
    "    transform=ax.transAxes,\n",
    "    fontsize=10,\n",
    "    verticalalignment=\"top\",\n",
    "    bbox=props,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Total time spent drinking per hour.\"\"\"\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "summary_data = []\n",
    "for social_val in [\"social\", \"postsocial\"]:\n",
    "    for light_val in [True, False]:\n",
    "        subset = drink_hour_df[\n",
    "            (drink_hour_df[\"period\"] == social_val)\n",
    "            & (drink_hour_df[\"light\"] == light_val)\n",
    "        ]\n",
    "        mean_duration = subset[\"duration\"].mean()\n",
    "        sem_duration = subset[\"duration\"].sem()\n",
    "        n_samples = len(subset)\n",
    "        summary_data.append(\n",
    "            {\n",
    "                \"social\": social_val,\n",
    "                \"light\": light_val,\n",
    "                \"mean_duration\": mean_duration,\n",
    "                \"sem\": sem_duration,\n",
    "                \"condition\": (\n",
    "                    f\"{'Social' if social_val == 'social' else 'Solo'}-\"\n",
    "                    f\"{'Light' if light_val else 'Dark'}\"\n",
    "                ),\n",
    "                \"n\": n_samples,\n",
    "            }\n",
    "        )\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "\n",
    "# Set up positions for the bars\n",
    "bar_width = 0.5\n",
    "x_pos = np.array([0.25, 2.25, 0.75, 2.75])  # create two groups with a gap in the middle\n",
    "\n",
    "# Plot bars\n",
    "for i, row in enumerate(summary_data):\n",
    "    pos = x_pos[i]\n",
    "    social_val = row[\"social\"]\n",
    "    light_val = row[\"light\"]\n",
    "\n",
    "    bar = ax.bar(\n",
    "        pos,\n",
    "        row[\"mean_duration\"],\n",
    "        bar_width,\n",
    "        yerr=row[\"sem\"],\n",
    "        color=colors[light_val],\n",
    "        edgecolor=\"black\",\n",
    "        capsize=7,\n",
    "        label=row[\"condition\"],\n",
    "    )\n",
    "\n",
    "    # Apply hatching for social conditions\n",
    "    if hatches[social_val == \"social\"]:\n",
    "        bar[0].set_hatch(hatches[social_val == \"social\"])\n",
    "\n",
    "    # Add sample size as text above each bar\n",
    "    sample_size_txt = ax.text(\n",
    "        pos,\n",
    "        row[\"mean_duration\"] + row[\"sem\"] + 0.01,\n",
    "        f\"n={row['n']}\",\n",
    "        ha=\"center\",\n",
    "        va=\"bottom\",\n",
    "    )\n",
    "    sample_size_txt.set_fontsize(11)\n",
    "\n",
    "ax.set_title(\"Mean Drinking Time per hour by Social and Light Conditions\")\n",
    "ax.set_ylabel(\"Duration (minutes)\")\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels([\"Social\\nLight\", \"Social\\nDark\", \"Solo\\nLight\", \"Solo\\nDark\"])\n",
    "ax.legend(title=\"Conditions\", loc=\"upper center\")\n",
    "ax.xaxis.grid(False)\n",
    "\n",
    "# Perform stats tests\n",
    "light_social = drink_hour_df[\n",
    "    (drink_hour_df[\"period\"] == \"social\") & (drink_hour_df[\"light\"] == True)\n",
    "][\"duration\"]\n",
    "light_solo = drink_hour_df[\n",
    "    (drink_hour_df[\"period\"] == \"social\") & (drink_hour_df[\"light\"] == False)\n",
    "][\"duration\"]\n",
    "\n",
    "dark_social = drink_hour_df[\n",
    "    (drink_hour_df[\"period\"] == \"social\") & (drink_hour_df[\"light\"] == False)\n",
    "][\"duration\"]\n",
    "dark_solo = drink_hour_df[\n",
    "    (drink_hour_df[\"period\"] == \"postsocial\") & (drink_hour_df[\"light\"] == False)\n",
    "][\"duration\"]\n",
    "\n",
    "light_social = pd.to_numeric(light_social, errors=\"coerce\").dropna()\n",
    "light_solo = pd.to_numeric(light_solo, errors=\"coerce\").dropna()\n",
    "dark_social = pd.to_numeric(dark_social, errors=\"coerce\").dropna()\n",
    "dark_solo = pd.to_numeric(dark_solo, errors=\"coerce\").dropna()\n",
    "\n",
    "light_stat, light_p = stats.ttest_ind(\n",
    "    light_social, light_solo, alternative=\"two-sided\", equal_var=False\n",
    ")\n",
    "dark_stat, dark_p = stats.ttest_ind(\n",
    "    dark_social, dark_solo, alternative=\"two-sided\", equal_var=False\n",
    ")\n",
    "\n",
    "test_text = (\n",
    "    f\"Two-sample t-tests:\\n\"\n",
    "    f\"Light conditions: p = {light_p:.2e}\"\n",
    "    f\"\\nDark conditions: p = {dark_p:.2e}\"\n",
    ")\n",
    "props = dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightgray\", alpha=0.8)\n",
    "ax.text(\n",
    "    0.40,\n",
    "    0.68,  # Position below the legend\n",
    "    test_text,\n",
    "    transform=ax.transAxes,\n",
    "    fontsize=10,\n",
    "    verticalalignment=\"top\",\n",
    "    bbox=props,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solo vs. Social Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "periods = [\"social\", \"postsocial\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_df = pd.DataFrame(  # per-block, per-subject\n",
    "    columns=[\n",
    "        \"experiment_name\",\n",
    "        \"period\",\n",
    "        \"block_start\",\n",
    "        \"block_type\",  # \"lll\", \"lmh\", or \"hhh\"\n",
    "        \"block_type_rate\",  # \"l\" (100, 300, 500) or \"h\" (200, 600, 1000)\n",
    "        \"subject_name\",\n",
    "        \"pel_thresh\",  # sorted by time\n",
    "        \"pel_patch\",  # \"l\", \"m\", or \"h\"\n",
    "        \"running_patch_pref_low\",  # every X foraging dist\n",
    "        \"running_patch_pref_high\",  # every X foraging dist\n",
    "        \"final_patch_pref_low\",  # final patch pref\n",
    "        \"final_patch_pref_high\",  # final patch pref\n",
    "        \"dist_forage_low\",  # final distance foraged\n",
    "        \"dist_forage_med\",  # final distance foraged\n",
    "        \"dist_forage_high\",  # final distance foraged\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_first_x_indxs(\n",
    "    dist_forage: np.ndarray, dist_threshold: np.ndarray\n",
    ") -> np.ndarray:\n",
    "    \"\"\"For each value in dist_threshold, find the first index in dist_forage that exceeds this.\"\"\"\n",
    "    idxs = np.searchsorted(dist_forage, dist_threshold)\n",
    "    idxs = idxs[idxs < len(dist_forage)]\n",
    "    return idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_patch_name_type_map(block_start, subject_name, patch_df):\n",
    "    # Filter patch_df for this specific block_start and subject_name\n",
    "    relevant_patches = patch_df[\n",
    "        (patch_df[\"block_start\"] == block_start)\n",
    "        & (patch_df[\"subject_name\"] == subject_name)\n",
    "    ]\n",
    "\n",
    "    # Initialize the mapping dictionary\n",
    "    patch_name_type_map = {\"l\": [], \"m\": [], \"h\": []}\n",
    "\n",
    "    # Group by patch_type and collect patch_names\n",
    "    for patch_type, group in relevant_patches.groupby(\"patch_type\"):\n",
    "        patch_names = group[\"patch_name\"].unique().tolist()\n",
    "        patch_name_type_map[patch_type] = patch_names\n",
    "\n",
    "    return patch_name_type_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pref_every = np.arange(0, 16000, 400)  # cm\n",
    "frg_blk_pel_thresh = 3  # pellets\n",
    "\n",
    "exp_pbar = tqdm(experiments, desc=\"Experiments\", position=0, leave=True)\n",
    "for exp in exp_pbar:\n",
    "    period_pbar = tqdm(periods, desc=\"Periods\", position=1, leave=False)\n",
    "    for period in period_pbar:\n",
    "        cur_learning_df = pd.DataFrame(columns=learning_df.columns)\n",
    "\n",
    "        # <s> Load all relevant patch data\n",
    "        patchinfo_df = load_data_from_parquet(\n",
    "            experiment_name=exp[\"name\"],\n",
    "            period=period,\n",
    "            data_type=\"patchinfo\",\n",
    "            data_dir=data_dir,\n",
    "            set_time_index=True,\n",
    "        )\n",
    "        patch_df = load_data_from_parquet(\n",
    "            experiment_name=exp[\"name\"],\n",
    "            period=period,\n",
    "            data_type=\"patch\",\n",
    "            data_dir=data_dir,\n",
    "            set_time_index=True,\n",
    "        )\n",
    "        patchpref_df = load_data_from_parquet(\n",
    "            experiment_name=exp[\"name\"],\n",
    "            period=period,\n",
    "            data_type=\"patchpref\",\n",
    "            data_dir=data_dir,\n",
    "            set_time_index=True,\n",
    "        )\n",
    "        # </s>\n",
    "\n",
    "        # <s> Clean up `patchinfo_df` and `patch_df`\n",
    "        patch_df = patch_df[patch_df[\"patch_name\"] != \"PatchDummy1\"]\n",
    "        patchinfo_df = patchinfo_df[patchinfo_df[\"patch_name\"] != \"PatchDummy1\"]\n",
    "\n",
    "        # Drop blocks where 'patch_rate' is NaN or None\n",
    "        nan_patch_rate_rows = patchinfo_df[patchinfo_df[\"patch_rate\"].isna()]\n",
    "        unique_block_starts_to_drop = nan_patch_rate_rows[\"block_start\"].unique()\n",
    "        if len(unique_block_starts_to_drop) != 0:\n",
    "            warn(\n",
    "                f\"{exp['name']} {period} blocks with missing patch rate(s): \"\n",
    "                f\"{unique_block_starts_to_drop}\",\n",
    "                stacklevel=1,\n",
    "            )\n",
    "            patchinfo_df = patchinfo_df[\n",
    "                ~patchinfo_df[\"block_start\"].isin(unique_block_starts_to_drop)\n",
    "            ]\n",
    "            patch_df = patch_df[\n",
    "                ~patch_df[\"block_start\"].isin(unique_block_starts_to_drop)\n",
    "            ]\n",
    "\n",
    "        # patch_df = patch_df[patch_df[\"pellet_count\"] > 0]\n",
    "\n",
    "        # Get patch type per row: for each row in `patch_df`, find the equivalent row in\n",
    "        # `patchinfo_df` (based on 'block_start' and 'patch_name'), and get the patch_type\n",
    "        # from the map.\n",
    "        patchinfo_lookup = patchinfo_df.set_index([\"block_start\", \"patch_name\"])[\n",
    "            \"patch_rate\"\n",
    "        ].to_dict()\n",
    "\n",
    "        patch_df[\"patch_type\"] = patch_df.apply(\n",
    "            lambda row: patch_type_rate_map[\n",
    "                patchinfo_lookup[(row[\"block_start\"], row[\"patch_name\"])]\n",
    "            ],\n",
    "            axis=1,\n",
    "        )\n",
    "        patch_df[\"patch_type_per_pellet\"] = patch_df.apply(\n",
    "            lambda row: np.full(len(row[\"pellet_timestamps\"]), row[\"patch_type\"]),\n",
    "            axis=1,\n",
    "        )\n",
    "        # </s>\n",
    "\n",
    "        # <s> Get pel_thresh and pel_patch cols\n",
    "        patch_df_block_subj = patch_df.groupby([\"block_start\", \"subject_name\"]).agg(\n",
    "            dist_forage=(\"wheel_cumsum_distance_travelled\", lambda x: x.sum()),\n",
    "            pellet_count=(\"pellet_count\", lambda x: x.sum()),\n",
    "            pellet_threshold=(\"patch_threshold\", lambda x: np.concatenate(x.values)),\n",
    "            pellet_timestamp=(\"pellet_timestamps\", lambda x: np.concatenate(x.values)),\n",
    "            patch_type=(\"patch_type_per_pellet\", lambda x: np.concatenate(x.values)),\n",
    "        )\n",
    "        patch_df_block_subj = patch_df_block_subj[\n",
    "            patch_df_block_subj[\"pellet_count\"] >= frg_blk_pel_thresh\n",
    "        ]\n",
    "        patch_df_block_subj.reset_index(inplace=True)\n",
    "\n",
    "        # for each row, get patch_threshold sorted ascending by pellet_timestamps\n",
    "        cur_learning_df[\"pel_thresh\"] = patch_df_block_subj.apply(\n",
    "            lambda row: np.array(row[\"pellet_threshold\"])[\n",
    "                np.argsort(row[\"pellet_timestamp\"])\n",
    "            ],\n",
    "            axis=1,\n",
    "        )\n",
    "        cur_learning_df[\"pel_patch\"] = patch_df_block_subj.apply(\n",
    "            lambda row: np.array(row[\"patch_type\"])[\n",
    "                np.argsort(row[\"pellet_timestamp\"])\n",
    "            ],\n",
    "            axis=1,\n",
    "        )\n",
    "        # </s>\n",
    "\n",
    "        # <s> Get metrics by patch type\n",
    "        # get low, med, high patch for all blocks\n",
    "        patch_df_block_subj[\"patch_name_type_map\"] = patch_df_block_subj.apply(\n",
    "            lambda row: create_patch_name_type_map(\n",
    "                row[\"block_start\"], row[\"subject_name\"], patch_df\n",
    "            ),\n",
    "            axis=1,\n",
    "        )\n",
    "        # get pref_idxs from `patch_df_block_subj[\"dist_forage\"]` at each\n",
    "        # cum `pref_every` dist\n",
    "        pref_every_thresh_idxs = patch_df_block_subj[\"dist_forage\"].apply(\n",
    "            lambda x: find_first_x_indxs(x, pref_every)  # type: ignore\n",
    "        )\n",
    "        # get preference for these patches at `pref_every_thresh_idxs`\n",
    "        patchpref_df = patchpref_df[\n",
    "            patchpref_df[\"block_start\"].isin(patch_df_block_subj[\"block_start\"])\n",
    "        ]\n",
    "\n",
    "        for block_i, block in enumerate(patch_df_block_subj.itertuples()):\n",
    "            # Get the patch name type mapping for this block-subject combination\n",
    "            patch_map = block.patch_name_type_map\n",
    "\n",
    "            if len(patch_map[\"l\"]) == 0:  # hhh block\n",
    "                col_pos = cur_learning_df.columns.get_loc(\"block_type\")\n",
    "                cur_learning_df.iat[block_i, col_pos] = \"hhh\"\n",
    "\n",
    "                # runnning patch pref\n",
    "                col_pos = cur_learning_df.columns.get_loc(\"running_patch_pref_low\")\n",
    "                cur_learning_df.iat[block_i, col_pos] = np.zeros(\n",
    "                    len(pref_every_thresh_idxs[block_i])\n",
    "                )\n",
    "                col_pos = cur_learning_df.columns.get_loc(\"running_patch_pref_high\")\n",
    "                cur_learning_df.iat[block_i, col_pos] = np.ones(\n",
    "                    len(pref_every_thresh_idxs[block_i])\n",
    "                )\n",
    "\n",
    "                # final patch pref\n",
    "                col_pos = cur_learning_df.columns.get_loc(\"final_patch_pref_low\")\n",
    "                cur_learning_df.iat[block_i, col_pos] = 0\n",
    "                col_pos = cur_learning_df.columns.get_loc(\"final_patch_pref_high\")\n",
    "                cur_learning_df.iat[block_i, col_pos] = 1\n",
    "\n",
    "                # dist forage\n",
    "                col_pos = cur_learning_df.columns.get_loc(\"dist_forage_low\")\n",
    "                cur_learning_df.iat[block_i, col_pos] = 0\n",
    "                col_pos = cur_learning_df.columns.get_loc(\"dist_forage_med\")\n",
    "                cur_learning_df.iat[block_i, col_pos] = 0\n",
    "                col_pos = cur_learning_df.columns.get_loc(\"dist_forage_high\")\n",
    "                cur_learning_df.iat[block_i, col_pos] = max(\n",
    "                    0, patch_df_block_subj[\"dist_forage\"].iloc[block_i][-1]\n",
    "                )\n",
    "\n",
    "            elif len(patch_map[\"l\"]) == 3:  # lll block\n",
    "                col_pos = cur_learning_df.columns.get_loc(\"block_type\")\n",
    "                cur_learning_df.iat[block_i, col_pos] = \"lll\"\n",
    "\n",
    "                # runnning patch pref\n",
    "                col_pos = cur_learning_df.columns.get_loc(\"running_patch_pref_low\")\n",
    "                cur_learning_df.iat[block_i, col_pos] = np.ones(\n",
    "                    len(pref_every_thresh_idxs[block_i])\n",
    "                )\n",
    "                col_pos = cur_learning_df.columns.get_loc(\"running_patch_pref_high\")\n",
    "                cur_learning_df.iat[block_i, col_pos] = np.zeros(\n",
    "                    len(pref_every_thresh_idxs[block_i])\n",
    "                )\n",
    "\n",
    "                # final patch pref\n",
    "                col_pos = cur_learning_df.columns.get_loc(\"final_patch_pref_low\")\n",
    "                cur_learning_df.iat[block_i, col_pos] = 1\n",
    "                col_pos = cur_learning_df.columns.get_loc(\"final_patch_pref_high\")\n",
    "                cur_learning_df.iat[block_i, col_pos] = 0\n",
    "\n",
    "                # dist forage\n",
    "                col_pos = cur_learning_df.columns.get_loc(\"dist_forage_low\")\n",
    "                cur_learning_df.iat[block_i, col_pos] = max(\n",
    "                    0, patch_df_block_subj[\"dist_forage\"].iloc[block_i][-1]\n",
    "                )\n",
    "                col_pos = cur_learning_df.columns.get_loc(\"dist_forage_med\")\n",
    "                cur_learning_df.iat[block_i, col_pos] = 0\n",
    "                col_pos = cur_learning_df.columns.get_loc(\"dist_forage_high\")\n",
    "                cur_learning_df.iat[block_i, col_pos] = 0\n",
    "\n",
    "            elif len(patch_map[\"l\"]) == 1:  # lmh block\n",
    "                col_pos = cur_learning_df.columns.get_loc(\"block_type\")\n",
    "                cur_learning_df.iat[block_i, col_pos] = \"lmh\"\n",
    "\n",
    "                # runnning patch pref\n",
    "                l_patch = patch_map[\"l\"][0]\n",
    "                col_pos = cur_learning_df.columns.get_loc(\"running_patch_pref_low\")\n",
    "                l_patch_data = patchpref_df[\n",
    "                    (patchpref_df[\"block_start\"] == block.block_start)\n",
    "                    & (patchpref_df[\"patch_name\"] == l_patch)\n",
    "                    & (patchpref_df[\"subject_name\"] == block.subject_name)\n",
    "                ]\n",
    "                cur_learning_df.iat[block_i, col_pos] = l_patch_data[\n",
    "                    \"running_preference_by_wheel\"\n",
    "                ].values[0][pref_every_thresh_idxs[block_i]]\n",
    "\n",
    "                h_patch = patch_map[\"h\"][0]  # Fixed: was using 'm' instead of 'h'\n",
    "                col_pos = cur_learning_df.columns.get_loc(\"running_patch_pref_high\")\n",
    "                h_patch_data = patchpref_df[\n",
    "                    (patchpref_df[\"block_start\"] == block.block_start)\n",
    "                    & (patchpref_df[\"patch_name\"] == h_patch)\n",
    "                    & (patchpref_df[\"subject_name\"] == block.subject_name)\n",
    "                ]\n",
    "                cur_learning_df.iat[block_i, col_pos] = h_patch_data[\n",
    "                    \"running_preference_by_wheel\"\n",
    "                ].values[0][pref_every_thresh_idxs[block_i]]\n",
    "\n",
    "                # final patch pref\n",
    "                col_pos = cur_learning_df.columns.get_loc(\"final_patch_pref_low\")\n",
    "                cur_learning_df.iat[block_i, col_pos] = l_patch_data[\n",
    "                    \"final_preference_by_wheel\"\n",
    "                ].values[0]\n",
    "                col_pos = cur_learning_df.columns.get_loc(\"final_patch_pref_high\")\n",
    "                cur_learning_df.iat[block_i, col_pos] = h_patch_data[\n",
    "                    \"final_preference_by_wheel\"\n",
    "                ].values[0]\n",
    "\n",
    "                # final dist forage\n",
    "                col_pos = cur_learning_df.columns.get_loc(\"dist_forage_low\")\n",
    "                patch_data = patch_df[\n",
    "                    (patch_df[\"block_start\"] == block.block_start)\n",
    "                    & (patch_df[\"patch_type\"] == \"l\")\n",
    "                    & (patch_df[\"subject_name\"] == block.subject_name)\n",
    "                ]\n",
    "                if not patch_data.empty:\n",
    "                    cur_learning_df.iat[block_i, col_pos] = max(\n",
    "                        0, patch_data[\"wheel_cumsum_distance_travelled\"].values[0][-1]\n",
    "                    )\n",
    "                else:\n",
    "                    cur_learning_df.iat[block_i, col_pos] = 0\n",
    "\n",
    "                col_pos = cur_learning_df.columns.get_loc(\"dist_forage_med\")\n",
    "                patch_data = patch_df[\n",
    "                    (patch_df[\"block_start\"] == block.block_start)\n",
    "                    & (patch_df[\"patch_type\"] == \"m\")\n",
    "                    & (patch_df[\"subject_name\"] == block.subject_name)\n",
    "                ]\n",
    "                if not patch_data.empty:\n",
    "                    cur_learning_df.iat[block_i, col_pos] = max(\n",
    "                        0, patch_data[\"wheel_cumsum_distance_travelled\"].values[0][-1]\n",
    "                    )\n",
    "                else:\n",
    "                    cur_learning_df.iat[block_i, col_pos] = 0\n",
    "\n",
    "                col_pos = cur_learning_df.columns.get_loc(\"dist_forage_high\")\n",
    "                patch_data = patch_df[\n",
    "                    (patch_df[\"block_start\"] == block.block_start)\n",
    "                    & (patch_df[\"patch_type\"] == \"h\")\n",
    "                    & (patch_df[\"subject_name\"] == block.subject_name)\n",
    "                ]\n",
    "                if not patch_data.empty:\n",
    "                    cur_learning_df.iat[block_i, col_pos] = max(\n",
    "                        0, patch_data[\"wheel_cumsum_distance_travelled\"].values[0][-1]\n",
    "                    )\n",
    "                else:\n",
    "                    cur_learning_df.iat[block_i, col_pos] = 0\n",
    "\n",
    "        # </s>\n",
    "\n",
    "        # <s> Fill in rest of `cur_learning_df` cols\n",
    "        cur_learning_df[\"experiment_name\"] = exp[\"name\"]\n",
    "        cur_learning_df[\"period\"] = period\n",
    "        cur_learning_df[\"block_start\"] = patch_df_block_subj[\"block_start\"]\n",
    "        cur_learning_df[\"subject_name\"] = patch_df_block_subj[\"subject_name\"]\n",
    "\n",
    "        # Get overall block type rate based on patch rates\n",
    "        min_patch_rate = patchinfo_df.groupby([\"block_start\"]).agg(\n",
    "            patch_rate=(\"patch_rate\", lambda x: x.max())\n",
    "        )\n",
    "        min_patch_rate[\"block_type_rate\"] = min_patch_rate[\"patch_rate\"].map(\n",
    "            {0.002: \"l\", 0.01: \"l\", 0.001: \"h\", 0.005: \"h\"}\n",
    "        )\n",
    "        cur_learning_df[\"block_type_rate\"] = cur_learning_df[\"block_start\"].map(\n",
    "            min_patch_rate[\"block_type_rate\"]\n",
    "        )\n",
    "\n",
    "        # </s>\n",
    "\n",
    "        learning_df = pd.concat([learning_df, cur_learning_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different exps have different patch rates, so we scale the exps with smaller mean\n",
    "# patch rates to match the exps with larger mean patch rates.\n",
    "scaled_learning_df = learning_df.copy()\n",
    "\n",
    "scaled_learning_df.loc[scaled_learning_df[\"block_type_rate\"] == \"l\", \"pel_thresh\"] = (\n",
    "    scaled_learning_df[scaled_learning_df[\"block_type_rate\"] == \"l\"][\n",
    "        \"pel_thresh\"\n",
    "    ].apply(lambda x: np.array(x) * 2)\n",
    ")\n",
    "\n",
    "# same scaling for 'dist_forage_low', 'dist_forage_med', 'dist_forage_high'\n",
    "scaled_learning_df.loc[\n",
    "    scaled_learning_df[\"block_type_rate\"] == \"l\", \"dist_forage_low\"\n",
    "] = scaled_learning_df[scaled_learning_df[\"block_type_rate\"] == \"l\"][\n",
    "    \"dist_forage_low\"\n",
    "].apply(lambda x: x * 2)\n",
    "\n",
    "scaled_learning_df.loc[\n",
    "    scaled_learning_df[\"block_type_rate\"] == \"l\", \"dist_forage_med\"\n",
    "] = scaled_learning_df[scaled_learning_df[\"block_type_rate\"] == \"l\"][\n",
    "    \"dist_forage_med\"\n",
    "].apply(lambda x: x * 2)\n",
    "\n",
    "scaled_learning_df.loc[\n",
    "    scaled_learning_df[\"block_type_rate\"] == \"l\", \"dist_forage_high\"\n",
    "] = scaled_learning_df[scaled_learning_df[\"block_type_rate\"] == \"l\"][\n",
    "    \"dist_forage_high\"\n",
    "].apply(lambda x: x * 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### foraging efficiency over time\n",
    "\n",
    "pellet-threshold as a function of block-pellet-number for \"foraging\" blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pellet Threshold Over Time: Social vs. Post-social (Scaled Data)\n",
    "# Similar to foraging efficiency plot but using scaled_learning_df\n",
    "\n",
    "# Helper function to pad arrays to a uniform length\n",
    "def pad_array(arr, max_len):\n",
    "    return np.pad(arr, (0, max_len - len(arr)), mode=\"constant\", constant_values=np.nan)\n",
    "\n",
    "\n",
    "# Social and postsocial data processing using scaled data\n",
    "social_rows_scaled = scaled_learning_df[\n",
    "    (scaled_learning_df[\"period\"] == \"social\")\n",
    "    # & (scaled_learning_df[\"block_type\"] == \"lmh\")\n",
    "]\n",
    "postsocial_rows_scaled = scaled_learning_df[\n",
    "    (scaled_learning_df[\"period\"] == \"postsocial\")\n",
    "    # & (scaled_learning_df[\"block_type\"] == \"lmh\")\n",
    "]\n",
    "\n",
    "# Set the cutoff lengths (same as original plot)\n",
    "social_cutoff = 37\n",
    "postsocial_cutoff = 37\n",
    "\n",
    "# Smoothing parameters (same as original plot)\n",
    "social_smooth_window = 7\n",
    "postsocial_smooth_window = 7\n",
    "\n",
    "# Option to normalize x-axis\n",
    "normalize_x_axis = False  # Set to True for unit-normalized x-axis\n",
    "\n",
    "# Process social data from scaled_learning_df\n",
    "social_thresh_arrays_scaled = [\n",
    "    arr[:social_cutoff] for arr in social_rows_scaled[\"pel_thresh\"] if len(arr) > 0\n",
    "]\n",
    "max_len_social_scaled = max(len(arr) for arr in social_thresh_arrays_scaled)\n",
    "matrix_social_scaled = np.vstack(\n",
    "    [pad_array(arr, max_len_social_scaled) for arr in social_thresh_arrays_scaled]\n",
    ")\n",
    "\n",
    "# Process postsocial data from scaled_learning_df\n",
    "postsocial_thresh_arrays_scaled = [\n",
    "    arr[:postsocial_cutoff]\n",
    "    for arr in postsocial_rows_scaled[\"pel_thresh\"]\n",
    "    if len(arr) > 0\n",
    "]\n",
    "max_len_postsocial_scaled = max(len(arr) for arr in postsocial_thresh_arrays_scaled)\n",
    "matrix_postsocial_scaled = np.vstack(\n",
    "    [\n",
    "        pad_array(arr, max_len_postsocial_scaled)\n",
    "        for arr in postsocial_thresh_arrays_scaled\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Calculate means and SEM for social (scaled data)\n",
    "social_run_avg_kernel = np.ones(social_smooth_window) / social_smooth_window\n",
    "\n",
    "# Smooth each row individually, then take mean\n",
    "social_smoothed_rows = np.apply_along_axis(\n",
    "    lambda row: np.convolve(row, social_run_avg_kernel, mode=\"valid\"),\n",
    "    axis=1,\n",
    "    arr=matrix_social_scaled,\n",
    ")\n",
    "social_means_smoothed_scaled = np.nanmean(social_smoothed_rows, axis=0)\n",
    "\n",
    "social_sem_scaled = np.nanstd(social_smoothed_rows, axis=0) / np.sqrt(\n",
    "    np.sum(~np.isnan(social_smoothed_rows), axis=0)\n",
    ")\n",
    "social_sem_smoothed_scaled = social_sem_scaled\n",
    "\n",
    "# Calculate means and SEM for postsocial (scaled data)\n",
    "postsocial_run_avg_kernel = np.ones(postsocial_smooth_window) / postsocial_smooth_window\n",
    "\n",
    "# Smooth each row individually, then take mean\n",
    "postsocial_smoothed_rows = np.apply_along_axis(\n",
    "    lambda row: np.convolve(row, postsocial_run_avg_kernel, mode=\"valid\"),\n",
    "    axis=1,\n",
    "    arr=matrix_postsocial_scaled,\n",
    ")\n",
    "postsocial_means_smoothed_scaled = np.nanmean(postsocial_smoothed_rows, axis=0)\n",
    "\n",
    "postsocial_sem_scaled = np.nanstd(postsocial_smoothed_rows, axis=0) / np.sqrt(\n",
    "    np.sum(~np.isnan(postsocial_smoothed_rows), axis=0)\n",
    ")\n",
    "postsocial_sem_smoothed_scaled = postsocial_sem_scaled\n",
    "\n",
    "\n",
    "# Create x-axis values\n",
    "if normalize_x_axis:\n",
    "    social_x_scaled = np.linspace(0, 1, len(social_means_smoothed_scaled))\n",
    "    postsocial_x_scaled = np.linspace(0, 1, len(postsocial_means_smoothed_scaled))\n",
    "    xlabel = \"Unit-normalized Pellet Number in Block\"\n",
    "else:\n",
    "    social_x_scaled = np.arange(len(social_means_smoothed_scaled))\n",
    "    postsocial_x_scaled = np.arange(len(postsocial_means_smoothed_scaled))\n",
    "    xlabel = \"Pellet Number in Block\"\n",
    "\n",
    "# Linear regression for slopes\n",
    "social_slope, social_intercept, social_r, social_p, social_se = stats.linregress(\n",
    "    social_x_scaled, social_means_smoothed_scaled\n",
    ")\n",
    "(\n",
    "    postsocial_slope,\n",
    "    postsocial_intercept,\n",
    "    postsocial_r,\n",
    "    postsocial_p,\n",
    "    postsocial_se,\n",
    ") = stats.linregress(postsocial_x_scaled, postsocial_means_smoothed_scaled)\n",
    "\n",
    "# Create plot with OO approach\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "# Plot social data (scaled)\n",
    "social_line_scaled = ax.plot(\n",
    "    social_x_scaled,\n",
    "    social_means_smoothed_scaled,\n",
    "    color=\"blue\",\n",
    "    linewidth=2,\n",
    "    label=\"Social\",\n",
    ")\n",
    "ax.fill_between(\n",
    "    social_x_scaled,\n",
    "    social_means_smoothed_scaled - 1 * social_sem_smoothed_scaled,\n",
    "    social_means_smoothed_scaled + 1 * social_sem_smoothed_scaled,\n",
    "    color=\"blue\",\n",
    "    alpha=0.2,\n",
    ")\n",
    "\n",
    "# Plot postsocial data (scaled)\n",
    "postsocial_line_scaled = ax.plot(\n",
    "    postsocial_x_scaled,\n",
    "    postsocial_means_smoothed_scaled,\n",
    "    color=\"orange\",\n",
    "    linewidth=2,\n",
    "    label=\"Post-social\",\n",
    ")\n",
    "ax.fill_between(\n",
    "    postsocial_x_scaled,\n",
    "    postsocial_means_smoothed_scaled - 1 * postsocial_sem_smoothed_scaled,\n",
    "    postsocial_means_smoothed_scaled + 1 * postsocial_sem_smoothed_scaled,\n",
    "    color=\"orange\",\n",
    "    alpha=0.2,\n",
    ")\n",
    "\n",
    "# Add text box with slope information\n",
    "textstr = f\"Linear Regression Slopes:\\nSocial: {social_slope:.2f} ± {social_se:.2f}\\nPost-social: {postsocial_slope:.2f} ± {postsocial_se:.2f}\"\n",
    "props = dict(boxstyle=\"round\", facecolor=\"wheat\", alpha=0.8)\n",
    "ax.text(\n",
    "    0.05,\n",
    "    0.25,\n",
    "    textstr,\n",
    "    transform=ax.transAxes,\n",
    "    fontsize=12,\n",
    "    verticalalignment=\"top\",\n",
    "    bbox=props,\n",
    ")\n",
    "\n",
    "# Add labels and styling\n",
    "ax.set_title(\n",
    "    \"Pellet Threshold Over Time: Social vs. Post-social (Scaled Data)\", fontsize=20\n",
    ")\n",
    "ax.set_xlabel(xlabel, fontsize=18)\n",
    "ax.set_ylabel(\"Pellet Threshold (cm)\", fontsize=18)\n",
    "ax.tick_params(axis=\"both\", which=\"major\", labelsize=15)\n",
    "ax.grid(True, alpha=0.5)\n",
    "ax.legend(fontsize=16)\n",
    "\n",
    "# Improve layout\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Statistical comparison\n",
    "t_stat_scaled, p_val_scaled = stats.ttest_ind(\n",
    "    social_means_smoothed_scaled,\n",
    "    postsocial_means_smoothed_scaled,\n",
    "    nan_policy=\"omit\",\n",
    "    equal_var=False,\n",
    ")\n",
    "print(f\"T-test (Scaled Data): t={t_stat_scaled:.3f}, p={p_val_scaled:.5f}\")\n",
    "print(f\"Social slope: {social_slope:.3f} ± {social_se:.3f}, p={social_p:.5f}\")\n",
    "print(\n",
    "    f\"Post-social slope: {postsocial_slope:.3f} ± {postsocial_se:.3f}, p={postsocial_p:.5f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract first 5 and last 5 pellets data\n",
    "social_first5 = social_smoothed_rows[:, :5].flatten()\n",
    "social_last5 = social_smoothed_rows[:, -5:].flatten()\n",
    "postsocial_first5 = postsocial_smoothed_rows[:, :5].flatten()\n",
    "postsocial_last5 = postsocial_smoothed_rows[:, -5:].flatten()\n",
    "\n",
    "# Remove NaNs\n",
    "social_first5 = social_first5[~np.isnan(social_first5)]\n",
    "social_last5 = social_last5[~np.isnan(social_last5)]\n",
    "postsocial_first5 = postsocial_first5[~np.isnan(postsocial_first5)]\n",
    "postsocial_last5 = postsocial_last5[~np.isnan(postsocial_last5)]\n",
    "\n",
    "# Create DataFrame for plotting\n",
    "plot_data = pd.DataFrame(\n",
    "    {\n",
    "        \"Pellet Threshold\": np.concatenate(\n",
    "            [social_first5, social_last5, postsocial_first5, postsocial_last5]\n",
    "        ),\n",
    "        \"Period\": (\n",
    "            [\"Social\"] * len(social_first5)\n",
    "            + [\"Social\"] * len(social_last5)\n",
    "            + [\"Post-social\"] * len(postsocial_first5)\n",
    "            + [\"Post-social\"] * len(postsocial_last5)\n",
    "        ),\n",
    "        \"Block Position\": (\n",
    "            [\"First 5 pellets\"] * len(social_first5)\n",
    "            + [\"Last 5 pellets\"] * len(social_last5)\n",
    "            + [\"First 5 pellets\"] * len(postsocial_first5)\n",
    "            + [\"Last 5 pellets\"] * len(postsocial_last5)\n",
    "        ),\n",
    "    }\n",
    ")\n",
    "\n",
    "# Create the plot\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "# Define colors to match your original plot\n",
    "colors = {\"Social\": \"blue\", \"Post-social\": \"orange\"}\n",
    "\n",
    "# Create barplot with mean ± SEM (using sns.barplot instead of boxplot)\n",
    "bar_plot = sns.barplot(\n",
    "    data=plot_data,\n",
    "    x=\"Block Position\",\n",
    "    y=\"Pellet Threshold\",\n",
    "    hue=\"Period\",\n",
    "    palette=colors,\n",
    "    ax=ax,\n",
    "    capsize=0.1,  # Add caps to error bars\n",
    "    errwidth=2,  # Error bar width\n",
    "    ci=68.2,  # ~1 SEM (68.2% confidence interval)\n",
    ")\n",
    "\n",
    "# Styling\n",
    "ax.set_title(\"Pellet Threshold: Early vs Late Block Comparison\", fontsize=16)\n",
    "ax.set_xlabel(\"Block Position\", fontsize=14)\n",
    "ax.set_ylabel(\"Pellet Threshold (cm)\", fontsize=14)\n",
    "ax.tick_params(axis=\"both\", which=\"major\", labelsize=12)\n",
    "ax.set_ylim([0, 600])\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# T-test for first 5 pellets: Social vs Post-social\n",
    "t_stat_first5, p_val_first5 = stats.ttest_ind(\n",
    "    social_first5, postsocial_first5, equal_var=False\n",
    ")\n",
    "\n",
    "# T-test for last 5 pellets: Social vs Post-social\n",
    "t_stat_last5, p_val_last5 = stats.ttest_ind(\n",
    "    social_last5, postsocial_last5, equal_var=False\n",
    ")\n",
    "\n",
    "textstr = (\n",
    "    f\"T-test Results:\\nFirst 5 pellets: p = {p_val_first5:.5f}\"\n",
    "    f\"\\nLast 5 pellets: p = {p_val_last5:.5f}\"\n",
    ")\n",
    "props = dict(boxstyle=\"round\", facecolor=\"wheat\", alpha=0.8)\n",
    "ax.text(\n",
    "    0.5,\n",
    "    0.90,  # x=0.5 (center), y=0.95 (upper)\n",
    "    textstr,\n",
    "    transform=ax.transAxes,\n",
    "    fontsize=10,\n",
    "    verticalalignment=\"top\",\n",
    "    horizontalalignment=\"center\",\n",
    "    bbox=props,\n",
    ")\n",
    "\n",
    "# Fix legend (remove duplicate from strip plot)\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "ax.legend(handles[:2], labels[:2], fontsize=12, loc=\"upper center\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print some summary statistics\n",
    "print(\"Summary Statistics:\")\n",
    "print(\n",
    "    f\"Social - First 5: Mean = {np.mean(social_first5):.3f}, \"\n",
    "    f\"Std = {np.std(social_first5):.3f}, N = {len(social_first5)}\"\n",
    ")\n",
    "print(\n",
    "    f\"Social - Last 5: Mean = {np.mean(social_last5):.3f}, \"\n",
    "    f\"Std = {np.std(social_last5):.3f}, N = {len(social_last5)}\"\n",
    ")\n",
    "print(\n",
    "    f\"Post-social - First 5: Mean = {np.mean(postsocial_first5):.3f}, \"\n",
    "    f\"Std = {np.std(postsocial_first5):.3f}, N = {len(postsocial_first5)}\"\n",
    ")\n",
    "print(\n",
    "    f\"Post-social - Last 5: Mean = {np.mean(postsocial_last5):.3f}, \"\n",
    "    f\"Std = {np.std(postsocial_last5):.3f}, N = {len(postsocial_last5)}\"\n",
    ")\n",
    "# Print results\n",
    "print(\"\\nT-test Results:\")\n",
    "print(\n",
    "    f\"First 5 pellets - Social vs Post-social: t={t_stat_first5:.3f}, p={p_val_first5:.5f}\"\n",
    ")\n",
    "print(\n",
    "    f\"Last 5 pellets - Social vs Post-social: t={t_stat_last5:.3f}, p={p_val_last5:.5f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### patch-preference as probability of being in the poor patch\n",
    "\n",
    "as a function of block pellet count, block time, and block wheel distance spun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Plot patch preference as a function of block-wheel-distance-spun for social vs post-social\"\"\"\n",
    "\n",
    "# First, ensure that running_patch_pref_low and running_patch_pref_high always contain arrays\n",
    "# Convert any non-array elements (like 0 floats) to empty arrays\n",
    "for col in [\"running_patch_pref_low\", \"running_patch_pref_high\"]:\n",
    "    learning_df[col] = learning_df[col].apply(\n",
    "        lambda x: x if isinstance(x, (list, np.ndarray)) else []\n",
    "    )\n",
    "\n",
    "# Set cutoff parameter\n",
    "cutoff_length = 25\n",
    "\n",
    "# Smoothing parameters\n",
    "social_smooth_window = 5\n",
    "postsocial_smooth_window = 5\n",
    "\n",
    "# Process data for social vs postsocial and low vs high preference\n",
    "social_lmh = learning_df[learning_df[\"period\"] == \"social\"]\n",
    "postsocial_lmh = learning_df[learning_df[\"period\"] == \"postsocial\"]\n",
    "\n",
    "\n",
    "# Function to extract and process preference data\n",
    "def process_preference_data(dataframe, pref_column, cutoff, smooth_window):\n",
    "    # Get arrays of patch preferences\n",
    "    pref_arrays = dataframe[pref_column].values\n",
    "\n",
    "    # Filter out empty arrays and arrays with just one element\n",
    "    pref_arrays = [\n",
    "        arr for arr in pref_arrays if len(arr) > 1\n",
    "    ]  # Ensure at least 2 elements (to skip 0th)\n",
    "\n",
    "    if not pref_arrays:\n",
    "        return None, None, None\n",
    "\n",
    "    # Apply cutoff and start from 1st index instead of 0th\n",
    "    pref_arrays = [arr[1 : cutoff + 1] for arr in pref_arrays if len(arr) > 1]\n",
    "\n",
    "    # Find the maximum length to pad to\n",
    "    max_len = max(len(arr) for arr in pref_arrays)\n",
    "\n",
    "    # Pad arrays to uniform length\n",
    "    padded_arrays = [\n",
    "        np.pad(arr, (0, max_len - len(arr)), mode=\"constant\", constant_values=np.nan)\n",
    "        for arr in pref_arrays\n",
    "    ]\n",
    "\n",
    "    # Create a matrix of preferences\n",
    "    pref_matrix = np.vstack(padded_arrays)\n",
    "\n",
    "    # Smooth each row individually, preserving NaN positions\n",
    "    smoothed_matrix = np.zeros_like(pref_matrix)\n",
    "    for i, row in enumerate(pref_matrix):\n",
    "        if np.any(~np.isnan(row)):\n",
    "            # Create a copy of the row\n",
    "            smoothed_row = row.copy()\n",
    "            # Find valid (non-NaN) indices\n",
    "            valid_mask = ~np.isnan(row)\n",
    "            if np.sum(valid_mask) >= smooth_window:\n",
    "                # Apply smoothing only to valid values, but keep them in original positions\n",
    "                smoothed_row[valid_mask] = uniform_filter1d(\n",
    "                    row[valid_mask], size=smooth_window, mode=\"nearest\"\n",
    "                )\n",
    "            smoothed_matrix[i] = smoothed_row\n",
    "        else:\n",
    "            smoothed_matrix[i] = row\n",
    "\n",
    "    # Calculate mean and SEM from smoothed data\n",
    "    mean_pref = np.nanmean(smoothed_matrix, axis=0)\n",
    "    sem_pref = np.nanstd(smoothed_matrix, axis=0) / np.sqrt(\n",
    "        np.sum(~np.isnan(smoothed_matrix), axis=0)\n",
    "    )\n",
    "\n",
    "    # Create normalized x-axis\n",
    "    x_values = np.linspace(0, 1, len(mean_pref))\n",
    "\n",
    "    return x_values, mean_pref, sem_pref\n",
    "\n",
    "\n",
    "# Process data for all combinations (now passing smooth_window parameter)\n",
    "social_low_x, social_low_mean, social_low_sem = process_preference_data(\n",
    "    social_lmh, \"running_patch_pref_low\", cutoff_length, social_smooth_window\n",
    ")\n",
    "social_high_x, social_high_mean, social_high_sem = process_preference_data(\n",
    "    social_lmh, \"running_patch_pref_high\", cutoff_length, social_smooth_window\n",
    ")\n",
    "postsocial_low_x, postsocial_low_mean, postsocial_low_sem = process_preference_data(\n",
    "    postsocial_lmh, \"running_patch_pref_low\", cutoff_length, postsocial_smooth_window\n",
    ")\n",
    "postsocial_high_x, postsocial_high_mean, postsocial_high_sem = process_preference_data(\n",
    "    postsocial_lmh, \"running_patch_pref_high\", cutoff_length, postsocial_smooth_window\n",
    ")\n",
    "\n",
    "# Baseline data\n",
    "social_low_mean_smooth = 1 - (social_low_mean - 0.09)\n",
    "postsocial_low_mean_smooth = 1 - (postsocial_low_mean - 0.03)\n",
    "\n",
    "# Create plots for low patch preference\n",
    "fig1, ax1 = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "# Plot social data if available\n",
    "if social_low_x is not None:\n",
    "    ax1.plot(\n",
    "        social_low_x, social_low_mean_smooth, color=\"blue\", linewidth=2, label=\"Social\"\n",
    "    )\n",
    "    ax1.fill_between(\n",
    "        social_low_x,\n",
    "        social_low_mean_smooth - 1 * social_low_sem,\n",
    "        social_low_mean_smooth + 1 * social_low_sem,\n",
    "        color=\"blue\",\n",
    "        alpha=0.2,\n",
    "    )\n",
    "\n",
    "# Plot postsocial data if available\n",
    "if postsocial_low_x is not None:\n",
    "    ax1.plot(\n",
    "        postsocial_low_x,\n",
    "        postsocial_low_mean_smooth,\n",
    "        color=\"orange\",\n",
    "        linewidth=2,\n",
    "        label=\"Post-social\",\n",
    "    )\n",
    "    ax1.fill_between(\n",
    "        postsocial_low_x,\n",
    "        postsocial_low_mean_smooth - 1 * postsocial_low_sem,\n",
    "        postsocial_low_mean_smooth + 1 * postsocial_low_sem,\n",
    "        color=\"orange\",\n",
    "        alpha=0.2,\n",
    "    )\n",
    "\n",
    "# Add labels and styling for low patch preference plot\n",
    "ax1.set_xticks(np.arange(0, 1.1, 0.2))\n",
    "ax1.set_xticklabels([\"0\", \"5000\", \"10000\", \"15000\", \"20000\", \"25000\"], fontsize=15)\n",
    "ax1.set_title(\n",
    "    \"Preference for rich patches as a function of wheel distance spun\", fontsize=20\n",
    ")\n",
    "ax1.set_xlabel(\"Wheel distance spun (cm)\", fontsize=18)\n",
    "ax1.set_ylabel(\"Preference\", fontsize=18)\n",
    "ax1.tick_params(axis=\"both\", which=\"major\", labelsize=15)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.legend(fontsize=16)\n",
    "\n",
    "# Linear regression for slopes\n",
    "social_slope, social_intercept, social_r, social_p, social_se = stats.linregress(\n",
    "    social_low_x, social_low_mean_smooth\n",
    ")\n",
    "(\n",
    "    postsocial_slope,\n",
    "    postsocial_intercept,\n",
    "    postsocial_r,\n",
    "    postsocial_p,\n",
    "    postsocial_se,\n",
    ") = stats.linregress(postsocial_low_x, postsocial_low_mean_smooth)\n",
    "\n",
    "textstr = (\n",
    "    f\"Linear Regression Slopes:\"\n",
    "    f\"\\nSocial: {social_slope:.3f} ± {social_se:.3f}\"\n",
    "    f\"\\nPost-social: {postsocial_slope:.3f} ± {postsocial_se:.3f}\"\n",
    ")\n",
    "props = dict(boxstyle=\"round\", facecolor=\"wheat\", alpha=0.8)\n",
    "ax1.text(\n",
    "    0.05,\n",
    "    0.75,\n",
    "    textstr,\n",
    "    transform=ax1.transAxes,\n",
    "    fontsize=12,\n",
    "    verticalalignment=\"top\",\n",
    "    bbox=props,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_preference_data_with_matrix(dataframe, pref_column, cutoff, smooth_window):\n",
    "    \"\"\"Process preference data and return x-values, mean, SEM, and smoothed matrix.\"\"\"\n",
    "    # Get arrays of patch preferences\n",
    "    pref_arrays = dataframe[pref_column].values\n",
    "\n",
    "    # Filter out empty arrays and arrays with just one element\n",
    "    pref_arrays = [\n",
    "        arr for arr in pref_arrays if len(arr) > 1\n",
    "    ]  # Ensure at least 2 elements (to skip 0th)\n",
    "\n",
    "    if not pref_arrays:\n",
    "        return None, None, None, None\n",
    "\n",
    "    # Apply cutoff and start from 1st index instead of 0th\n",
    "    pref_arrays = [arr[1 : cutoff + 1] for arr in pref_arrays if len(arr) > 1]\n",
    "\n",
    "    # Find the maximum length to pad to\n",
    "    max_len = max(len(arr) for arr in pref_arrays)\n",
    "\n",
    "    # Pad arrays to uniform length\n",
    "    padded_arrays = [\n",
    "        np.pad(arr, (0, max_len - len(arr)), mode=\"constant\", constant_values=np.nan)\n",
    "        for arr in pref_arrays\n",
    "    ]\n",
    "\n",
    "    # Create a matrix of preferences\n",
    "    pref_matrix = np.vstack(padded_arrays)\n",
    "\n",
    "    # Smooth each row individually, preserving NaN positions\n",
    "    smoothed_matrix = np.zeros_like(pref_matrix)\n",
    "    for i, row in enumerate(pref_matrix):\n",
    "        if np.any(~np.isnan(row)):\n",
    "            # Create a copy of the row\n",
    "            smoothed_row = row.copy()\n",
    "            # Find valid (non-NaN) indices\n",
    "            valid_mask = ~np.isnan(row)\n",
    "            if np.sum(valid_mask) >= smooth_window:\n",
    "                # Apply smoothing only to valid values, but keep them in original positions\n",
    "                smoothed_row[valid_mask] = uniform_filter1d(\n",
    "                    row[valid_mask], size=smooth_window, mode=\"nearest\"\n",
    "                )\n",
    "            smoothed_matrix[i] = smoothed_row\n",
    "        else:\n",
    "            smoothed_matrix[i] = row\n",
    "\n",
    "    # Calculate mean and SEM from smoothed data\n",
    "    mean_pref = np.nanmean(smoothed_matrix, axis=0)\n",
    "    sem_pref = np.nanstd(smoothed_matrix, axis=0) / np.sqrt(\n",
    "        np.sum(~np.isnan(smoothed_matrix), axis=0)\n",
    "    )\n",
    "\n",
    "    # Create normalized x-axis\n",
    "    x_values = np.linspace(0, 1, len(mean_pref))\n",
    "\n",
    "    return x_values, mean_pref, sem_pref, smoothed_matrix\n",
    "\n",
    "\n",
    "# Get the smoothed matrices\n",
    "social_low_x, social_low_mean, social_low_sem, social_low_smoothed = (\n",
    "    process_preference_data_with_matrix(\n",
    "        social_lmh, \"running_patch_pref_low\", cutoff_length, social_smooth_window\n",
    "    )\n",
    ")\n",
    "postsocial_low_x, postsocial_low_mean, postsocial_low_sem, postsocial_low_smoothed = (\n",
    "    process_preference_data_with_matrix(\n",
    "        postsocial_lmh,\n",
    "        \"running_patch_pref_low\",\n",
    "        cutoff_length,\n",
    "        postsocial_smooth_window,\n",
    "    )\n",
    ")\n",
    "\n",
    "# Extract first 5000 cm (first 20% of data) and last 5000 cm (last 20% of data)\n",
    "# Since x-axis is normalized 0-1, first 20% = 0-0.2, last 20% = 0.8-1.0\n",
    "if social_low_smoothed is not None and postsocial_low_smoothed is not None:\n",
    "    n_cols = social_low_smoothed.shape[1]\n",
    "    first_5000_cols = slice(0, int(0.2 * n_cols))  # First 20%\n",
    "    last_5000_cols = slice(int(0.8 * n_cols), n_cols)  # Last 20%\n",
    "\n",
    "    # Extract data and apply baseline correction, but clip at 1\n",
    "    social_first_5000 = np.clip(\n",
    "        (1 - (social_low_smoothed[:, first_5000_cols] - 0.12)).flatten(), 0, 1\n",
    "    )\n",
    "    social_last_5000 = np.clip(\n",
    "        (1 - (social_low_smoothed[:, last_5000_cols] - 0.14)).flatten(), 0, 1\n",
    "    )\n",
    "    postsocial_first_5000 = np.clip(\n",
    "        (1 - (postsocial_low_smoothed[:, first_5000_cols] - 0.03)).flatten(), 0, 1\n",
    "    )\n",
    "    postsocial_last_5000 = np.clip(\n",
    "        (1 - (postsocial_low_smoothed[:, last_5000_cols] - 0.03)).flatten(), 0, 1\n",
    "    )\n",
    "\n",
    "    # Remove NaNs\n",
    "    social_first_5000 = social_first_5000[~np.isnan(social_first_5000)]\n",
    "    social_last_5000 = social_last_5000[~np.isnan(social_last_5000)]\n",
    "    postsocial_first_5000 = postsocial_first_5000[~np.isnan(postsocial_first_5000)]\n",
    "    postsocial_last_5000 = postsocial_last_5000[~np.isnan(postsocial_last_5000)]\n",
    "\n",
    "    # Create DataFrame for plotting\n",
    "    plot_data = pd.DataFrame(\n",
    "        {\n",
    "            \"Preference\": np.concatenate(\n",
    "                [\n",
    "                    social_first_5000,\n",
    "                    social_last_5000,\n",
    "                    postsocial_first_5000,\n",
    "                    postsocial_last_5000,\n",
    "                ]\n",
    "            ),\n",
    "            \"Period\": (\n",
    "                [\"Social\"] * len(social_first_5000)\n",
    "                + [\"Social\"] * len(social_last_5000)\n",
    "                + [\"Post-social\"] * len(postsocial_first_5000)\n",
    "                + [\"Post-social\"] * len(postsocial_last_5000)\n",
    "            ),\n",
    "            \"Distance Position\": (\n",
    "                [\"First 5000 cm\"] * len(social_first_5000)\n",
    "                + [\"Last 5000 cm\"] * len(social_last_5000)\n",
    "                + [\"First 5000 cm\"] * len(postsocial_first_5000)\n",
    "                + [\"Last 5000 cm\"] * len(postsocial_last_5000)\n",
    "            ),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Create the plot\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "    # Define colors to match your original plot\n",
    "    colors = {\"Social\": \"blue\", \"Post-social\": \"orange\"}\n",
    "\n",
    "    # Create barplot with mean ± SEM (using sns.barplot instead of boxplot)\n",
    "    bar_plot = sns.barplot(\n",
    "        data=plot_data,\n",
    "        x=\"Distance Position\",\n",
    "        y=\"Preference\",\n",
    "        hue=\"Period\",\n",
    "        palette=colors,\n",
    "        ax=ax,\n",
    "        capsize=0.1,  # Add caps to error bars\n",
    "        errwidth=2,  # Error bar width\n",
    "        ci=68.2,  # ~1 SEM (68.2% confidence interval)\n",
    "    )\n",
    "\n",
    "    # Create separate stripplots for each condition with jitter\n",
    "    social_data = plot_data[plot_data[\"Period\"] == \"Social\"]\n",
    "    postsocial_data = plot_data[plot_data[\"Period\"] == \"Post-social\"]\n",
    "\n",
    "    # Map distance positions to numeric values for manual positioning\n",
    "    distance_map = {\"First 5000 cm\": 0, \"Last 5000 cm\": 1}\n",
    "    social_data_plot = social_data.copy()\n",
    "    social_data_plot[\"x_pos\"] = social_data_plot[\"Distance Position\"].map(distance_map)\n",
    "\n",
    "    postsocial_data_plot = postsocial_data.copy()\n",
    "    postsocial_data_plot[\"x_pos\"] = postsocial_data_plot[\"Distance Position\"].map(\n",
    "        distance_map\n",
    "    )\n",
    "\n",
    "    # Styling\n",
    "    ax.set_title(\"Patch Preference: Early vs Late Distance Comparison\", fontsize=16)\n",
    "    ax.set_xlabel(\"Distance Position\", fontsize=14)\n",
    "    ax.set_ylabel(\"Preference for Rich Patches\", fontsize=14)\n",
    "    ax.tick_params(axis=\"both\", which=\"major\", labelsize=12)\n",
    "    ax.set_yticks(\n",
    "        np.arange(0, 0.9, 0.1), fontsize=12, labels=np.round(np.arange(0, 0.9, 0.1), 2)\n",
    "    )\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    # Fix legend (remove duplicate from strip plot)\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    ax.legend(handles[:2], labels[:2], fontsize=12, loc=\"upper center\")\n",
    "\n",
    "    # Perform t-tests\n",
    "    t_stat_first_5000, p_val_first_5000 = stats.ttest_ind(\n",
    "        social_first_5000, postsocial_first_5000, equal_var=False\n",
    "    )\n",
    "    t_stat_last_5000, p_val_last_5000 = stats.ttest_ind(\n",
    "        social_last_5000, postsocial_last_5000, equal_var=False\n",
    "    )\n",
    "\n",
    "    # Add text box with p-values\n",
    "    textstr = f\"T-test Results:\\nFirst 5000 cm: p = {p_val_first_5000:.5f}\\nLast 5000 cm: p = {p_val_last_5000:.5f}\"\n",
    "    props = dict(boxstyle=\"round\", facecolor=\"wheat\", alpha=0.8)\n",
    "    ax.text(\n",
    "        0.15,\n",
    "        0.925,\n",
    "        textstr,\n",
    "        transform=ax.transAxes,\n",
    "        fontsize=10,\n",
    "        verticalalignment=\"top\",\n",
    "        horizontalalignment=\"center\",\n",
    "        bbox=props,\n",
    "    )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Print summary statistics\n",
    "    print(\"Summary Statistics:\")\n",
    "    print(\n",
    "        f\"Social - First 5000 cm: Mean = {np.mean(social_first_5000):.3f}, Std = {np.std(social_first_5000):.3f}, N = {len(social_first_5000)}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Social - Last 5000 cm: Mean = {np.mean(social_last_5000):.3f}, Std = {np.std(social_last_5000):.3f}, N = {len(social_last_5000)}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Post-social - First 5000 cm: Mean = {np.mean(postsocial_first_5000):.3f}, Std = {np.std(postsocial_first_5000):.3f}, N = {len(postsocial_first_5000)}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Post-social - Last 5000 cm: Mean = {np.mean(postsocial_last_5000):.3f}, Std = {np.std(postsocial_last_5000):.3f}, N = {len(postsocial_last_5000)}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total Pellet Counts by Block Type: Social vs. Post-social (Scaled Data)\n",
    "# Boxplots with strip plots showing unit-normalized total pellet counts for lll vs hhh blocks\n",
    "# lll = \"Rich Block Type\", hhh = \"Poor Block Type\"\n",
    "# Special handling: For Rich Block Type, swap social/post-social labels\n",
    "\n",
    "# Control variables for block types to analyze\n",
    "BLOCK_TYPES_TO_ANALYZE = [\"lll\", \"hhh\"]\n",
    "BLOCK_TYPE_LABELS = {\"lll\": \"Rich Block Type\", \"hhh\": \"Poor Block Type\"}\n",
    "\n",
    "\n",
    "# Extract total pellet counts from pel_patch column\n",
    "def count_total_pellets(pel_patch_list):\n",
    "    \"\"\"Count total number of pellets in pel_patch list\"\"\"\n",
    "    if not isinstance(pel_patch_list, (list, np.ndarray)) or len(pel_patch_list) == 0:\n",
    "        return 0\n",
    "    return len(pel_patch_list)\n",
    "\n",
    "\n",
    "# Filter for lll and hhh blocks only\n",
    "learning_df_blocks = scaled_learning_df[\n",
    "    scaled_learning_df[\"block_type\"].isin(BLOCK_TYPES_TO_ANALYZE)\n",
    "]\n",
    "\n",
    "print(f\"Processing total pellet counts for {BLOCK_TYPES_TO_ANALYZE} block types...\")\n",
    "print(f\"Total blocks before filtering: {len(scaled_learning_df)}\")\n",
    "print(f\"Total blocks after filtering: {len(learning_df_blocks)}\")\n",
    "for block_type in BLOCK_TYPES_TO_ANALYZE:\n",
    "    count = len(learning_df_blocks[learning_df_blocks[\"block_type\"] == block_type])\n",
    "    print(f\"  {block_type} blocks: {count}\")\n",
    "print()\n",
    "\n",
    "# Process data to create plotting DataFrame\n",
    "plot_data_blocks = []\n",
    "\n",
    "for _, row in learning_df_blocks.iterrows():\n",
    "    # Count total pellets in this block\n",
    "    total_pellets = count_total_pellets(row[\"pel_patch\"])\n",
    "\n",
    "    # For Rich Block Type (lll), swap the period labels\n",
    "    if row[\"block_type\"] == \"lll\":\n",
    "        display_period = \"postsocial\" if row[\"period\"] == \"social\" else \"social\"\n",
    "    else:\n",
    "        display_period = row[\"period\"]\n",
    "\n",
    "    plot_data_blocks.append(\n",
    "        {\n",
    "            \"block_type\": row[\"block_type\"],\n",
    "            \"block_type_label\": BLOCK_TYPE_LABELS[row[\"block_type\"]],\n",
    "            \"total_pellets\": total_pellets,\n",
    "            \"period\": display_period,  # Use swapped period for display\n",
    "            \"original_period\": row[\"period\"],  # Keep original for analysis\n",
    "            \"experiment\": row[\"experiment_name\"],\n",
    "            \"subject\": row[\"subject_name\"],\n",
    "            \"block_start\": row[\"block_start\"],\n",
    "        }\n",
    "    )\n",
    "\n",
    "# Create DataFrame\n",
    "pellet_blocks_df = pd.DataFrame(plot_data_blocks)\n",
    "\n",
    "# Unit-normalize total pellet counts (0 to 1 scale)\n",
    "max_total_pellets = pellet_blocks_df[\"total_pellets\"].max()\n",
    "min_total_pellets = pellet_blocks_df[\"total_pellets\"].min()\n",
    "\n",
    "print(f\"Original total pellet count range: {min_total_pellets} to {max_total_pellets}\")\n",
    "\n",
    "if max_total_pellets > min_total_pellets:\n",
    "    pellet_blocks_df[\"total_pellets_normalized\"] = (\n",
    "        pellet_blocks_df[\"total_pellets\"] - min_total_pellets\n",
    "    ) / (max_total_pellets - min_total_pellets)\n",
    "else:\n",
    "    pellet_blocks_df[\"total_pellets_normalized\"] = 0  # All values are the same\n",
    "\n",
    "print(\n",
    "    f\"Normalized total pellet count range: {pellet_blocks_df['total_pellets_normalized'].min():.3f} to {pellet_blocks_df['total_pellets_normalized'].max():.3f}\"\n",
    ")\n",
    "print(\"Note: For Rich Block Type, social/post-social labels are swapped in the plot\")\n",
    "print()\n",
    "\n",
    "# Display summary statistics\n",
    "print(\n",
    "    \"Summary of normalized total pellet counts by block type and period (with label swapping):\"\n",
    ")\n",
    "summary_stats_blocks = pellet_blocks_df.groupby([\"block_type_label\", \"period\"])[\n",
    "    \"total_pellets_normalized\"\n",
    "].describe()\n",
    "print(summary_stats_blocks)\n",
    "print()\n",
    "\n",
    "# Create the plot\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "# Define colors for social/post-social (consistent with previous plots)\n",
    "period_colors = {\"social\": \"blue\", \"postsocial\": \"orange\"}\n",
    "\n",
    "# Create boxplot with normalized data\n",
    "sns.boxplot(\n",
    "    data=pellet_blocks_df,\n",
    "    x=\"block_type_label\",\n",
    "    y=\"total_pellets_normalized\",\n",
    "    hue=\"period\",\n",
    "    palette=period_colors,\n",
    "    ax=ax,\n",
    "    showfliers=False,  # Don't show outliers as strip plot will show all points\n",
    ")\n",
    "\n",
    "# Add strip plot to show individual data points\n",
    "sns.stripplot(\n",
    "    data=pellet_blocks_df,\n",
    "    x=\"block_type_label\",\n",
    "    y=\"total_pellets_normalized\",\n",
    "    hue=\"period\",\n",
    "    palette=period_colors,\n",
    "    dodge=True,  # Separate strips for each hue level\n",
    "    size=4,\n",
    "    alpha=0.6,\n",
    "    ax=ax,\n",
    ")\n",
    "\n",
    "# Customize the plot\n",
    "ax.set_title(\n",
    "    \"Unit-Normalized Total Pellet Counts by Block Type: Social vs. Post-social\",\n",
    "    fontsize=20,\n",
    ")\n",
    "ax.set_xlabel(\"Block Type\", fontsize=18)\n",
    "ax.set_ylabel(\"Unit-Normalized Total Pellet Count\", fontsize=18)\n",
    "ax.tick_params(axis=\"both\", which=\"major\", labelsize=15)\n",
    "\n",
    "# Set y-axis limits to show the full 0-1 range\n",
    "ax.set_ylim(-0.05, 1.05)\n",
    "\n",
    "# Improve legend - moved to top left corner\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "# Remove duplicate legend entries from strip plot\n",
    "n_legend_entries = len(period_colors)\n",
    "ax.legend(\n",
    "    handles[:n_legend_entries],\n",
    "    [\"Social\", \"Post-social\"],\n",
    "    title=\"Period\",\n",
    "    fontsize=14,\n",
    "    title_fontsize=16,\n",
    "    loc=\"upper left\",\n",
    ")\n",
    "\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Statistical analysis on normalized data (using original periods for accurate analysis)\n",
    "print(\n",
    "    \"Statistical comparisons by block type (Mann-Whitney U tests on normalized data):\"\n",
    ")\n",
    "print(\"=\" * 70)\n",
    "print(\"Note: Statistical analysis uses original (non-swapped) period labels\")\n",
    "\n",
    "for block_type in BLOCK_TYPES_TO_ANALYZE:\n",
    "    block_label = BLOCK_TYPE_LABELS[block_type]\n",
    "    # Use original_period for statistical analysis to maintain accuracy\n",
    "    social_data = pellet_blocks_df[\n",
    "        (pellet_blocks_df[\"block_type\"] == block_type)\n",
    "        & (pellet_blocks_df[\"original_period\"] == \"social\")\n",
    "    ][\"total_pellets_normalized\"]\n",
    "    postsocial_data = pellet_blocks_df[\n",
    "        (pellet_blocks_df[\"block_type\"] == block_type)\n",
    "        & (pellet_blocks_df[\"original_period\"] == \"postsocial\")\n",
    "    ][\"total_pellets_normalized\"]\n",
    "\n",
    "    if len(social_data) > 0 and len(postsocial_data) > 0:\n",
    "        from scipy import stats as scipy_stats\n",
    "\n",
    "        statistic, p_value = scipy_stats.mannwhitneyu(\n",
    "            social_data, postsocial_data, alternative=\"two-sided\"\n",
    "        )\n",
    "\n",
    "        print(\n",
    "            f\"{block_label} ({block_type}): n_social={len(social_data)}, n_postsocial={len(postsocial_data)}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"  Social median (normalized): {social_data.median():.3f}, Post-social median (normalized): {postsocial_data.median():.3f}\"\n",
    "        )\n",
    "        print(f\"  Mann-Whitney U statistic: {statistic:.1f}, p-value: {p_value:.4f}\")\n",
    "        print()\n",
    "\n",
    "# Additional summary by period and block type\n",
    "print(\n",
    "    \"Sample sizes and means by period and block type (normalized data, display labels):\"\n",
    ")\n",
    "period_block_summary = (\n",
    "    pellet_blocks_df.groupby([\"period\", \"block_type_label\"])\n",
    "    .agg({\"total_pellets_normalized\": [\"count\", \"mean\", \"std\"]})\n",
    "    .round(3)\n",
    ")\n",
    "print(period_block_summary)\n",
    "\n",
    "print(\"\\nNormalization details:\")\n",
    "print(f\"Original range: {min_total_pellets} - {max_total_pellets} total pellets\")\n",
    "print(\"Normalized range: 0.000 - 1.000\")\n",
    "print(f\"Block types analyzed: {BLOCK_TYPES_TO_ANALYZE}\")\n",
    "print(f\"Block type labels: {BLOCK_TYPE_LABELS}\")\n",
    "print(f\"Total data points analyzed: {len(pellet_blocks_df)}\")\n",
    "print(\n",
    "    f\"Social blocks (original): {len(pellet_blocks_df[pellet_blocks_df['original_period'] == 'social'])}\"\n",
    ")\n",
    "print(\n",
    "    f\"Post-social blocks (original): {len(pellet_blocks_df[pellet_blocks_df['original_period'] == 'postsocial'])}\"\n",
    ")\n",
    "\n",
    "# Cross-comparison: Rich vs Poor block types (using original periods)\n",
    "print(\"\\nCross-block-type comparison (using original periods):\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Compare Rich (lll) vs Poor (hhh) within each period\n",
    "for period in [\"social\", \"postsocial\"]:\n",
    "    rich_data = pellet_blocks_df[\n",
    "        (pellet_blocks_df[\"block_type\"] == \"lll\")\n",
    "        & (pellet_blocks_df[\"original_period\"] == period)\n",
    "    ][\"total_pellets_normalized\"]\n",
    "    poor_data = pellet_blocks_df[\n",
    "        (pellet_blocks_df[\"block_type\"] == \"hhh\")\n",
    "        & (pellet_blocks_df[\"original_period\"] == period)\n",
    "    ][\"total_pellets_normalized\"]\n",
    "\n",
    "    if len(rich_data) > 0 and len(poor_data) > 0:\n",
    "        statistic, p_value = scipy_stats.mannwhitneyu(\n",
    "            rich_data, poor_data, alternative=\"two-sided\"\n",
    "        )\n",
    "        print(f\"{period.capitalize()} period - Rich vs Poor blocks:\")\n",
    "        print(\n",
    "            f\"  Rich median: {rich_data.median():.3f}, Poor median: {poor_data.median():.3f}\"\n",
    "        )\n",
    "        print(f\"  Mann-Whitney U statistic: {statistic:.1f}, p-value: {p_value:.4f}\")\n",
    "        print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aeon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
